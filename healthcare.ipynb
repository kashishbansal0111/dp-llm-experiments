{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5396fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "\n",
    "from groq import Groq\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b18f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEFAULT_TARGET_EPSILON = 1.0\n",
    "DEFAULT_TARGET_DELTA = 1e-5 \n",
    "DEFAULT_MAX_GRAD_NORM = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3260d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized successfully for model: deepseek-r1-distill-llama-70b\n"
     ]
    }
   ],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\", \"llama3-70b-8192\")\n",
    "\n",
    "client = None \n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Error: GROQ_API_KEY not found in .env file or environment variables.\")\n",
    "    print(\"Please ensure a .env file exists with GROQ_API_KEY='your_key_here'\")\n",
    "else:\n",
    "    try:\n",
    "        client = Groq(api_key=GROQ_API_KEY)\n",
    "        print(f\"Groq client initialized successfully for model: {GROQ_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Groq client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854680d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini client initialized successfully for model: gemini-2.5-pro-exp-03-25\n"
     ]
    }
   ],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL_NAME = os.getenv(\"GEMINI_MODEL_NAME\", \"llama3-70b-8192\")\n",
    "\n",
    "client = None \n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"Error: GEMINI_API_KEY not found in .env file or environment variables.\")\n",
    "    print(\"Please ensure a .env file exists with GEMINI_API_KEY='your_key_here'\")\n",
    "else:\n",
    "    try:\n",
    "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        print(f\"Gemini client initialized successfully for model: {GEMINI_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Gemini client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7ed740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Dataset shape: (5110, 12)\n",
      "First 5 rows:\n",
      "   Unnamed: 0  gender   age  hypertension  heart_disease ever_married  \\\n",
      "0           0    Male  67.0             0              1          Yes   \n",
      "1           1  Female  61.0             0              0          Yes   \n",
      "2           2    Male  80.0             0              1          Yes   \n",
      "3           3  Female  49.0             0              0          Yes   \n",
      "4           4  Female  79.0             1              0          Yes   \n",
      "\n",
      "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
      "0        Private          Urban             228.69  36.6  formerly smoked   \n",
      "1  Self-employed          Rural             202.21  28.1     never smoked   \n",
      "2        Private          Rural             105.92  32.5     never smoked   \n",
      "3        Private          Urban             171.23  34.4           smokes   \n",
      "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
      "\n",
      "   stroke  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         5110 non-null   int64  \n",
      " 1   gender             5110 non-null   object \n",
      " 2   age                5110 non-null   float64\n",
      " 3   hypertension       5110 non-null   int64  \n",
      " 4   heart_disease      5110 non-null   int64  \n",
      " 5   ever_married       5110 non-null   object \n",
      " 6   work_type          5110 non-null   object \n",
      " 7   Residence_type     5110 non-null   object \n",
      " 8   avg_glucose_level  5110 non-null   float64\n",
      " 9   bmi                5110 non-null   float64\n",
      " 10  smoking_status     5110 non-null   object \n",
      " 11  stroke             5110 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 479.2+ KB\n",
      "\n",
      "Original Columns: ['Unnamed: 0', 'gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'cleaned_healthcare_stroke.csv' \n",
    "TARGET_COLUMN = 'stroke'\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    print(\"First 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataset Info:\")\n",
    "    df.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {DATA_FILE}. Please ensure the cleaned data file exists.\")\n",
    "    df = None \n",
    "\n",
    "if df is not None:\n",
    "    original_columns = df.columns.tolist()\n",
    "    print(\"\\nOriginal Columns:\", original_columns)\n",
    "else:\n",
    "    original_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e82ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>28.1</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0           0    Male  67.0             0              1          Yes   \n",
       "1           1  Female  61.0             0              0          Yes   \n",
       "2           2    Male  80.0             0              1          Yes   \n",
       "3           3  Female  49.0             0              0          Yes   \n",
       "4           4  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21  28.1     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650140d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stroke\n",
       "0    4861\n",
       "1     249\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e962d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ea1d3",
   "metadata": {},
   "source": [
    "#### Data preprocessing for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23df4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified Categorical Features: ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
      "Identified Numerical Features: ['age', 'avg_glucose_level', 'bmi']\n",
      "\n",
      "Number of features after preprocessing: 15\n",
      "\n",
      "Data preprocessing and splitting complete.\n",
      "Training set size: 4088\n",
      "Test set size: 1022\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(TARGET_COLUMN, axis=1)\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "potential_numerical_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "numerical_features = [col for col in potential_numerical_features if col in X.columns and pd.api.types.is_numeric_dtype(X[col])]\n",
    "\n",
    "print(f\"\\nIdentified Categorical Features: {categorical_features}\")\n",
    "print(f\"Identified Numerical Features: {numerical_features}\")\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first') \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "n_features = X_train_processed.shape[1]\n",
    "print(f\"\\nNumber of features after preprocessing: {n_features}\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_processed.astype(np.float32)).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).unsqueeze(1).to(device) \n",
    "X_test_tensor = torch.tensor(X_test_processed.astype(np.float32)).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).unsqueeze(1).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"\\nData preprocessing and splitting complete.\")\n",
    "print(f\"Training set size: {len(X_train_tensor)}\")\n",
    "print(f\"Test set size: {len(X_test_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39c6823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated pos_weight for minority class (stroke=1): 19.54\n"
     ]
    }
   ],
   "source": [
    "if 'y_train' in locals():\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "\n",
    "    if pos_count > 0:\n",
    "        pos_weight_value = neg_count / pos_count\n",
    "        print(f\"\\nCalculated pos_weight for minority class (stroke=1): {pos_weight_value:.2f}\")\n",
    "        pos_weight_tensor = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        print(\"Warning: No positive samples found in training data. Cannot calculate pos_weight.\")\n",
    "        pos_weight_tensor = None \n",
    "else:\n",
    "    pos_weight_tensor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cf448",
   "metadata": {},
   "source": [
    "### Logistic regression train Without DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59e6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression model defined:\n",
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=15, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "if 'n_features' in locals():\n",
    "    model_non_dp = LogisticRegression(n_features).to(device)\n",
    "    print(\"\\nLogistic Regression model defined:\")\n",
    "    print(model_non_dp)\n",
    "else:\n",
    "    print(\"Cannot instantiate model - n_features not defined due to earlier error.\")\n",
    "    model_non_dp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e09265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Standard Logistic Regression (SGD) ---\n",
      "Epoch [1/10], Loss: 1.2298\n",
      "Epoch [2/10], Loss: 1.1303\n",
      "Epoch [3/10], Loss: 1.0696\n",
      "Epoch [4/10], Loss: 1.0313\n",
      "Epoch [5/10], Loss: 1.0050\n",
      "Epoch [6/10], Loss: 0.9845\n",
      "Epoch [7/10], Loss: 0.9690\n",
      "Epoch [8/10], Loss: 0.9581\n",
      "Epoch [9/10], Loss: 0.9497\n",
      "Epoch [10/10], Loss: 0.9425\n",
      "--- Standard Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer_non_dp = optim.SGD(model_non_dp.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\n--- Training Standard Logistic Regression (SGD) ---\")\n",
    "model_non_dp.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer_non_dp.zero_grad()\n",
    "\n",
    "        outputs = model_non_dp(batch_X)\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_non_dp.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"--- Standard Training Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34a51436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Standard Model ---\n",
      "Accuracy: 0.7250\n",
      "Precision: 0.1333\n",
      "Recall: 0.8400\n",
      "F1-Score: 0.2301\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.72      0.83       972\n",
      "         1.0       0.13      0.84      0.23        50\n",
      "\n",
      "    accuracy                           0.73      1022\n",
      "   macro avg       0.56      0.78      0.53      1022\n",
      "weighted avg       0.95      0.73      0.80      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluating Standard Model ---\")\n",
    "model_non_dp.eval()\n",
    "all_preds_non_dp = []\n",
    "all_targets_non_dp = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model_non_dp(batch_X)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        all_preds_non_dp.extend(preds.cpu().numpy())\n",
    "        all_targets_non_dp.extend(batch_y.cpu().numpy())\n",
    "accuracy = accuracy_score(all_targets_non_dp, all_preds_non_dp)\n",
    "precision = precision_score(all_targets_non_dp, all_preds_non_dp, zero_division=0)\n",
    "recall = recall_score(all_targets_non_dp, all_preds_non_dp, zero_division=0)\n",
    "f1 = f1_score(all_targets_non_dp, all_preds_non_dp, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets_non_dp, all_preds_non_dp, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651f106",
   "metadata": {},
   "source": [
    "### LLM output testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184ddf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stroke\n",
       "0    4861\n",
       "1     249\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de7f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Requesting DP Configuration from LLM ---\n",
      "Task Configuration:\n",
      "{\n",
      "  \"dataset_name\": \"Kaggle Stroke Prediction\",\n",
      "  \"data_domain\": \"Healthcare\",\n",
      "  \"task_description\": \"Train a binary classification model (Logistic Regression using DP-SGD) to predict stroke occurrence.\",\n",
      "  \"target_variable\": \"stroke\",\n",
      "  \"model_type\": \"Logistic Regression\",\n",
      "  \"dp_mechanism_family\": \"DP-SGD\",\n",
      "  \"details\": \"The stroke (target column) is heavily imbalanced, there are way more 0's than 1's. So keep in mind that disbalance\"\n",
      "}\n",
      "\n",
      "Sending request to LLM...\n",
      "Error during LLM API call or parsing: 'Client' object has no attribute 'chat'\n",
      "\n",
      "LLM configuration failed or invalid. Using default DP parameters.\n",
      "{\n",
      "  \"dp_algorithm\": \"DP-SGD with Gaussian Noise (Default)\",\n",
      "  \"target_epsilon\": 1.0,\n",
      "  \"target_delta\": 1e-05,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"preprocessing_suggestions\": [\n",
      "    \"Normalize numerical features\",\n",
      "    \"Remove identifiers (if any)\"\n",
      "  ],\n",
      "  \"column_sensitivity_epsilon\": {\n",
      "    \"Info\": \"Using default parameters\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Requesting DP Configuration from LLM ---\")\n",
    "\n",
    "task_config = {\n",
    "    \"dataset_name\": \"Kaggle Stroke Prediction\",\n",
    "    \"data_domain\": \"Healthcare\",\n",
    "    \"task_description\": \"Train a binary classification model (Logistic Regression using DP-SGD) to predict stroke occurrence.\",\n",
    "    \"target_variable\": TARGET_COLUMN,\n",
    "    \"model_type\": \"Logistic Regression\",\n",
    "    \"dp_mechanism_family\": \"DP-SGD\",\n",
    "    \"details\": \"The stroke (target column) is heavily imbalanced, there are way more 0's than 1's. So keep in mind that disbalance\"\n",
    "}\n",
    "print(\"Task Configuration:\")\n",
    "print(json.dumps(task_config, indent=2))\n",
    "\n",
    "schema_string = \", \".join(df.columns.tolist())\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Analyze the provided dataset context and task to recommend Differential Privacy (DP) settings for training a Logistic Regression model using DP-SGD. \n",
    "The goal is to predict the target variable '{task_config['target_variable']}'. \n",
    "Don't just suggest standard values, they should be accurate enough based on provided context to require minimal tuning. \n",
    "For 'max_grad_norm' suggest large enough value if target variable is largely skewed for enough accuracy.\n",
    "\n",
    "Dataset Context:\n",
    "- Name: {task_config['dataset_name']}\n",
    "- Domain: {task_config['data_domain']}\n",
    "- Task: {task_config['task_description']}\n",
    "- Schema (Original Columns): {schema_string}\n",
    "- Data Shape: {df.shape}\n",
    "- Extra details: {task_config['details']}\n",
    "\n",
    "Provide your recommendations ONLY in a structured JSON format. The JSON object must include the following keys:\n",
    "- \"dp_algorithm\": String, the specific DP algorithm variant recommended (e.g., \"DP-SGD with Gaussian Noise\").\n",
    "- \"target_epsilon\": Float, recommended privacy budget epsilon (e.g., 1.5). Consider domain sensitivity and model training needs.\n",
    "- \"target_delta\": Float, recommended privacy budget delta (e.g., 1e-5 or suggest calculating based on dataset size N as 1/N).\n",
    "- \"max_grad_norm\": Float, recommended gradient clipping norm.\n",
    "- \"preprocessing_suggestions\": List of strings, specific preprocessing actions recommended BEFORE applying DP (e.g., \"Remove: id\", \"Normalize: age, avg_glucose_level, bmi\").\n",
    "- \"column_sensitivity_epsilon\": A dictionary where keys are original column names and values are relative epsilons value for adaptive privacy budget for each column. Example: {{\"id\": 1.0, \"age\": 0.7, \"gender\": 0.6}}. This is best privacy budget for these columns.\n",
    "- \"reasoning\": String, the reasoning behind output, concisely (include reasoning for column_sensitivity hints too)\n",
    "JSON Output ONLY:\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSending request to LLM...\")\n",
    "llm_config = None\n",
    "try:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=GEMINI_MODEL_NAME,   \n",
    "        temperature=0.2, \n",
    "        max_tokens=8000,\n",
    "        top_p=0.8,\n",
    "        response_format={\"type\": \"json_object\"}, \n",
    "    )\n",
    "\n",
    "    response_content = chat_completion.choices[0].message.content\n",
    "    print(\"LLM Response Received.\")\n",
    "\n",
    "    llm_config = json.loads(response_content)\n",
    "    print(\"\\nParsed LLM DP Configuration:\")\n",
    "    print(json.dumps(llm_config, indent=2))\n",
    "\n",
    "    required_keys = [\"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\"]\n",
    "    if not all(key in llm_config for key in required_keys):\n",
    "        print(\"Warning: LLM response missing some required keys.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during LLM API call or parsing: {e}\")\n",
    "    llm_config = None\n",
    "\n",
    "if llm_config is None:\n",
    "    print(\"\\nLLM configuration failed or invalid. Using default DP parameters.\")\n",
    "    llm_config = {\n",
    "        \"dp_algorithm\": \"DP-SGD with Gaussian Noise (Default)\",\n",
    "        \"target_epsilon\": DEFAULT_TARGET_EPSILON,\n",
    "        \"target_delta\": DEFAULT_TARGET_DELTA, # Calculate based on actual train size later\n",
    "        \"max_grad_norm\": DEFAULT_MAX_GRAD_NORM,\n",
    "        \"preprocessing_suggestions\": [\"Normalize numerical features\", \"Remove identifiers (if any)\"],\n",
    "        \"column_sensitivity_epsilon\": {\"Info\": \"Using default parameters\"}\n",
    "    }\n",
    "    print(json.dumps(llm_config, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "042cfaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"dp_algorithm\": \"DP-SGD with Gaussian Noise\",\n",
      "  \"target_epsilon\": 3.0,\n",
      "  \"target_delta\": 1.95e-4,\n",
      "  \"max_grad_norm\": 3.0,\n",
      "  \"preprocessing_suggestions\": [\n",
      "    \"Handle missing values (e.g., median imputation for 'bmi')\",\n",
      "    \"One-hot encode categorical features: gender, hypertension, heart_disease, ever_married, work_type, Residence_type, smoking_status\",\n",
      "    \"Standardize numerical features: age, avg_glucose_level, bmi\"\n",
      "  ],\n",
      "  \"column_sensitivity_epsilon\": {\n",
      "    \"gender\": 0.8,\n",
      "    \"age\": 0.2,\n",
      "    \"hypertension\": 0.3,\n",
      "    \"heart_disease\": 0.3,\n",
      "    \"ever_married\": 0.6,\n",
      "    \"work_type\": 0.6,\n",
      "    \"Residence_type\": 0.8,\n",
      "    \"avg_glucose_level\": 0.1,\n",
      "    \"bmi\": 0.1,\n",
      "    \"smoking_status\": 0.4\n",
      "  },\n",
      "  \"reasoning\": \"DP-SGD with Gaussian noise is standard for DP model training. Epsilon=3.0 is chosen as a starting point balancing the sensitivity of healthcare data with the need for model utility on a relatively small dataset (N=5110). Delta is set to 1/N (1/5110 ≈ 1.95e-4), a standard recommendation. Max_grad_norm=3.0 is suggested; a slightly higher value than typical defaults (like 1.0) is chosen to potentially reduce the risk of disproportionately clipping gradients from the rare positive 'stroke' class, aiming to preserve accuracy on the imbalanced target, while still providing gradient protection. Preprocessing (imputation, encoding, scaling) is crucial for model performance and DP effectiveness, applied before DP training. The column_sensitivity_epsilon values reflect perceived data sensitivity (higher value = lower sensitivity, allowing potentially more budget share if adaptive mechanisms were used); specific health metrics (glucose, bmi) and age are deemed most sensitive, while common demographics (gender, residence) are least sensitive. Note: This column-wise budget allocation isn't a standard parameter in basic DP-SGD but represents relative sensitivity.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=GEMINI_MODEL_NAME, contents=prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef6f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed LLM config from Gemini response:\n",
      "{\n",
      "  \"dp_algorithm\": \"DP-SGD with Gaussian Noise\",\n",
      "  \"target_epsilon\": 3.0,\n",
      "  \"target_delta\": 0.000195,\n",
      "  \"max_grad_norm\": 3.0,\n",
      "  \"preprocessing_suggestions\": [\n",
      "    \"Handle missing values (e.g., median imputation for 'bmi')\",\n",
      "    \"One-hot encode categorical features: gender, hypertension, heart_disease, ever_married, work_type, Residence_type, smoking_status\",\n",
      "    \"Standardize numerical features: age, avg_glucose_level, bmi\"\n",
      "  ],\n",
      "  \"column_sensitivity_epsilon\": {\n",
      "    \"gender\": 0.8,\n",
      "    \"age\": 0.2,\n",
      "    \"hypertension\": 0.3,\n",
      "    \"heart_disease\": 0.3,\n",
      "    \"ever_married\": 0.6,\n",
      "    \"work_type\": 0.6,\n",
      "    \"Residence_type\": 0.8,\n",
      "    \"avg_glucose_level\": 0.1,\n",
      "    \"bmi\": 0.1,\n",
      "    \"smoking_status\": 0.4\n",
      "  },\n",
      "  \"reasoning\": \"DP-SGD with Gaussian noise is standard for DP model training. Epsilon=3.0 is chosen as a starting point balancing the sensitivity of healthcare data with the need for model utility on a relatively small dataset (N=5110). Delta is set to 1/N (1/5110 \\u2248 1.95e-4), a standard recommendation. Max_grad_norm=3.0 is suggested; a slightly higher value than typical defaults (like 1.0) is chosen to potentially reduce the risk of disproportionately clipping gradients from the rare positive 'stroke' class, aiming to preserve accuracy on the imbalanced target, while still providing gradient protection. Preprocessing (imputation, encoding, scaling) is crucial for model performance and DP effectiveness, applied before DP training. The column_sensitivity_epsilon values reflect perceived data sensitivity (higher value = lower sensitivity, allowing potentially more budget share if adaptive mechanisms were used); specific health metrics (glucose, bmi) and age are deemed most sensitive, while common demographics (gender, residence) are least sensitive. Note: This column-wise budget allocation isn't a standard parameter in basic DP-SGD but represents relative sensitivity.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "start_index = response.text.find('{')\n",
    "end_index = response.text.rfind('}')\n",
    "\n",
    "llm_config = None\n",
    "\n",
    "json_string_only = response.text[start_index : end_index + 1]\n",
    "\n",
    "llm_config = json.loads(json_string_only)\n",
    "print(\"Successfully parsed LLM config from Gemini response:\")\n",
    "print(json.dumps(llm_config, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20112a90",
   "metadata": {},
   "source": [
    "### Training with LLM configured DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c739027",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config['target_epsilon'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a255bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Differentially Private Logistic Regression (DP-SGD) ---\n",
      "Epoch [1/10], Loss: 1.3178, Current ε: 4.2780\n",
      "Epoch [1/10], Loss: 1.3178\n",
      "Epoch [2/10], Loss: 1.6448, Current ε: 5.2813\n",
      "Epoch [2/10], Loss: 1.6448\n",
      "Epoch [3/10], Loss: 1.4848, Current ε: 6.0792\n",
      "Epoch [3/10], Loss: 1.4848\n",
      "Epoch [4/10], Loss: 1.4421, Current ε: 6.7716\n",
      "Epoch [4/10], Loss: 1.4421\n",
      "Epoch [5/10], Loss: 1.9714, Current ε: 7.3971\n",
      "Epoch [5/10], Loss: 1.9714\n",
      "Epoch [6/10], Loss: 1.7374, Current ε: 7.9752\n",
      "Epoch [6/10], Loss: 1.7374\n",
      "Epoch [7/10], Loss: 1.8112, Current ε: 8.5177\n",
      "Epoch [7/10], Loss: 1.8112\n",
      "Epoch [8/10], Loss: 1.7101, Current ε: 9.0320\n",
      "Epoch [8/10], Loss: 1.7101\n",
      "Epoch [9/10], Loss: 2.0938, Current ε: 9.5233\n",
      "Epoch [9/10], Loss: 2.0938\n",
      "Epoch [10/10], Loss: 2.1845, Current ε: 9.9953\n",
      "Epoch [10/10], Loss: 2.1845\n",
      "--- DP Training Complete ---\n",
      "Final privacy budget spent: ε = 9.9953 for δ = 1.95e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'n_features' in locals() and llm_config is not None and 'train_loader' in locals():\n",
    "    print(\"\\n--- Training Differentially Private Logistic Regression (DP-SGD) ---\")\n",
    "\n",
    "    dp_model = LogisticRegression(n_features).to(device)\n",
    "    dp_optimizer = optim.SGD(dp_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    dp_model, dp_optimizer, dp_data_loader = privacy_engine.make_private_with_epsilon(\n",
    "        module=dp_model,\n",
    "        optimizer=dp_optimizer,\n",
    "        data_loader=train_loader,\n",
    "        max_grad_norm=llm_config.get(\"max_grad_norm\", DEFAULT_MAX_GRAD_NORM),\n",
    "        target_epsilon=llm_config.get(\"target_epsilon\", DEFAULT_TARGET_EPSILON),\n",
    "        target_delta=llm_config.get(\"target_delta\", DEFAULT_TARGET_DELTA),\n",
    "        epochs=EPOCHS,\n",
    "    )\n",
    "\n",
    "\n",
    "    dp_model.train()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss_dp = 0.0\n",
    "        for batch_X, batch_y in dp_data_loader: \n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "\n",
    "            dp_optimizer.zero_grad() \n",
    "\n",
    "            outputs = dp_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            dp_optimizer.step()\n",
    "\n",
    "            epoch_loss_dp += loss.item()\n",
    "\n",
    "        avg_epoch_loss_dp = epoch_loss_dp / len(dp_data_loader)\n",
    "\n",
    "        current_epsilon = privacy_engine.get_epsilon(delta=llm_config.get(\"target_delta\", DEFAULT_TARGET_DELTA))\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_epoch_loss_dp:.4f}, Current ε: {current_epsilon:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_epoch_loss_dp:.4f}\")\n",
    "\n",
    "\n",
    "    # Get final privacy budget spent\n",
    "    final_epsilon = privacy_engine.get_epsilon(delta=llm_config.get(\"target_delta\", DEFAULT_TARGET_DELTA))\n",
    "    print(\"--- DP Training Complete ---\")\n",
    "    print(f\"Final privacy budget spent: ε = {final_epsilon:.4f} for δ = {llm_config.get('target_delta', DEFAULT_TARGET_DELTA):.2e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping DP training due to missing components (model, config, or data loader).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2daa7169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating DP Model ---\n",
      "Accuracy (DP): 0.9511\n",
      "Precision (DP): 0.0000\n",
      "Recall (DP): 0.0000\n",
      "F1-Score (DP): 0.0000\n",
      "\n",
      "Classification Report (DP):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97       972\n",
      "         1.0       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.95      1022\n",
      "   macro avg       0.48      0.50      0.49      1022\n",
      "weighted avg       0.90      0.95      0.93      1022\n",
      "\n",
      "\n",
      "--- Comparison ---\n",
      "Metric      | Non-DP | DP\n",
      "------------|--------|--------\n",
      "Accuracy    | 0.7250 | 0.9511\n",
      "Precision   | 0.1333 | 0.0000\n",
      "Recall      | 0.8400 | 0.0000\n",
      "F1-Score    | 0.2301 | 0.0000\n",
      "(DP Model used ε ≈ 10.00 for δ ≈ 1.9e-04)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'dp_model' in locals() and 'test_loader' in locals():\n",
    "    print(\"\\n--- Evaluating DP Model ---\")\n",
    "    dp_model.eval()\n",
    "    all_preds_dp = []\n",
    "    all_targets_dp = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = dp_model(batch_X)\n",
    "            preds = torch.round(torch.sigmoid(outputs)) \n",
    "            all_preds_dp.extend(preds.cpu().numpy())\n",
    "            all_targets_dp.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    accuracy_dp = accuracy_score(all_targets_dp, all_preds_dp)\n",
    "    precision_dp = precision_score(all_targets_dp, all_preds_dp, zero_division=0)\n",
    "    recall_dp = recall_score(all_targets_dp, all_preds_dp, zero_division=0)\n",
    "    f1_dp = f1_score(all_targets_dp, all_preds_dp, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy (DP): {accuracy_dp:.4f}\")\n",
    "    print(f\"Precision (DP): {precision_dp:.4f}\")\n",
    "    print(f\"Recall (DP): {recall_dp:.4f}\")\n",
    "    print(f\"F1-Score (DP): {f1_dp:.4f}\")\n",
    "    print(\"\\nClassification Report (DP):\")\n",
    "    print(classification_report(all_targets_dp, all_preds_dp, zero_division=0))\n",
    "\n",
    "    if 'accuracy' in locals():\n",
    "         print(\"\\n--- Comparison ---\")\n",
    "         print(f\"Metric      | Non-DP | DP\")\n",
    "         print(f\"------------|--------|--------\")\n",
    "         print(f\"Accuracy    | {accuracy:.4f} | {accuracy_dp:.4f}\")\n",
    "         print(f\"Precision   | {precision:.4f} | {precision_dp:.4f}\")\n",
    "         print(f\"Recall      | {recall:.4f} | {recall_dp:.4f}\")\n",
    "         print(f\"F1-Score    | {f1:.4f} | {f1_dp:.4f}\")\n",
    "         if 'final_epsilon' in locals():\n",
    "             print(f\"(DP Model used ε ≈ {final_epsilon:.2f} for δ ≈ {llm_config.get('target_delta', DEFAULT_TARGET_DELTA):.1e})\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping DP evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
