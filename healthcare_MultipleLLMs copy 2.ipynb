{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd545ae",
   "metadata": {},
   "source": [
    "### Comparing Groq model and Gemini 2.5 pro on UCI Diabetes Dataset\n",
    "- Let's see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ef565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rest of imports and setup from previous Cell 1 ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Opacus\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b7cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Groq client initialized successfully for model: deepseek-r1-distill-llama-70b\n",
      "Gemini client initialized successfully for model: gemini-2.5-pro-exp-03-25\n",
      "\n",
      "Basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Description: Import libraries, set up constants, initialize/manage results list.\n",
    "\n",
    "# --- Make sure results_list is either reset or you append with clear names ---\n",
    "# Option 1: Reset for a fresh run on this dataset\n",
    "# results_list = []\n",
    "# Option 2: Keep previous results (make sure Run Type names are distinct)\n",
    "#print(f\"Starting Diabetes run. Current results count: {len(results_list)}\")\n",
    "\n",
    "# --- Constants for Diabetes ---\n",
    "DATA_FILE_DIABETES = 'diabetic_data.csv' # Assuming this is the filename\n",
    "TARGET_COLUMN_DIABETES = 'readmitted_binary' # Our new binary target\n",
    "\n",
    "DATA_FILE_INSURANCE = 'insurance_cleaned.csv' # YOUR FILENAME HERE\n",
    "TARGET_COLUMN_INSURANCE = 'charges' # Target for regression\n",
    "\n",
    "# LLM Clients\n",
    "from groq import Groq\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# --- Base Training Hyperparameters (can be reused) ---\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# --- Default DP Parameters (will be recalculated) ---\n",
    "DEFAULT_TARGET_EPSILON = 1.0\n",
    "DEFAULT_TARGET_DELTA = 1e-5 # Placeholder, recalculate based on N\n",
    "DEFAULT_MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- LLM Clients Initialization (reuse from previous) ---\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\", \"llama3-70b-8192\")\n",
    "GEMINI_MODEL_NAME = os.getenv(\"GEMINI_MODEL_NAME\", \"gemini-1.5-flash\")\n",
    "\n",
    "groq_client = None\n",
    "gemini_client = None\n",
    "# ... (rest of client initialization logic from previous Cell 1) ...\n",
    "if GROQ_API_KEY:\n",
    "    try:\n",
    "        groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "        print(f\"Groq client initialized successfully for model: {GROQ_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Groq client: {e}\")\n",
    "else:\n",
    "    print(\"Warning: GROQ_API_KEY not found. Groq LLM will not be used.\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    try:\n",
    "        gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        print(f\"Gemini client initialized successfully for model: {GEMINI_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Gemini client: {e}\")\n",
    "else:\n",
    "    print(\"Warning: GEMINI_API_KEY not found. Gemini LLM will not be used.\")\n",
    "\n",
    "# --- LLM Helper Functions (reuse from previous Cell 6) ---\n",
    "# Includes: create_llm_prompt, get_gemini_config, get_groq_config\n",
    "# Make sure create_llm_prompt is the latest version including ML context\n",
    "\n",
    "# --- DP Training Function (reuse from previous Cell 7) ---\n",
    "# Includes: train_evaluate_dp_model (ensure it handles classification)\n",
    "\n",
    "# --- Logistic Regression Model Class (reuse from previous Cell 4) ---\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "print(\"\\nBasic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d238250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Dataset: insurance_cleaned.csv ---\n",
      "Insurance Dataset loaded successfully.\n",
      "Dataset shape: (1338, 7)\n",
      "\n",
      "First 5 rows (Insurance):\n",
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520\n",
      "\n",
      "Dataset Info (Insurance):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1338 entries, 0 to 1337\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1338 non-null   int64  \n",
      " 1   sex       1338 non-null   object \n",
      " 2   bmi       1338 non-null   float64\n",
      " 3   children  1338 non-null   int64  \n",
      " 4   smoker    1338 non-null   object \n",
      " 5   region    1338 non-null   object \n",
      " 6   charges   1338 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 73.3+ KB\n",
      "\n",
      "Missing values per column (Insurance):\n",
      "age         0\n",
      "sex         0\n",
      "bmi         0\n",
      "children    0\n",
      "smoker      0\n",
      "region      0\n",
      "charges     0\n",
      "dtype: int64\n",
      "\n",
      "Original Columns (Insurance): ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges']\n"
     ]
    }
   ],
   "source": [
    "# Description: Load the Insurance (Medical Cost) dataset.\n",
    "\n",
    "print(f\"\\n--- Loading Dataset: {DATA_FILE_INSURANCE} ---\")\n",
    "try:\n",
    "    df_insurance = pd.read_csv(DATA_FILE_INSURANCE)\n",
    "    print(\"Insurance Dataset loaded successfully.\")\n",
    "    print(\"Dataset shape:\", df_insurance.shape)\n",
    "    print(\"\\nFirst 5 rows (Insurance):\")\n",
    "    print(df_insurance.head())\n",
    "    print(\"\\nDataset Info (Insurance):\")\n",
    "    df_insurance.info()\n",
    "\n",
    "    # Check for missing values (insurance.csv usually clean, but good practice)\n",
    "    print(\"\\nMissing values per column (Insurance):\")\n",
    "    print(df_insurance.isnull().sum())\n",
    "\n",
    "    if TARGET_COLUMN_INSURANCE not in df_insurance.columns:\n",
    "        print(f\"ERROR: Target column '{TARGET_COLUMN_INSURANCE}' not found!\")\n",
    "        df_insurance = None\n",
    "\n",
    "    if df_insurance is not None:\n",
    "        original_columns_insurance = df_insurance.columns.tolist()\n",
    "        print(\"\\nOriginal Columns (Insurance):\", original_columns_insurance)\n",
    "    else:\n",
    "        original_columns_insurance = []\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {DATA_FILE_INSURANCE}.\")\n",
    "    df_insurance = None\n",
    "    original_columns_insurance = []\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    df_insurance = None\n",
    "    original_columns_insurance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd4e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Insurance Data (Regression with Log Transform) ---\n",
      "Target variable 'charges' log-transformed (using np.log1p).\n",
      "\n",
      "Identified Categorical Features (Insurance): ['sex', 'smoker', 'region']\n",
      "Identified Numerical Features (Insurance): ['age', 'bmi', 'children']\n",
      "\n",
      "Number of features after preprocessing (Insurance): 9\n",
      "\n",
      "Insurance Data preprocessing and splitting complete (target log-transformed).\n",
      "Training set size: 1070\n",
      "Default Target Delta for Insurance dataset (1/N): 9.35e-04\n"
     ]
    }
   ],
   "source": [
    "# Description: Preprocess insurance data for regression task, including log transformation of target.\n",
    "\n",
    "print(\"\\n--- Preprocessing Insurance Data (Regression with Log Transform) ---\")\n",
    "if df_insurance is not None:\n",
    "    X_insurance = df_insurance.drop(TARGET_COLUMN_INSURANCE, axis=1)\n",
    "    \n",
    "    # --- Log transform the target variable ---\n",
    "    # Using np.log1p to handle potential zeros gracefully (log(1+x))\n",
    "    # Ensure target is numeric before transformation\n",
    "    if pd.api.types.is_numeric_dtype(df_insurance[TARGET_COLUMN_INSURANCE]):\n",
    "        y_insurance_original_scale = df_insurance[TARGET_COLUMN_INSURANCE].copy() # Keep original for final eval\n",
    "        y_insurance = np.log1p(df_insurance[TARGET_COLUMN_INSURANCE])\n",
    "        print(f\"Target variable '{TARGET_COLUMN_INSURANCE}' log-transformed (using np.log1p).\")\n",
    "    else:\n",
    "        print(f\"ERROR: Target column '{TARGET_COLUMN_INSURANCE}' is not numeric. Cannot apply log transform.\")\n",
    "        y_insurance = None # Prevent downstream errors\n",
    "        y_insurance_original_scale = None\n",
    "    # -----------------------------------------\n",
    "\n",
    "    if y_insurance is not None:\n",
    "        # Identify column types for Insurance dataset\n",
    "        categorical_features_insurance = X_insurance.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        numerical_features_insurance = X_insurance.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "        print(f\"\\nIdentified Categorical Features (Insurance): {categorical_features_insurance}\")\n",
    "        print(f\"Identified Numerical Features (Insurance): {numerical_features_insurance}\")\n",
    "\n",
    "        # Create preprocessing pipelines\n",
    "        numerical_transformer_insurance = StandardScaler()\n",
    "        categorical_transformer_insurance = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
    "\n",
    "        preprocessor_insurance = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer_insurance, numerical_features_insurance),\n",
    "                ('cat', categorical_transformer_insurance, categorical_features_insurance)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        # Split data - NO STRATIFICATION for regression\n",
    "        # Use the log-transformed y for training\n",
    "        X_train_ins, X_test_ins, y_train_log_ins, y_test_log_ins = train_test_split(\n",
    "            X_insurance, y_insurance, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "        )\n",
    "        # Also keep a split of the original scale y_test for final evaluation\n",
    "        _, _, _, y_test_original_scale_ins = train_test_split(\n",
    "            X_insurance, y_insurance_original_scale, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "\n",
    "        # Fit preprocessor on training data and transform\n",
    "        try:\n",
    "            X_train_processed_ins = preprocessor_insurance.fit_transform(X_train_ins)\n",
    "            X_test_processed_ins = preprocessor_insurance.transform(X_test_ins)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during preprocessing transformation (Insurance): {e}\")\n",
    "            X_train_processed_ins, X_test_processed_ins = None, None\n",
    "\n",
    "        if X_train_processed_ins is not None:\n",
    "            n_features_ins = X_train_processed_ins.shape[1]\n",
    "            print(f\"\\nNumber of features after preprocessing (Insurance): {n_features_ins}\")\n",
    "\n",
    "            # Convert to Tensors (using log-transformed y for training/prediction)\n",
    "            X_train_tensor_ins = torch.tensor(X_train_processed_ins.astype(np.float32)).to(device)\n",
    "            y_train_tensor_log_ins = torch.tensor(y_train_log_ins.values.astype(np.float32)).unsqueeze(1).to(device)\n",
    "            X_test_tensor_ins = torch.tensor(X_test_processed_ins.astype(np.float32)).to(device)\n",
    "            y_test_tensor_log_ins = torch.tensor(y_test_log_ins.values.astype(np.float32)).unsqueeze(1).to(device) # For calculating loss on log scale\n",
    "\n",
    "            # Create DataLoaders\n",
    "            train_dataset_ins = TensorDataset(X_train_tensor_ins, y_train_tensor_log_ins)\n",
    "            test_dataset_ins = TensorDataset(X_test_tensor_ins, y_test_tensor_log_ins) # Loader for test features and LOGGED target\n",
    "            train_loader_ins = DataLoader(train_dataset_ins, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            test_loader_ins = DataLoader(test_dataset_ins, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            print(\"\\nInsurance Data preprocessing and splitting complete (target log-transformed).\")\n",
    "            print(f\"Training set size: {len(train_dataset_ins)}\")\n",
    "\n",
    "            DEFAULT_TARGET_DELTA_INS = 1 / len(train_dataset_ins)\n",
    "            print(f\"Default Target Delta for Insurance dataset (1/N): {DEFAULT_TARGET_DELTA_INS:.2e}\")\n",
    "        else:\n",
    "            print(\"Skipping Insurance data tensor conversion due to preprocessing error.\")\n",
    "            # ... (set None to prevent errors) ...\n",
    "            n_features_ins, train_loader_ins, test_loader_ins, DEFAULT_TARGET_DELTA_INS = None, None, None, 1e-5\n",
    "            y_test_original_scale_ins = None # Also ensure this is None\n",
    "    else:\n",
    "        print(\"Skipping Insurance preprocessing further due to target transformation error.\")\n",
    "        # ... (set None to prevent errors) ...\n",
    "        n_features_ins, train_loader_ins, test_loader_ins, DEFAULT_TARGET_DELTA_INS = None, None, None, 1e-5\n",
    "        y_test_original_scale_ins = None # Also ensure this is None\n",
    "else:\n",
    "    print(\"Skipping Insurance preprocessing due to data loading error.\")\n",
    "    # ... (set None to prevent errors) ...\n",
    "    n_features_ins, train_loader_ins, test_loader_ins, DEFAULT_TARGET_DELTA_INS = None, None, None, 1e-5\n",
    "    y_test_original_scale_ins = None # Also ensure this is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d77a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9068d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Insurance Non-DP Linear Regression (Log-Target) ---\n",
      "Training Standard Linear Regression (Insurance - Log-Target)...\n",
      "Standard Training Complete (Insurance - Log-Target).\n",
      "Evaluating Standard Model (Insurance - Log-Target)...\n",
      "MAE (Original Scale): 12114.29, MSE (Original Scale): 334369426.56, R2 Score: -1.1538\n",
      "Insurance Non-DP (Log-Target) results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Train and evaluate standard model for Insurance cost prediction (Regression),\n",
    "# accounting for log-transformed target.\n",
    "\n",
    "print(\"\\n--- Running: Insurance Non-DP Linear Regression (Log-Target) ---\")\n",
    "if n_features_ins is not None and train_loader_ins is not None and test_loader_ins is not None and 'y_test_original_scale_ins' in locals() and y_test_original_scale_ins is not None:\n",
    "    model_non_dp_ins = LogisticRegression(n_features_ins).to(device)\n",
    "    criterion_ins_reg = nn.MSELoss() # Loss is calculated on log-transformed scale\n",
    "    optimizer_non_dp_ins = optim.SGD(model_non_dp_ins.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"Training Standard Linear Regression (Insurance - Log-Target)...\")\n",
    "    model_non_dp_ins.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_X, batch_y_log in train_loader_ins: # y is log-transformed\n",
    "            batch_X, batch_y_log = batch_X.to(device), batch_y_log.to(device)\n",
    "            optimizer_non_dp_ins.zero_grad()\n",
    "            outputs_log = model_non_dp_ins(batch_X) # Predicts log(charges)\n",
    "            loss = criterion_ins_reg(outputs_log, batch_y_log)\n",
    "            loss.backward()\n",
    "            optimizer_non_dp_ins.step()\n",
    "    print(\"Standard Training Complete (Insurance - Log-Target).\")\n",
    "\n",
    "    print(\"Evaluating Standard Model (Insurance - Log-Target)...\")\n",
    "    model_non_dp_ins.eval()\n",
    "    all_preds_log_non_dp_ins = []\n",
    "    # True targets for loss calculation are y_test_tensor_log_ins (from test_loader_ins)\n",
    "    # True targets for final metric calculation are y_test_original_scale_ins\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader_ins: # We only need X from loader, true y_log is used for loss\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs_log = model_non_dp_ins(batch_X) # Predictions are log(charges)\n",
    "            all_preds_log_non_dp_ins.extend(outputs_log.cpu().numpy().flatten())\n",
    "\n",
    "    # Inverse transform predictions to original scale\n",
    "    # Predictions were on log(1+y) scale, so inverse is exp(pred)-1\n",
    "    all_preds_original_scale_non_dp_ins = np.expm1(np.array(all_preds_log_non_dp_ins))\n",
    "    # Ensure no negative predictions after inverse transform if charges must be positive\n",
    "    all_preds_original_scale_non_dp_ins = np.maximum(0, all_preds_original_scale_non_dp_ins)\n",
    "\n",
    "\n",
    "    # Calculate regression metrics on the ORIGINAL scale\n",
    "    true_targets_original_scale = y_test_original_scale_ins.values # Get numpy array from Series\n",
    "\n",
    "    mae_non_dp_ins = mean_absolute_error(true_targets_original_scale, all_preds_original_scale_non_dp_ins)\n",
    "    mse_non_dp_ins = mean_squared_error(true_targets_original_scale, all_preds_original_scale_non_dp_ins)\n",
    "    r2_non_dp_ins = r2_score(true_targets_original_scale, all_preds_original_scale_non_dp_ins)\n",
    "\n",
    "    print(f\"MAE (Original Scale): {mae_non_dp_ins:.2f}, MSE (Original Scale): {mse_non_dp_ins:.2f}, R2 Score: {r2_non_dp_ins:.4f}\")\n",
    "\n",
    "    results_list.append({\n",
    "        \"Run Type\": \"Insurance Non-DP Regression (Log-Target)\",\n",
    "        # ... (rest of the fields, MAE, MSE, R2 Score populated) ...\n",
    "        \"Accuracy\": \"N/A\",\n",
    "        \"MAE\": mae_non_dp_ins, \"MSE\": mse_non_dp_ins, \"R2 Score\": r2_non_dp_ins,\n",
    "        \"LLM Epsilon Suggestion\": \"N/A\", \"LLM Reasoning\": \"N/A\"\n",
    "    })\n",
    "    print(\"Insurance Non-DP (Log-Target) results recorded.\")\n",
    "else:\n",
    "    print(\"Skipping Insurance non-DP run (Log-Target) due to missing components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73cb8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Description: Define the standard prompt structure and functions to call LLMs.\n",
    "\n",
    "# # v2\n",
    "# def create_llm_prompt(task_config, schema_string, data_shape):\n",
    "#     \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices.\"\"\"\n",
    "#     prompt = f\"\"\"\n",
    "# Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a Logistic Regression model using DP-SGD.\n",
    "# The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "# **Dataset Context:**\n",
    "# - Name: {task_config['dataset_name']}\n",
    "# - Domain: {task_config['data_domain']} (Note: Healthcare data is generally considered sensitive).\n",
    "# - Task: {task_config['task_description']}\n",
    "# - Schema (Original Columns): {schema_string}\n",
    "# - Extra details: {task_config['details']} (Pay close attention to class imbalance).\n",
    "\n",
    "# **Parameter Guidance - IMPORTANT:** Avoid generic default values. Base your recommendations *specifically* on the context provided above.\n",
    "\n",
    "# Provide your recommendations ONLY in a structured JSON format. The JSON object must include the following keys:\n",
    "# - \"dp_algorithm\": String, the specific DP algorithm variant recommended (e.g., \"DP-SGD with Gaussian Noise\").\n",
    "# - \"target_epsilon\": Float, recommended privacy budget epsilon (e.g., 1.5). Justify this based on sensitivity, utility needs, and domain.\n",
    "# - \"target_delta\": Float or String, recommended privacy budget delta (e.g., 1e-5 or suggest calculating as \"1/N\"). Justify choice.\n",
    "# - \"max_grad_norm\": Float, recommended gradient clipping norm (e.g., 1.0). Justify based on model stability and potential gradient explosion, especially considering class imbalance if noted.\n",
    "# - \"preprocessing_suggestions\": List of strings, specific preprocessing actions recommended BEFORE applying DP (e.g., \"Remove: id\", \"Normalize: age, avg_glucose_level, bmi\").\n",
    "# - \"column_sensitivity_epsilon\": A dictionary where keys are original column names and values are *conceptual* relative sensitivity floats (0.0=low, 1.0=high/ID) or labels (Low, Medium, High). This guides understanding, not direct budget split in standard DP-SGD. Exclude the target variable.\n",
    "# - \"reasoning\": String, concise reasoning behind the overall recommendations (epsilon, delta, max_grad_norm choices, linking back to context).\n",
    "\n",
    "# JSON Output ONLY:\n",
    "# \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "# #v3\n",
    "# def create_llm_prompt(task_config, schema_string, data_shape):\n",
    "#     \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices\n",
    "#        and including general ML best practices context.\"\"\"\n",
    "\n",
    "#     # Calculate approximate training size N for context\n",
    "#     approx_N_train = int(data_shape[0] * (1-TEST_SIZE)) if data_shape else 'Unknown'\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a Logistic Regression model using DP-SGD.\n",
    "# The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "# **Dataset Context:**\n",
    "# - Name: {task_config['dataset_name']}\n",
    "# - Domain: {task_config['data_domain']} (Note: Healthcare data is generally considered sensitive).\n",
    "# - Task: {task_config['task_description']}\n",
    "# - Schema (Original Columns): {schema_string}\n",
    "# - Data Shape: {data_shape} (Approx. Training N = {approx_N_train})\n",
    "# - Extra details: {task_config['details']} (Pay close attention to class imbalance).\n",
    "\n",
    "# **General ML Best Practices Context (Keep these in mind):**\n",
    "# - **Normalization/Scaling:** Features with different scales (like 'age' vs 'avg_glucose_level') MUST be normalized or standardized (e.g., StandardScaler, MinMaxScaler) for models like Logistic Regression and especially before applying gradient clipping in DP-SGD. This ensures stable gradient computations. Apply scaling AFTER splitting data and ideally AFTER imputation if applicable.\n",
    "# - **Gradient Stability & Clipping:** DP-SGD uses gradient clipping (`max_grad_norm`) to bound the influence of any single data point. Choosing the norm value is a trade-off:\n",
    "#     - Too low: Clips potentially useful gradient information, slowing learning or preventing convergence, especially for minority classes or complex patterns.\n",
    "#     - Too high: Less protection against outliers, potentially higher noise required for the same privacy budget (ε).\n",
    "#     - Imbalanced Data Impact: Gradients from rare class samples might be infrequent but large; aggressive clipping can disproportionately affect learning for that class.\n",
    "# - **Imbalanced Data Handling:** Beyond class weighting in the loss (which is assumed here), model evaluation should focus on metrics like F1-score, Precision, Recall for the minority class, not just accuracy. The goal is often to improve detection of the rare class.\n",
    "# - **Preprocessing Order:** Typically: Split Data -> Impute Missing -> Encode Categorical -> Scale Numerical -> Train Model. DP is applied during the training step.\n",
    "\n",
    "# **Parameter Guidance - IMPORTANT:** Based on the Dataset Context AND the ML Best Practices above, provide specific, justified recommendations. Avoid generic defaults.\n",
    "\n",
    "# 1.  **`target_epsilon`**: Balance '{task_config['data_domain']}' sensitivity vs. utility needed for training. Justify the specific trade-off. (Range 1.0-5.0 often considered, but justify *your* choice).\n",
    "# 2.  **`target_delta`**: Recommend a specific small value (e.g., 1e-5, 1e-6) or suggest \"1/N\". Justify why (e.g., related to approx N={approx_N_train}).\n",
    "# 3.  **`max_grad_norm`**: **Connect this directly to the Gradient Stability & Imbalanced Data points above.** Given the heavy imbalance, suggest a value (e.g., range 5.0 - 15.0, or a specific reasoned value) likely higher than a generic default (like 1.0) to preserve minority class signals. Justify based *explicitly* on imbalance and the need for stable yet informative gradients.\n",
    "# 4.  **`column_sensitivity_epsilon`**: Conceptual relative sensitivity hints (0.0 low, 1.0 high). Reflect potential identifiability/sensitivity based on domain/name. Exclude target.\n",
    "# 5.  **`reasoning`**: Concise but detailed justification for epsilon, delta, and max_grad_norm, *explicitly linking* choices to dataset context (domain, N, imbalance) and the relevant ML best practices mentioned (normalization, gradient stability).\n",
    "\n",
    "# **Output Format:**\n",
    "# Provide recommendations ONLY in a structured JSON format with keys: \"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\".\n",
    "\n",
    "# JSON Output ONLY:\n",
    "# \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "# Description: Define the standard prompt structure and functions to call LLMs.\n",
    "\n",
    "def create_llm_prompt(task_config, schema_string, data_shape, task_type=\"classification\"): # Add task_type\n",
    "    \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices\n",
    "       and including general ML best practices context. Adapts for task_type.\"\"\"\n",
    "\n",
    "    approx_N_train = int(data_shape[0] * (1-TEST_SIZE)) if data_shape else 'Unknown'\n",
    "\n",
    "    # Adjust parts of the prompt based on task_type\n",
    "    if task_type == \"regression\":\n",
    "        imbalance_guidance = \"\" # No class imbalance for regression\n",
    "        target_guidance_metrics = \"metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or R2 Score. The goal is often to minimize error.\"\n",
    "        max_grad_norm_focus = \"focus on overall gradient stability, considering the range and scale of the target variable values, rather than specific class signals. A moderate value (e.g., 1.0-10.0, depending on target scale and feature normalization) is common.\"\n",
    "        epsilon_utility_focus = \"utility needs for accurate predictions (e.g., low MAE/MSE)\"\n",
    "        model_type_in_prompt = \"Linear Regression\" # Or generic \"Regression Model\"\n",
    "    else: # classification (default)\n",
    "        imbalance_guidance = \"(Pay close attention to class imbalance if mentioned in 'Extra details').\"\n",
    "        target_guidance_metrics = \"metrics like F1-score, Precision, Recall for the minority class, not just accuracy. The goal is often to improve detection of the rare class.\"\n",
    "        max_grad_norm_focus = \"**Connect this directly to the Gradient Stability & Imbalanced Data points above.** Given potential class imbalance (see 'Extra details'), suggest a value (e.g., range 5.0 - 15.0, or a specific reasoned value) likely higher than a generic default (like 1.0) to preserve minority class signals. Justify based *explicitly* on imbalance and the need for stable yet informative gradients.\"\n",
    "        epsilon_utility_focus = \"utility needs for model training (which often requires sufficient signal)\"\n",
    "        model_type_in_prompt = \"Logistic Regression\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a {model_type_in_prompt} model using DP-SGD.\n",
    "The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "**Dataset Context:**\n",
    "- Name: {task_config['dataset_name']}\n",
    "- Domain: {task_config['data_domain']}\n",
    "- Task: {task_config['task_description']}\n",
    "- Schema (Original Columns): {schema_string}\n",
    "- Data Shape: {data_shape} (Approx. Training N = {approx_N_train})\n",
    "- Extra details: {task_config['details']} {imbalance_guidance}\n",
    "\n",
    "**General ML Best Practices Context (Keep these in mind):**\n",
    "- **Normalization/Scaling:** Features MUST be normalized or standardized for models like {model_type_in_prompt} and especially before applying gradient clipping in DP-SGD.\n",
    "- **Gradient Stability & Clipping (`max_grad_norm`):** Bound influence of single points. Trade-off:\n",
    "    - Too low: Clips useful info, hinders learning.\n",
    "    - Too high: Less protection, more noise needed.\n",
    "    - { \"Imbalanced Data Impact: Gradients from rare class samples might be infrequent but large; aggressive clipping can disproportionately affect learning for that class.\" if task_type==\"classification\" else \"For regression, consider the scale of the target variable when thinking about gradient magnitudes.\"}\n",
    "- **Evaluation Focus ({task_type}):** Model evaluation should focus on {target_guidance_metrics}\n",
    "- **Preprocessing Order:** Typically: Split Data -> Impute -> Encode -> Scale -> Train.\n",
    "\n",
    "**Parameter Guidance - IMPORTANT:** Base recommendations *specifically* on context. Avoid generic defaults.\n",
    "\n",
    "1.  **`target_epsilon`**: Balance '{task_config['data_domain']}' sensitivity vs. {epsilon_utility_focus}. Justify. (Range 1.0-5.0 often considered, but justify *your* choice).\n",
    "2.  **`target_delta`**: Recommend small value (e.g., 1e-5) or \"1/N\". Justify (e.g., N={approx_N_train}).\n",
    "3.  **`max_grad_norm`**: For this {task_type} task, {max_grad_norm_focus} Justify.\n",
    "4.  **`column_sensitivity_epsilon`**: Conceptual relative sensitivity hints (0.0 low, 1.0 high). Exclude target.\n",
    "5.  **`reasoning`**: Detailed justification for epsilon, delta, `max_grad_norm`, linking to dataset context (domain, N, imbalance/target scale) and ML practices.\n",
    "\n",
    "**Output Format:**\n",
    "JSON ONLY with keys: \"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\".\n",
    "\n",
    "JSON Output ONLY:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "def get_gemini_config(prompt, client):\n",
    "    \"\"\"Gets DP config from Gemini API.\"\"\"\n",
    "    if not client:\n",
    "        print(\"Gemini client not available.\")\n",
    "        return None\n",
    "    print(\"\\nSending request to Gemini...\")\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=GEMINI_MODEL_NAME, contents=prompt\n",
    "            )\n",
    "        response_text = response.text\n",
    "        print(\"Gemini Response Received.\")\n",
    "        # Extract JSON part\n",
    "        start_index = response_text.find('{')\n",
    "        end_index = response_text.rfind('}')\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_string_only = response_text[start_index : end_index + 1]\n",
    "            config = json.loads(json_string_only)\n",
    "            print(\"Successfully parsed Gemini config.\")\n",
    "            print(config)\n",
    "            # Basic validation\n",
    "            required_keys = [\"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\"]\n",
    "            if not all(key in config for key in required_keys):\n",
    "                print(\"Warning: Gemini response missing some required keys.\")\n",
    "            return config\n",
    "        else:\n",
    "            print(\"Error: Could not find JSON object in Gemini response.\")\n",
    "            print(\"Raw Response:\", response_text)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call or parsing: {e}\")\n",
    "        try:\n",
    "            print(\"Gemini Response Content (if available):\", response.candidates) # Might show safety blocks\n",
    "        except: pass\n",
    "        return None\n",
    "\n",
    "def get_groq_config(prompt, client, model_name):\n",
    "    \"\"\"Gets DP config from Groq API.\"\"\"\n",
    "    if not client:\n",
    "        print(\"Groq client not available.\")\n",
    "        return None\n",
    "    print(\"\\nSending request to Groq...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model_name,\n",
    "            temperature=0.2,\n",
    "            max_tokens=3024, \n",
    "            top_p=0.8,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "        print(\"Groq Response Received.\")\n",
    "        config = json.loads(response_content)\n",
    "        print(\"Successfully parsed Groq config.\")\n",
    "        print(config)\n",
    "\n",
    "        # Basic validation\n",
    "        required_keys = [\"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\"]\n",
    "        if not all(key in config for key in required_keys):\n",
    "            print(\"Warning: Groq response missing some required keys.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Groq API call or parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"LLM Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6f82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Training/Evaluation function (with task_type and y_test_original_scale_for_eval) defined/updated.\n"
     ]
    }
   ],
   "source": [
    "# Description: Function to train and evaluate a DP model using Opacus, for classification or regression.\n",
    "\n",
    "def train_evaluate_dp_model(\n",
    "    config, run_name, train_loader, test_loader, # test_loader yields (X_test, y_test_LOG_TRANSFORMED) for regression\n",
    "    n_features, device, epochs, learning_rate,\n",
    "    task_type=\"classification\",\n",
    "    pos_weight_tensor=None,       # Only for classification\n",
    "    target_metric_prefix=\"Class 1\", # For naming classification metrics\n",
    "    y_test_original_scale_for_eval=None # NEW/CORRECTED: Pass original scale y_test for regression evaluation\n",
    "):\n",
    "    \"\"\"Trains and evaluates DP model, returns results dictionary.\"\"\"\n",
    "    print(f\"\\n--- Running: {run_name} (Task: {task_type}) ---\")\n",
    "    if config is None:\n",
    "        print(\"Skipping run due to missing configuration.\")\n",
    "        return None\n",
    "\n",
    "    target_eps = config.get(\"target_epsilon\", DEFAULT_TARGET_EPSILON)\n",
    "    target_del_config = config.get(\"target_delta\", \"1/N\")\n",
    "    max_norm = config.get(\"max_grad_norm\", DEFAULT_MAX_GRAD_NORM)\n",
    "    llm_reasoning = config.get(\"reasoning\", \"N/A\")\n",
    "    llm_eps_suggestion = config.get(\"target_epsilon\", \"N/A (Default used)\")\n",
    "\n",
    "    # Calculate actual delta\n",
    "    if isinstance(target_del_config, str) and \"1/N\" in target_del_config and train_loader:\n",
    "        actual_delta = 1 / len(train_loader.dataset)\n",
    "    elif isinstance(target_del_config, (int, float)):\n",
    "        actual_delta = target_del_config\n",
    "    else: # Fallback\n",
    "        actual_delta = 1 / len(train_loader.dataset) if train_loader else DEFAULT_TARGET_DELTA # Use global default if no loader\n",
    "\n",
    "    dp_model = LogisticRegression(n_features).to(device)\n",
    "    dp_optimizer = optim.SGD(dp_model.parameters(), lr=learning_rate)\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    try:\n",
    "        dp_model, dp_optimizer, dp_data_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=dp_model, optimizer=dp_optimizer, data_loader=train_loader,\n",
    "            max_grad_norm=max_norm, target_epsilon=target_eps, target_delta=actual_delta, epochs=epochs\n",
    "        )\n",
    "        print(f\"Opacus Attached. Target ε={target_eps:.2f}, Target δ={actual_delta:.2e}, Max Grad Norm={max_norm}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching Opacus PrivacyEngine: {e}\")\n",
    "        return None\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    elif task_type == \"regression\":\n",
    "        criterion = nn.MSELoss() # Loss is calculated on log-transformed scale for regression\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_type. Must be 'classification' or 'regression'.\")\n",
    "\n",
    "    print(f\"Training DP Model ({task_type})...\")\n",
    "    dp_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_dp = 0.0\n",
    "        for batch_X, batch_y_train in dp_data_loader: # batch_y_train is y_log for regression\n",
    "            batch_X, batch_y_train = batch_X.to(device), batch_y_train.to(device)\n",
    "            dp_optimizer.zero_grad()\n",
    "            outputs_train = dp_model(batch_X) # Predicts log-scale for regression\n",
    "            loss = criterion(outputs_train, batch_y_train)\n",
    "            loss.backward()\n",
    "            dp_optimizer.step()\n",
    "            epoch_loss_dp += loss.item()\n",
    "        # print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss_dp / len(dp_data_loader):.4f}\")\n",
    "\n",
    "    try:\n",
    "        final_epsilon = privacy_engine.get_epsilon(delta=actual_delta)\n",
    "        print(f\"DP Training Complete. Final ε = {final_epsilon:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get final epsilon: {e}\")\n",
    "        final_epsilon = float('nan')\n",
    "\n",
    "    print(f\"Evaluating DP Model ({task_type})...\")\n",
    "    dp_model.eval()\n",
    "    all_preds_eval_log_scale = [] # For regression, these are log-scale predictions\n",
    "    all_preds_eval_final = []     # For classification, these are 0/1 predictions\n",
    "    \n",
    "    # For regression, test_loader yields (X_test, y_test_log).\n",
    "    # For classification, test_loader yields (X_test, y_test_labels_for_loss_calc).\n",
    "    # We will use y_test_original_scale_for_eval for metric calculation.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader: # We only need X from loader for predictions\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs_eval = dp_model(batch_X)\n",
    "            if task_type == \"classification\":\n",
    "                preds_final = torch.round(torch.sigmoid(outputs_eval))\n",
    "                all_preds_eval_final.extend(preds_final.cpu().numpy().flatten())\n",
    "            else: # regression\n",
    "                all_preds_eval_log_scale.extend(outputs_eval.cpu().numpy().flatten())\n",
    "\n",
    "    results_metrics = {}\n",
    "    base_results = {\n",
    "        \"Run Type\": run_name,\n",
    "        \"LLM Used\": config.get(\"llm_model_name\", \"N/A\"),\n",
    "        \"Target Epsilon\": target_eps, \"Final Epsilon\": final_epsilon,\n",
    "        \"Target Delta\": actual_delta, \"Max Grad Norm\": max_norm,\n",
    "        \"LLM Epsilon Suggestion\": llm_eps_suggestion,\n",
    "        \"LLM Reasoning\": llm_reasoning\n",
    "    }\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        if y_test_original_scale_for_eval is None: # For classification, this should be y_test_labels\n",
    "            print(\"Error: True labels for classification evaluation not provided (y_test_original_scale_for_eval is None).\")\n",
    "            return None\n",
    "        true_targets_for_metrics = y_test_original_scale_for_eval # This is y_test_labels (numpy array)\n",
    "        \n",
    "        accuracy_dp = accuracy_score(true_targets_for_metrics, all_preds_eval_final)\n",
    "        precision_dp = precision_score(true_targets_for_metrics, all_preds_eval_final, pos_label=1, zero_division=0)\n",
    "        recall_dp = recall_score(true_targets_for_metrics, all_preds_eval_final, pos_label=1, zero_division=0)\n",
    "        f1_dp = f1_score(true_targets_for_metrics, all_preds_eval_final, pos_label=1, zero_division=0)\n",
    "        print(f\"Accuracy: {accuracy_dp:.4f}, Precision ({target_metric_prefix}): {precision_dp:.4f}, Recall ({target_metric_prefix}): {recall_dp:.4f}, F1 ({target_metric_prefix}): {f1_dp:.4f}\")\n",
    "        results_metrics.update({\n",
    "            \"Accuracy\": accuracy_dp,\n",
    "            f\"Precision ({target_metric_prefix})\": precision_dp,\n",
    "            f\"Recall ({target_metric_prefix})\": recall_dp,\n",
    "            f\"F1 ({target_metric_prefix})\": f1_dp\n",
    "        })\n",
    "    elif task_type == \"regression\":\n",
    "        if y_test_original_scale_for_eval is None:\n",
    "            print(\"ERROR: y_test_original_scale_for_eval is None for regression evaluation.\")\n",
    "            return None\n",
    "            \n",
    "        # Inverse transform predictions from log(1+y) scale to original y scale\n",
    "        all_preds_eval_original_scale = np.expm1(np.array(all_preds_eval_log_scale))\n",
    "        all_preds_eval_original_scale = np.maximum(0, all_preds_eval_original_scale) # Ensure non-negative\n",
    "\n",
    "        true_targets_original_scale_eval = y_test_original_scale_for_eval # This should be a numpy array or pandas Series\n",
    "        if isinstance(true_targets_original_scale_eval, pd.Series):\n",
    "            true_targets_original_scale_eval = true_targets_original_scale_eval.values\n",
    "\n",
    "\n",
    "        mae_dp = mean_absolute_error(true_targets_original_scale_eval, all_preds_eval_original_scale)\n",
    "        mse_dp = mean_squared_error(true_targets_original_scale_eval, all_preds_eval_original_scale)\n",
    "        r2_dp = r2_score(true_targets_original_scale_eval, all_preds_eval_original_scale)\n",
    "        print(f\"MAE (Original Scale): {mae_dp:.2f}, MSE (Original Scale): {mse_dp:.2f}, R2 Score: {r2_dp:.4f}\")\n",
    "        results_metrics.update({\"MAE\": mae_dp, \"MSE\": mse_dp, \"R2 Score\": r2_dp})\n",
    "    \n",
    "    base_results.update(results_metrics)\n",
    "    print(f\"{run_name} results recorded.\")\n",
    "    return base_results\n",
    "\n",
    "print(\"DP Training/Evaluation function (with task_type and y_test_original_scale_for_eval) defined/updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43822b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Config and Prompt for Insurance Regression ---\n",
      "Insurance Regression task config and LLM prompt prepared.\n"
     ]
    }
   ],
   "source": [
    "# Description: Define task details for Insurance dataset (Regression) and generate prompt.\n",
    "\n",
    "print(\"\\n--- Preparing Config and Prompt for Insurance Regression ---\")\n",
    "if df_insurance is not None and 'original_columns_insurance' in locals():\n",
    "    # Regression details\n",
    "    regression_details = f\"Predict continuous medical charges ('{TARGET_COLUMN_INSURANCE}'). Focus on minimizing MAE/MSE. Target variable likely has a skewed distribution.\"\n",
    "\n",
    "    task_config_insurance = {\n",
    "        \"dataset_name\": \"Kaggle Medical Cost Personal\",\n",
    "        \"data_domain\": \"Healthcare/Finance\",\n",
    "        \"task_description\": f\"Train a Linear Regression model using DP-SGD to predict medical charges ('{TARGET_COLUMN_INSURANCE}').\",\n",
    "        \"target_variable\": TARGET_COLUMN_INSURANCE,\n",
    "        \"model_type\": \"Linear Regression\", # For LLM context\n",
    "        \"dp_mechanism_family\": \"DP-SGD\",\n",
    "        \"details\": regression_details\n",
    "    }\n",
    "\n",
    "    schema_string_insurance = \", \".join(original_columns_insurance)\n",
    "    data_shape_tuple_insurance = df_insurance.shape\n",
    "\n",
    "    # Call create_llm_prompt with task_type=\"regression\"\n",
    "    llm_prompt_insurance = create_llm_prompt(\n",
    "        task_config_insurance,\n",
    "        schema_string_insurance,\n",
    "        data_shape_tuple_insurance,\n",
    "        task_type=\"regression\" # Specify task type\n",
    "    )\n",
    "    print(\"Insurance Regression task config and LLM prompt prepared.\")\n",
    "    # Optional: print prompt\n",
    "    # print(\"\\n--- Insurance Regression LLM Prompt ---\")\n",
    "    # print(llm_prompt_insurance)\n",
    "else:\n",
    "    print(\"Skipping Insurance Regression prompt creation due to data loading error.\")\n",
    "    llm_prompt_insurance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "631dd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Insurance Fixed DP Regression (Log-Target) ---\n",
      "\n",
      "--- Running: Insurance Fixed DP Regression (Log-Target) (Task: regression) ---\n",
      "Opacus Attached. Target ε=1.00, Target δ=9.35e-04, Max Grad Norm=1.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 0.9917\n",
      "Evaluating DP Model (regression)...\n",
      "MAE (Original Scale): 12965.51, MSE (Original Scale): 323325780.57, R2 Score: -1.0826\n",
      "Insurance Fixed DP Regression (Log-Target) results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Execute Fixed DP run for Insurance Regression (Log-Target).\n",
    "\n",
    "print(\"\\n--- Running: Insurance Fixed DP Regression (Log-Target) ---\")\n",
    "# Ensure y_test_original_scale_ins is available from the preprocessing cell\n",
    "if n_features_ins is not None and train_loader_ins is not None and \\\n",
    "   'DEFAULT_TARGET_DELTA_INS' in locals() and \\\n",
    "   'y_test_original_scale_ins' in locals() and y_test_original_scale_ins is not None:\n",
    "\n",
    "    fixed_dp_config_ins_reg = {\n",
    "        \"dp_algorithm\": \"DP-SGD with Gaussian Noise (Fixed)\",\n",
    "        \"target_epsilon\": DEFAULT_TARGET_EPSILON,\n",
    "        \"target_delta\": DEFAULT_TARGET_DELTA_INS,\n",
    "        \"max_grad_norm\": DEFAULT_MAX_GRAD_NORM, # Default 1.0, consider if appropriate for log-scale target\n",
    "        \"reasoning\": \"Using standard fixed parameters for Insurance Regression (Log-Target).\",\n",
    "        \"llm_model_name\": \"N/A (Fixed Defaults)\",\n",
    "        \"preprocessing_suggestions\": [\"Default\"],\n",
    "        \"column_sensitivity_epsilon\": {\"Info\": \"Fixed parameters\"}\n",
    "    }\n",
    "    results_fixed_ins_reg = train_evaluate_dp_model(\n",
    "        config=fixed_dp_config_ins_reg,\n",
    "        run_name=\"Insurance Fixed DP Regression (Log-Target)\",\n",
    "        train_loader=train_loader_ins,\n",
    "        test_loader=test_loader_ins,\n",
    "        n_features=n_features_ins,\n",
    "        device=device,\n",
    "        epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        task_type=\"regression\", # Specify task type\n",
    "        pos_weight_tensor=None, # Not used for regression\n",
    "        y_test_original_scale_for_eval=y_test_original_scale_ins # Pass original y_test\n",
    "    )\n",
    "    if results_fixed_ins_reg:\n",
    "        results_list.append(results_fixed_ins_reg)\n",
    "else:\n",
    "    print(\"Skipping Insurance Fixed DP Regression (Log-Target) due to missing components (check n_features_ins, loaders, or y_test_original_scale_ins).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2a2cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Insurance Gemini DP Regression (Log-Target) ---\n",
      "\n",
      "Sending request to Gemini...\n",
      "Gemini Response Received.\n",
      "Successfully parsed Gemini config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 3.0, 'target_delta': 1e-05, 'max_grad_norm': 4.0, 'preprocessing_suggestions': ['Split data into training and testing sets first.', \"For categorical features ('sex', 'smoker', 'region'): Apply One-Hot Encoding. Consider using `drop='first'` to avoid multicollinearity, though scaling mitigates this for LR.\", \"For the target variable 'charges': Apply a log transformation (e.g., `numpy.log1p`) to handle its likely skewed distribution. This helps stabilize variance and improves linear model performance.\", \"For numerical features ('age', 'bmi', 'children') and the one-hot encoded categorical features: Apply scaling (e.g., `StandardScaler` to achieve mean 0 and unit variance, or `MinMaxScaler` to scale to [0,1]). Consistent scaling is crucial for Linear Regression and DP-SGD.\", \"For the log-transformed target variable 'charges': Apply scaling (e.g., `StandardScaler`) as well. This brings the target to a predictable scale, which aids in setting `max_grad_norm` and stabilizes training by ensuring errors are also on a controlled scale.\", 'Ensure the order of operations is correct: Split -> Encode (on train/test consistently) -> Transform Target (on train/test consistently) -> Scale (fit scaler on training data, then transform train and test).'], 'column_sensitivity_epsilon': {'age': 0.6, 'sex': 0.4, 'bmi': 0.7, 'children': 0.3, 'smoker': 0.8, 'region': 0.2}, 'reasoning': \"The recommended DP settings are tailored to the Kaggle Medical Cost Personal dataset for predicting 'charges' using DP-SGD with a Linear Regression model.\\n\\n1.  **`target_epsilon = 3.0`**: This value is chosen to balance privacy needs with model utility. The 'Healthcare/Finance' domain implies data sensitivity, warranting a reasonably strong privacy guarantee (lower epsilon). However, with a training size N approximately 1070, an extremely low epsilon (e.g., <1) might severely degrade model utility (MAE/MSE). Epsilon=3.0 is within the commonly accepted range (1.0-5.0) for meaningful privacy and offers a compromise. It provides a stronger guarantee than higher values (e.g., 5-10) while being potentially more utility-preserving than epsilon=1 on a dataset of this size.\\n\\n2.  **`target_delta = 1e-5`**: Delta is the probability of the privacy guarantee failing. It should be very small. A common recommendation is `delta < 1/N`. For N=1070, 1/N is approx 9.3e-4. Choosing `delta = 1e-5` provides a significantly stronger guarantee (it is much smaller than 1/N) and is a standard, conservative choice in DP literature, ensuring robust privacy protection.\\n\\n3.  **`max_grad_norm = 4.0`**: This L2 norm clipping threshold is crucial for DP-SGD. It's determined by considering the feature and target variable scales *after* the recommended preprocessing:\\n    *   **Feature Scaling**: All input features (numerical and OHE categorical) are assumed to be standardized (e.g., mean 0, std 1).\\n    *   **Target Scaling**: The target 'charges' is assumed to be log-transformed (due to skewness) and then standardized. This makes the error term (prediction - true_scaled_target) more well-behaved and of a smaller, predictable magnitude.\\n    *   **Gradient Estimation**: For linear regression, the per-sample gradient is `error * feature_vector`. If standardized features have an L2 norm around `sqrt(num_features)` (approx. `sqrt(8) ≈ 2.8` after OHE), and the standardized error is typically within +/- 1.5 (for a decent fit), the gradient norm would be around `1.5 * 2.8 ≈ 4.2`. A `max_grad_norm` of 4.0 allows for such typical gradients while clipping larger ones (potential outliers or during initial unstable training phases). This value is within the suggested 1.0-10.0 range and balances information preservation with the need to bound individual influence.\\n\\n4.  **`preprocessing_suggestions`**: These are critical for both Linear Regression performance and effective DP-SGD application:\\n    *   Encoding categorical variables is standard.\\n    *   Log-transforming the skewed 'charges' target is essential for linear models and stabilizes gradient magnitudes.\\n    *   Scaling all features ensures that gradient magnitudes are not dominated by features with naturally larger scales and is a prerequisite for meaningful gradient clipping.\\n    *   Scaling the transformed target variable is also recommended to ensure errors (and thus gradients) are on a consistent, predictable scale.\\n\\n5.  **`column_sensitivity_epsilon`**: These are conceptual relative sensitivity scores (0.0 low, 1.0 high) for non-target features. 'smoker' (strong health indicator) and 'bmi' (health metric) are rated higher in sensitivity, while 'region' is rated lower. These reflect potential re-identification risk or sensitivity if individual feature values were disclosed, although DP-SGD primarily protects at the record level via gradient clipping and noise.\\n\\nThese settings provide a strong, justified starting point for training a DP Linear Regression model on this dataset. Empirical tuning, particularly of `max_grad_norm` (by observing unclipped gradient norms if possible) and `target_epsilon` (based on the acceptable utility-privacy trade-off), would be beneficial for further optimization.\"}\n",
      "\n",
      "--- Running: Insurance Gemini DP Regression (Log-Target) (Task: regression) ---\n",
      "Opacus Attached. Target ε=3.00, Target δ=1.00e-05, Max Grad Norm=4.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 2.9924\n",
      "Evaluating DP Model (regression)...\n",
      "MAE (Original Scale): 12836.52, MSE (Original Scale): 319556465.14, R2 Score: -1.0584\n",
      "Insurance Gemini DP Regression (Log-Target) results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Get config from Gemini for Insurance Regression and run (Log-Target).\n",
    "\n",
    "print(\"\\n--- Running: Insurance Gemini DP Regression (Log-Target) ---\")\n",
    "# Ensure y_test_original_scale_ins is available\n",
    "if gemini_client and llm_prompt_insurance and \\\n",
    "   n_features_ins is not None and train_loader_ins is not None and \\\n",
    "   'y_test_original_scale_ins' in locals() and y_test_original_scale_ins is not None:\n",
    "\n",
    "    gemini_config_ins_reg = get_gemini_config(llm_prompt_insurance, gemini_client) # Assuming gemini_client is your GenerativeModel instance\n",
    "    if gemini_config_ins_reg:\n",
    "        gemini_config_ins_reg[\"llm_model_name\"] = GEMINI_MODEL_NAME # Ensure GEMINI_MODEL_NAME is defined\n",
    "        results_gemini_ins_reg = train_evaluate_dp_model(\n",
    "            config=gemini_config_ins_reg,\n",
    "            run_name=\"Insurance Gemini DP Regression (Log-Target)\",\n",
    "            train_loader=train_loader_ins,\n",
    "            test_loader=test_loader_ins,\n",
    "            n_features=n_features_ins,\n",
    "            device=device,\n",
    "            epochs=EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            task_type=\"regression\",\n",
    "            pos_weight_tensor=None,\n",
    "            y_test_original_scale_for_eval=y_test_original_scale_ins # Pass original y_test\n",
    "        )\n",
    "        if results_gemini_ins_reg:\n",
    "            results_list.append(results_gemini_ins_reg)\n",
    "    else:\n",
    "        print(\"Failed to get config from Gemini for Insurance Regression.\")\n",
    "elif not gemini_client: # Changed from gemini_client for clarity if you use the GenerativeModel instance directly\n",
    "    print(\"Skipping Insurance Gemini run - client not initialized.\")\n",
    "else:\n",
    "    print(\"Skipping Insurance Gemini run - missing components (prompt, features, loaders, or y_test_original_scale_ins).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bac5b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Multiple Groq LLM Models for insurance_cleaned.csv ---\n",
      "\n",
      "--- Testing Groq Model: llama-3.3-70b-versatile for Insurance Regression ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 2.5, 'target_delta': 1e-05, 'max_grad_norm': 5.0, 'preprocessing_suggestions': ['Split Data', 'Impute missing values', 'One-hot encode categorical variables (sex, smoker, region)', 'Standardize features (age, bmi, children)'], 'column_sensitivity_epsilon': {'age': 0.5, 'sex': 0.8, 'bmi': 0.4, 'children': 0.6, 'smoker': 0.9, 'region': 0.7}, 'reasoning': 'The chosen target epsilon of 2.5 balances the need for accurate predictions in the healthcare/finance domain with the sensitivity of the data. A target delta of 1e-5 is selected to provide a strong privacy guarantee. The max grad norm of 5.0 is moderate, considering the scale of the target variable and the standardized features. Column sensitivity epsilons are assigned based on the relative sensitivity of each feature, with higher values for features that are more sensitive (e.g., sex, smoker) and lower values for less sensitive features (e.g., age, bmi).'}\n",
      "\n",
      "--- Running: Insurance Groq (llama-3.3-70b-versatile) DP Regression (Log-Target) (Task: regression) ---\n",
      "Opacus Attached. Target ε=2.50, Target δ=1.00e-05, Max Grad Norm=5.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 2.4994\n",
      "Evaluating DP Model (regression)...\n",
      "MAE (Original Scale): 12187.24, MSE (Original Scale): 296060172.85, R2 Score: -0.9070\n",
      "Insurance Groq (llama-3.3-70b-versatile) DP Regression (Log-Target) results recorded.\n",
      "\n",
      "--- Testing Groq Model: qwen-qwq-32b for Insurance Regression ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 5.0, 'target_delta': 1e-05, 'max_grad_norm': 5.0, 'preprocessing_suggestions': 'Normalize/standardize all features (age, bmi), one-hot encode categorical variables (sex, smoker, region), and ensure no target leakage. Scale features before applying DP-SGD to stabilize gradient magnitudes.', 'column_sensitivity_epsilon': {'age': 0.6, 'sex': 0.5, 'bmi': 0.8, 'children': 0.3, 'smoker': 1.0, 'region': 0.2}, 'reasoning': \"Target epsilon set to 5.0 balances healthcare/finance privacy needs with regression utility, leveraging N=1070 for robustness. Delta uses 1e-5 to minimize privacy risk below 1/N (≈0.00093). Max_grad_norm=5.0 accommodates charges' scale post-normalization while preventing extreme gradient spikes. Column sensitivities prioritize smoker (1.0) and BMI (0.8) as high-risk features, while region (0.2) is least sensitive. Scaling ensures gradients are stable for DP-SGD, critical for minimizing MAE/MSE in skewed targets.\"}\n",
      "\n",
      "--- Running: Insurance Groq (qwen-qwq-32b) DP Regression (Log-Target) (Task: regression) ---\n",
      "Opacus Attached. Target ε=5.00, Target δ=1.00e-05, Max Grad Norm=5.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 4.9922\n",
      "Evaluating DP Model (regression)...\n",
      "MAE (Original Scale): 12262.23, MSE (Original Scale): 298213392.18, R2 Score: -0.9209\n",
      "Insurance Groq (qwen-qwq-32b) DP Regression (Log-Target) results recorded.\n",
      "\n",
      "--- Testing Groq Model: gemma2-9b-it for Insurance Regression ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 3.0, 'target_delta': 1e-05, 'max_grad_norm': 5.0, 'preprocessing_suggestions': ['Normalize all features (age, sex, bmi, children, smoker, region) to have zero mean and unit variance.', 'Consider using a robust scaling method like RobustScaler to handle potential outliers.'], 'column_sensitivity_epsilon': {'age': 0.5, 'sex': 0.1, 'bmi': 0.3, 'children': 0.2, 'smoker': 0.4, 'region': 0.3}, 'reasoning': \"The chosen DP settings aim to balance privacy and utility for this healthcare cost prediction task.  \\n\\n* **`target_epsilon` = 3.0**: This value provides a reasonable level of privacy protection while allowing for accurate model training. In the healthcare domain, where data sensitivity is high, a slightly higher epsilon value is justified to ensure model performance. \\n\\n* **`target_delta` = 1e-5**: This small value ensures a very low probability of violating privacy. \\n\\n* **`max_grad_norm` = 5.0**: This moderate clipping value helps stabilize gradients during training, preventing excessively large updates that could lead to overfitting or instability. The value is chosen considering the potential scale of the target variable 'charges'. \\n\\n* **`column_sensitivity_epsilon`**: These values are conceptual estimates of the relative sensitivity of each feature to changes in the model parameters. They are used to fine-tune the privacy budget allocation across different features. The healthcare domain often involves sensitive personal information, so features like 'sex', 'smoker', and 'region' are assigned higher sensitivity values. \\n\\nThe preprocessing suggestions are crucial for linear regression models. Normalization ensures that all features contribute equally to the model training process, preventing features with larger scales from dominating the learning process. Robust scaling helps mitigate the impact of potential outliers, which can disproportionately influence model parameters.\"}\n",
      "\n",
      "--- Running: Insurance Groq (gemma2-9b-it) DP Regression (Log-Target) (Task: regression) ---\n",
      "Opacus Attached. Target ε=3.00, Target δ=1.00e-05, Max Grad Norm=5.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 2.9924\n",
      "Evaluating DP Model (regression)...\n",
      "MAE (Original Scale): 12496.85, MSE (Original Scale): 306996002.80, R2 Score: -0.9774\n",
      "Insurance Groq (gemma2-9b-it) DP Regression (Log-Target) results recorded.\n",
      "\n",
      "--- Testing Groq Model: deepseek-r1-distill-llama-70b for Insurance Regression ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 2.0, 'target_delta': '1/1070', 'max_grad_norm': 5.0, 'preprocessing_suggestions': ['Normalize or standardize features before training.', 'Handle categorical variables through one-hot encoding or similar methods.', 'Split data into training and validation sets before any preprocessing.'], 'column_sensitivity_epsilon': {'age': 1.0, 'sex': 0.5, 'bmi': 1.0, 'children': 0.5, 'smoker': 0.5, 'region': 0.5}, 'reasoning': 'The target_epsilon of 2.0 balances the sensitivity of healthcare data with the need for model accuracy. A target_delta of 1/1070 ensures strong privacy guarantees relative to the dataset size. The max_grad_norm of 5.0 is chosen to accommodate the scale of the target variable and normalized features. Column sensitivity values reflect the relative importance and sensitivity of each feature, with higher values for more sensitive features like age and bmi.'}\n",
      "\n",
      "--- Running: Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression (Log-Target) (Task: regression) ---\n",
      "Opacus Attached. Target ε=2.00, Target δ=9.35e-04, Max Grad Norm=5.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 1.9949\n",
      "Evaluating DP Model (regression)...\n",
      "MAE (Original Scale): 12577.80, MSE (Original Scale): 311918010.78, R2 Score: -1.0092\n",
      "Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression (Log-Target) results recorded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Description: Loop through Groq models, get config, and run DP training for CURRENT dataset.\n",
    "\n",
    "print(f\"\\n--- Comparing Multiple Groq LLM Models for {DATA_FILE_INSURANCE} ---\")\n",
    "groq_models_to_test = [\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"qwen-qwq-32b\",\n",
    "    \"gemma2-9b-it\",\n",
    "    \"deepseek-r1-distill-llama-70b\" \n",
    "]\n",
    "required_vars_defined = True\n",
    "vars_to_check = {\n",
    "    'groq_client': 'Groq client',\n",
    "    'llm_prompt_insurance': 'LLM prompt for insurance regression',\n",
    "    'n_features_ins': 'Number of features for insurance model',\n",
    "    'train_loader_ins': 'Training data loader for insurance',\n",
    "    'test_loader_ins': 'Test data loader for insurance',\n",
    "    'y_test_original_scale_ins': 'Original scale y_test for insurance evaluation' # CRITICAL for regression\n",
    "}\n",
    "\n",
    "for var_name, desc in vars_to_check.items():\n",
    "    if var_name not in locals() or locals()[var_name] is None:\n",
    "        print(f\"Prerequisite Error: '{desc}' (variable: {var_name}) is not defined or is None.\")\n",
    "        required_vars_defined = False\n",
    "\n",
    "if not required_vars_defined:\n",
    "    print(\"Skipping Groq model comparison for Insurance Regression due to missing prerequisites.\")\n",
    "else:\n",
    "    for groq_model_id in groq_models_to_test:\n",
    "        # Using a clear run name including the dataset, LLM, and model ID\n",
    "        run_name_groq_dp_ins_reg = f\"Insurance Groq ({groq_model_id}) DP Regression (Log-Target)\"\n",
    "        print(f\"\\n--- Testing Groq Model: {groq_model_id} for Insurance Regression ---\")\n",
    "\n",
    "        current_groq_config_ins_reg = get_groq_config(llm_prompt_insurance, groq_client, groq_model_id)\n",
    "\n",
    "        if current_groq_config_ins_reg:\n",
    "            current_groq_config_ins_reg[\"llm_model_name\"] = groq_model_id # Store specific model\n",
    "\n",
    "            results_current_groq_ins_reg = train_evaluate_dp_model(\n",
    "                config=current_groq_config_ins_reg,\n",
    "                run_name=run_name_groq_dp_ins_reg,\n",
    "                train_loader=train_loader_ins,\n",
    "                test_loader=test_loader_ins, # This loader yields (X_test, y_test_log)\n",
    "                n_features=n_features_ins,\n",
    "                device=device,\n",
    "                epochs=EPOCHS,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                task_type=\"regression\", # Specify regression\n",
    "                pos_weight_tensor=None, # Not used for regression\n",
    "                y_test_original_scale_for_eval=y_test_original_scale_ins # Crucial for regression metrics\n",
    "            )\n",
    "\n",
    "            if results_current_groq_ins_reg:\n",
    "                # The train_evaluate_dp_model should now return regression-specific metrics like MAE, MSE, R2\n",
    "                # The results_df cell will handle missing classification columns later.\n",
    "                results_list.append(results_current_groq_ins_reg)\n",
    "        else:\n",
    "            print(f\"Failed to get config from Groq model {groq_model_id} for Insurance Regression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d99065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined Experiment Results (Including Insurance Regression) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>Precision (Readmit)</th>\n",
       "      <th>Recall (Readmit)</th>\n",
       "      <th>F1 (Readmit)</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insurance Non-DP Regression (Log-Target)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12114.291913</td>\n",
       "      <td>3.343694e+08</td>\n",
       "      <td>-1.153766</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insurance Fixed DP Regression</td>\n",
       "      <td>N/A (Fixed Defaults)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991737</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>7.375744</td>\n",
       "      <td>5.551144e+01</td>\n",
       "      <td>-60.758069</td>\n",
       "      <td>Using standard fixed parameters for Insurance Regression.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insurance Gemini DP Regression</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.499418</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>3.854968</td>\n",
       "      <td>1.631756e+01</td>\n",
       "      <td>-17.153749</td>\n",
       "      <td>{'target_epsilon': 'A `target_epsilon` of 2.5 is chosen to balance the sensitivity of 'Healthcar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insurance Fixed DP Regression (Log-Target)</td>\n",
       "      <td>N/A (Fixed Defaults)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991737</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12965.510338</td>\n",
       "      <td>3.233258e+08</td>\n",
       "      <td>-1.082631</td>\n",
       "      <td>Using standard fixed parameters for Insurance Regression (Log-Target).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Insurance Gemini DP Regression (Log-Target)</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.992385</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12836.516386</td>\n",
       "      <td>3.195565e+08</td>\n",
       "      <td>-1.058352</td>\n",
       "      <td>The recommended DP settings are tailored to the Kaggle Medical Cost Personal dataset for predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Insurance Groq (llama-3.3-70b-versatile) DP Regression (Log-Target)</td>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.499418</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12187.239588</td>\n",
       "      <td>2.960602e+08</td>\n",
       "      <td>-0.907005</td>\n",
       "      <td>The chosen target epsilon of 2.5 balances the need for accurate predictions in the healthcare/fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Insurance Groq (qwen-qwq-32b) DP Regression (Log-Target)</td>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.992243</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12262.226502</td>\n",
       "      <td>2.982134e+08</td>\n",
       "      <td>-0.920875</td>\n",
       "      <td>Target epsilon set to 5.0 balances healthcare/finance privacy needs with regression utility, lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Insurance Groq (gemma2-9b-it) DP Regression (Log-Target)</td>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.992385</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12496.854989</td>\n",
       "      <td>3.069960e+08</td>\n",
       "      <td>-0.977446</td>\n",
       "      <td>The chosen DP settings aim to balance privacy and utility for this healthcare cost prediction ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression (Log-Target)</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.994851</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12577.803593</td>\n",
       "      <td>3.119180e+08</td>\n",
       "      <td>-1.009150</td>\n",
       "      <td>The target_epsilon of 2.0 balances the sensitivity of healthcare data with the need for model ac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm LLM Epsilon Suggestion Accuracy Precision (Stroke) Recall (Stroke) F1 (Stroke) Precision (Readmit) Recall (Readmit) F1 (Readmit)           MAE           MSE   R2 Score                                                                                        LLM Reasoning\n",
       "0                                   Insurance Non-DP Regression (Log-Target)                            N/A            N/A           N/A          N/A           N/A                    N/A      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12114.291913  3.343694e+08  -1.153766                                                                                                  N/A\n",
       "1                                              Insurance Fixed DP Regression           N/A (Fixed Defaults)            1.0      0.991737     0.000935           1.0                    1.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A      7.375744  5.551144e+01 -60.758069                                            Using standard fixed parameters for Insurance Regression.\n",
       "2                                             Insurance Gemini DP Regression       gemini-2.5-pro-exp-03-25            2.5      2.499418      0.00001           5.0                    2.5      N/A                N/A             N/A         N/A                 N/A              N/A          N/A      3.854968  1.631756e+01 -17.153749  {'target_epsilon': 'A `target_epsilon` of 2.5 is chosen to balance the sensitivity of 'Healthcar...\n",
       "3                                 Insurance Fixed DP Regression (Log-Target)           N/A (Fixed Defaults)            1.0      0.991737     0.000935           1.0                    1.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12965.510338  3.233258e+08  -1.082631                               Using standard fixed parameters for Insurance Regression (Log-Target).\n",
       "4                                Insurance Gemini DP Regression (Log-Target)       gemini-2.5-pro-exp-03-25            3.0      2.992385      0.00001           4.0                    3.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12836.516386  3.195565e+08  -1.058352  The recommended DP settings are tailored to the Kaggle Medical Cost Personal dataset for predict...\n",
       "5        Insurance Groq (llama-3.3-70b-versatile) DP Regression (Log-Target)        llama-3.3-70b-versatile            2.5      2.499418      0.00001           5.0                    2.5      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12187.239588  2.960602e+08  -0.907005  The chosen target epsilon of 2.5 balances the need for accurate predictions in the healthcare/fi...\n",
       "6                   Insurance Groq (qwen-qwq-32b) DP Regression (Log-Target)                   qwen-qwq-32b            5.0      4.992243      0.00001           5.0                    5.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12262.226502  2.982134e+08  -0.920875  Target epsilon set to 5.0 balances healthcare/finance privacy needs with regression utility, lev...\n",
       "7                   Insurance Groq (gemma2-9b-it) DP Regression (Log-Target)                   gemma2-9b-it            3.0      2.992385      0.00001           5.0                    3.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12496.854989  3.069960e+08  -0.977446  The chosen DP settings aim to balance privacy and utility for this healthcare cost prediction ta...\n",
       "8  Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression (Log-Target)  deepseek-r1-distill-llama-70b            2.0      1.994851     0.000935           5.0                    2.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12577.803593  3.119180e+08  -1.009150  The target_epsilon of 2.0 balances the sensitivity of healthcare data with the need for model ac..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Description: Show the results from ALL runs in a table.\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    # Define desired column order, including ALL possible metrics\n",
    "    # Order them logically\n",
    "    cols_order = [\n",
    "        \"Run Type\", \"LLM Used\",\n",
    "        \"Target Epsilon\", \"Final Epsilon\", \"Target Delta\", \"Max Grad Norm\",\n",
    "        \"LLM Epsilon Suggestion\", # Moved up for easier comparison with Target Epsilon\n",
    "        # Classification Metrics\n",
    "        \"Accuracy\", \"Precision (Stroke)\", \"Recall (Stroke)\", \"F1 (Stroke)\",\n",
    "        \"Precision (Readmit)\", \"Recall (Readmit)\", \"F1 (Readmit)\",\n",
    "        # Regression Metrics\n",
    "        \"MAE\", \"MSE\", \"R2 Score\",\n",
    "        # LLM Reasoning last as it can be long\n",
    "        \"LLM Reasoning\"\n",
    "    ]\n",
    "    # Ensure all expected columns exist, add if missing (fill with N/A or np.nan)\n",
    "    for col in cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = np.nan # Use np.nan for numerical/potentially numerical\n",
    "\n",
    "    results_df = results_df[cols_order]\n",
    "    # Fill NaNs that might have occurred if some runs failed or metrics weren't applicable\n",
    "    results_df.fillna(\"N/A\", inplace=True) # Replace np.nan with \"N/A\" string for display\n",
    "\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', 1200) # Wider for more columns\n",
    "    print(\"\\n--- Combined Experiment Results (Including Insurance Regression) ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results recorded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bac37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.drop([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2af97ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>Precision (Readmit)</th>\n",
       "      <th>Recall (Readmit)</th>\n",
       "      <th>F1 (Readmit)</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insurance Non-DP Regression (Log-Target)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12114.291913</td>\n",
       "      <td>3.343694e+08</td>\n",
       "      <td>-1.153766</td>\n",
       "      <td>N/A</td>\n",
       "      <td>insurance_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insurance Fixed DP Regression (Log-Target)</td>\n",
       "      <td>N/A (Fixed Defaults)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991737</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12965.510338</td>\n",
       "      <td>3.233258e+08</td>\n",
       "      <td>-1.082631</td>\n",
       "      <td>Using standard fixed parameters for Insurance Regression (Log-Target).</td>\n",
       "      <td>insurance_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Insurance Gemini DP Regression (Log-Target)</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.992385</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12836.516386</td>\n",
       "      <td>3.195565e+08</td>\n",
       "      <td>-1.058352</td>\n",
       "      <td>The recommended DP settings are tailored to the Kaggle Medical Cost Personal dataset for predict...</td>\n",
       "      <td>insurance_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Insurance Groq (llama-3.3-70b-versatile) DP Regression (Log-Target)</td>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.499418</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12187.239588</td>\n",
       "      <td>2.960602e+08</td>\n",
       "      <td>-0.907005</td>\n",
       "      <td>The chosen target epsilon of 2.5 balances the need for accurate predictions in the healthcare/fi...</td>\n",
       "      <td>insurance_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Insurance Groq (qwen-qwq-32b) DP Regression (Log-Target)</td>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.992243</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12262.226502</td>\n",
       "      <td>2.982134e+08</td>\n",
       "      <td>-0.920875</td>\n",
       "      <td>Target epsilon set to 5.0 balances healthcare/finance privacy needs with regression utility, lev...</td>\n",
       "      <td>insurance_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Insurance Groq (gemma2-9b-it) DP Regression (Log-Target)</td>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.992385</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12496.854989</td>\n",
       "      <td>3.069960e+08</td>\n",
       "      <td>-0.977446</td>\n",
       "      <td>The chosen DP settings aim to balance privacy and utility for this healthcare cost prediction ta...</td>\n",
       "      <td>insurance_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression (Log-Target)</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.994851</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12577.803593</td>\n",
       "      <td>3.119180e+08</td>\n",
       "      <td>-1.009150</td>\n",
       "      <td>The target_epsilon of 2.0 balances the sensitivity of healthcare data with the need for model ac...</td>\n",
       "      <td>insurance_cleaned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm LLM Epsilon Suggestion Accuracy Precision (Stroke) Recall (Stroke) F1 (Stroke) Precision (Readmit) Recall (Readmit) F1 (Readmit)           MAE           MSE  R2 Score                                                                                        LLM Reasoning            dataset\n",
       "0                                   Insurance Non-DP Regression (Log-Target)                            N/A            N/A           N/A          N/A           N/A                    N/A      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12114.291913  3.343694e+08 -1.153766                                                                                                  N/A  insurance_cleaned\n",
       "3                                 Insurance Fixed DP Regression (Log-Target)           N/A (Fixed Defaults)            1.0      0.991737     0.000935           1.0                    1.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12965.510338  3.233258e+08 -1.082631                               Using standard fixed parameters for Insurance Regression (Log-Target).  insurance_cleaned\n",
       "4                                Insurance Gemini DP Regression (Log-Target)       gemini-2.5-pro-exp-03-25            3.0      2.992385      0.00001           4.0                    3.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12836.516386  3.195565e+08 -1.058352  The recommended DP settings are tailored to the Kaggle Medical Cost Personal dataset for predict...  insurance_cleaned\n",
       "5        Insurance Groq (llama-3.3-70b-versatile) DP Regression (Log-Target)        llama-3.3-70b-versatile            2.5      2.499418      0.00001           5.0                    2.5      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12187.239588  2.960602e+08 -0.907005  The chosen target epsilon of 2.5 balances the need for accurate predictions in the healthcare/fi...  insurance_cleaned\n",
       "6                   Insurance Groq (qwen-qwq-32b) DP Regression (Log-Target)                   qwen-qwq-32b            5.0      4.992243      0.00001           5.0                    5.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12262.226502  2.982134e+08 -0.920875  Target epsilon set to 5.0 balances healthcare/finance privacy needs with regression utility, lev...  insurance_cleaned\n",
       "7                   Insurance Groq (gemma2-9b-it) DP Regression (Log-Target)                   gemma2-9b-it            3.0      2.992385      0.00001           5.0                    3.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12496.854989  3.069960e+08 -0.977446  The chosen DP settings aim to balance privacy and utility for this healthcare cost prediction ta...  insurance_cleaned\n",
       "8  Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression (Log-Target)  deepseek-r1-distill-llama-70b            2.0      1.994851     0.000935           5.0                    2.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12577.803593  3.119180e+08 -1.009150  The target_epsilon of 2.0 balances the sensitivity of healthcare data with the need for model ac...  insurance_cleaned"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['dataset'] = DATA_FILE_INSURANCE.split('.')[0]\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20c9d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index()\n",
    "results_df.to_csv('regression_multiple_llm.csv', index_label='SrNo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0f487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diabetes Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.606169</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diabetes Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.004022</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.595428</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Recommendations are tailored for DP-SGD training of a Logistic Regression model on the UCI Diabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Groq (deepseek-r1-distill-llama-70b) DP SGD</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.997348</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.595153</td>\n",
       "      <td>0.616633</td>\n",
       "      <td>0.502479</td>\n",
       "      <td>0.553734</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diabetes Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.004022</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.608923</td>\n",
       "      <td>0.628164</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.576877</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The recommended DP settings balance the sensitivity of healthcare data with the need for model u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.604792</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>0.511295</td>\n",
       "      <td>0.563962</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm  Accuracy Precision (Stroke) Recall (Stroke) F1 (Stroke) LLM Epsilon Suggestion                                                                                        LLM Reasoning\n",
       "0                          Diabetes Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.606169                N/A             N/A         N/A                    N/A                                                                                                  N/A\n",
       "1                       Diabetes Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      3.004022      0.00001           1.5  0.595428                N/A             N/A         N/A                    3.0  Recommendations are tailored for DP-SGD training of a Logistic Regression model on the UCI Diabe...\n",
       "2  Groq (deepseek-r1-distill-llama-70b) DP SGD  deepseek-r1-distill-llama-70b            2.0      1.997348      0.00001          10.0  0.595153           0.616633        0.502479    0.553734                    2.0  The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...\n",
       "3                       Diabetes Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      3.004022      0.00001           7.5  0.608923           0.628164        0.533333    0.576877                    3.0  The recommended DP settings balance the sensitivity of healthcare data with the need for model u...\n",
       "4                          Diabetes Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.604792           0.628726        0.511295    0.563962                    N/A                                                                                                  N/A"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Description: Show the results from all runs in a table.\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    # Reorder columns for clarity\n",
    "    cols_order = [\n",
    "        \"Run Type\", \"LLM Used\", \"Target Epsilon\", \"Final Epsilon\", \"Target Delta\",\n",
    "        \"Max Grad Norm\", \"Accuracy\", \"Precision (Stroke)\", \"Recall (Stroke)\", \"F1 (Stroke)\",\n",
    "        \"LLM Epsilon Suggestion\", \"LLM Reasoning\"\n",
    "    ]\n",
    "    # Ensure all expected columns exist, add if missing\n",
    "    for col in cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = \"N/A\"\n",
    "\n",
    "    results_df = results_df[cols_order]\n",
    "    pd.set_option('display.max_colwidth', 100) # Show more of the reasoning column\n",
    "    pd.set_option('display.width', 1000) # Adjust display width\n",
    "    print(\"\\n--- Combined Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results recorded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "100a18c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Groq (deepseek-r1-distill-llama-70b) DP SGD</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.997348</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.595153</td>\n",
       "      <td>0.616633</td>\n",
       "      <td>0.502479</td>\n",
       "      <td>0.553734</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diabetes Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.004022</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.608923</td>\n",
       "      <td>0.628164</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.576877</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The recommended DP settings balance the sensitivity of healthcare data with the need for model u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.604792</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>0.511295</td>\n",
       "      <td>0.563962</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm  Accuracy Precision (Stroke) Recall (Stroke) F1 (Stroke) LLM Epsilon Suggestion                                                                                        LLM Reasoning\n",
       "2  Groq (deepseek-r1-distill-llama-70b) DP SGD  deepseek-r1-distill-llama-70b            2.0      1.997348      0.00001          10.0  0.595153           0.616633        0.502479    0.553734                    2.0  The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...\n",
       "3                       Diabetes Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      3.004022      0.00001           7.5  0.608923           0.628164        0.533333    0.576877                    3.0  The recommended DP settings balance the sensitivity of healthcare data with the need for model u...\n",
       "4                          Diabetes Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.604792           0.628726        0.511295    0.563962                    N/A                                                                                                  N/A"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.drop(0, axis=0, inplace=True)\n",
    "results_df.drop(1, axis=0, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e8a5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0741e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec24248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
