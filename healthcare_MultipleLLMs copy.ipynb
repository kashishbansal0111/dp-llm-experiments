{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd545ae",
   "metadata": {},
   "source": [
    "### Comparing Groq model and Gemini 2.5 pro on UCI Diabetes Dataset\n",
    "- Let's see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88ef565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rest of imports and setup from previous Cell 1 ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Opacus\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01b7cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Groq client initialized successfully for model: deepseek-r1-distill-llama-70b\n",
      "Gemini client initialized successfully for model: gemini-2.5-pro-exp-03-25\n",
      "\n",
      "Basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Description: Import libraries, set up constants, initialize/manage results list.\n",
    "\n",
    "# --- Make sure results_list is either reset or you append with clear names ---\n",
    "# Option 1: Reset for a fresh run on this dataset\n",
    "# results_list = []\n",
    "# Option 2: Keep previous results (make sure Run Type names are distinct)\n",
    "#print(f\"Starting Diabetes run. Current results count: {len(results_list)}\")\n",
    "\n",
    "# --- Constants for Diabetes ---\n",
    "DATA_FILE_DIABETES = 'diabetic_data.csv' # Assuming this is the filename\n",
    "TARGET_COLUMN_DIABETES = 'readmitted_binary' # Our new binary target\n",
    "\n",
    "DATA_FILE_INSURANCE = 'insurance_cleaned.csv' # YOUR FILENAME HERE\n",
    "TARGET_COLUMN_INSURANCE = 'charges' # Target for regression\n",
    "\n",
    "# LLM Clients\n",
    "from groq import Groq\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# --- Base Training Hyperparameters (can be reused) ---\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# --- Default DP Parameters (will be recalculated) ---\n",
    "DEFAULT_TARGET_EPSILON = 1.0\n",
    "DEFAULT_TARGET_DELTA = 1e-5 # Placeholder, recalculate based on N\n",
    "DEFAULT_MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- LLM Clients Initialization (reuse from previous) ---\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\", \"llama3-70b-8192\")\n",
    "GEMINI_MODEL_NAME = os.getenv(\"GEMINI_MODEL_NAME\", \"gemini-1.5-flash\")\n",
    "\n",
    "groq_client = None\n",
    "gemini_client = None\n",
    "# ... (rest of client initialization logic from previous Cell 1) ...\n",
    "if GROQ_API_KEY:\n",
    "    try:\n",
    "        groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "        print(f\"Groq client initialized successfully for model: {GROQ_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Groq client: {e}\")\n",
    "else:\n",
    "    print(\"Warning: GROQ_API_KEY not found. Groq LLM will not be used.\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    try:\n",
    "        gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        print(f\"Gemini client initialized successfully for model: {GEMINI_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Gemini client: {e}\")\n",
    "else:\n",
    "    print(\"Warning: GEMINI_API_KEY not found. Gemini LLM will not be used.\")\n",
    "\n",
    "# --- LLM Helper Functions (reuse from previous Cell 6) ---\n",
    "# Includes: create_llm_prompt, get_gemini_config, get_groq_config\n",
    "# Make sure create_llm_prompt is the latest version including ML context\n",
    "\n",
    "# --- DP Training Function (reuse from previous Cell 7) ---\n",
    "# Includes: train_evaluate_dp_model (ensure it handles classification)\n",
    "\n",
    "# --- Logistic Regression Model Class (reuse from previous Cell 4) ---\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "print(\"\\nBasic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c0d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_models_to_test = [\n",
    "    \"llama3-70b-8192\",\n",
    "    \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b-it\" # Add other available/suitable models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d77a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0c250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73cb8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Description: Define the standard prompt structure and functions to call LLMs.\n",
    "\n",
    "# # v2\n",
    "# def create_llm_prompt(task_config, schema_string, data_shape):\n",
    "#     \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices.\"\"\"\n",
    "#     prompt = f\"\"\"\n",
    "# Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a Logistic Regression model using DP-SGD.\n",
    "# The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "# **Dataset Context:**\n",
    "# - Name: {task_config['dataset_name']}\n",
    "# - Domain: {task_config['data_domain']} (Note: Healthcare data is generally considered sensitive).\n",
    "# - Task: {task_config['task_description']}\n",
    "# - Schema (Original Columns): {schema_string}\n",
    "# - Extra details: {task_config['details']} (Pay close attention to class imbalance).\n",
    "\n",
    "# **Parameter Guidance - IMPORTANT:** Avoid generic default values. Base your recommendations *specifically* on the context provided above.\n",
    "\n",
    "# Provide your recommendations ONLY in a structured JSON format. The JSON object must include the following keys:\n",
    "# - \"dp_algorithm\": String, the specific DP algorithm variant recommended (e.g., \"DP-SGD with Gaussian Noise\").\n",
    "# - \"target_epsilon\": Float, recommended privacy budget epsilon (e.g., 1.5). Justify this based on sensitivity, utility needs, and domain.\n",
    "# - \"target_delta\": Float or String, recommended privacy budget delta (e.g., 1e-5 or suggest calculating as \"1/N\"). Justify choice.\n",
    "# - \"max_grad_norm\": Float, recommended gradient clipping norm (e.g., 1.0). Justify based on model stability and potential gradient explosion, especially considering class imbalance if noted.\n",
    "# - \"preprocessing_suggestions\": List of strings, specific preprocessing actions recommended BEFORE applying DP (e.g., \"Remove: id\", \"Normalize: age, avg_glucose_level, bmi\").\n",
    "# - \"column_sensitivity_epsilon\": A dictionary where keys are original column names and values are *conceptual* relative sensitivity floats (0.0=low, 1.0=high/ID) or labels (Low, Medium, High). This guides understanding, not direct budget split in standard DP-SGD. Exclude the target variable.\n",
    "# - \"reasoning\": String, concise reasoning behind the overall recommendations (epsilon, delta, max_grad_norm choices, linking back to context).\n",
    "\n",
    "# JSON Output ONLY:\n",
    "# \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "# #v3\n",
    "# def create_llm_prompt(task_config, schema_string, data_shape):\n",
    "#     \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices\n",
    "#        and including general ML best practices context.\"\"\"\n",
    "\n",
    "#     # Calculate approximate training size N for context\n",
    "#     approx_N_train = int(data_shape[0] * (1-TEST_SIZE)) if data_shape else 'Unknown'\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a Logistic Regression model using DP-SGD.\n",
    "# The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "# **Dataset Context:**\n",
    "# - Name: {task_config['dataset_name']}\n",
    "# - Domain: {task_config['data_domain']} (Note: Healthcare data is generally considered sensitive).\n",
    "# - Task: {task_config['task_description']}\n",
    "# - Schema (Original Columns): {schema_string}\n",
    "# - Data Shape: {data_shape} (Approx. Training N = {approx_N_train})\n",
    "# - Extra details: {task_config['details']} (Pay close attention to class imbalance).\n",
    "\n",
    "# **General ML Best Practices Context (Keep these in mind):**\n",
    "# - **Normalization/Scaling:** Features with different scales (like 'age' vs 'avg_glucose_level') MUST be normalized or standardized (e.g., StandardScaler, MinMaxScaler) for models like Logistic Regression and especially before applying gradient clipping in DP-SGD. This ensures stable gradient computations. Apply scaling AFTER splitting data and ideally AFTER imputation if applicable.\n",
    "# - **Gradient Stability & Clipping:** DP-SGD uses gradient clipping (`max_grad_norm`) to bound the influence of any single data point. Choosing the norm value is a trade-off:\n",
    "#     - Too low: Clips potentially useful gradient information, slowing learning or preventing convergence, especially for minority classes or complex patterns.\n",
    "#     - Too high: Less protection against outliers, potentially higher noise required for the same privacy budget (ε).\n",
    "#     - Imbalanced Data Impact: Gradients from rare class samples might be infrequent but large; aggressive clipping can disproportionately affect learning for that class.\n",
    "# - **Imbalanced Data Handling:** Beyond class weighting in the loss (which is assumed here), model evaluation should focus on metrics like F1-score, Precision, Recall for the minority class, not just accuracy. The goal is often to improve detection of the rare class.\n",
    "# - **Preprocessing Order:** Typically: Split Data -> Impute Missing -> Encode Categorical -> Scale Numerical -> Train Model. DP is applied during the training step.\n",
    "\n",
    "# **Parameter Guidance - IMPORTANT:** Based on the Dataset Context AND the ML Best Practices above, provide specific, justified recommendations. Avoid generic defaults.\n",
    "\n",
    "# 1.  **`target_epsilon`**: Balance '{task_config['data_domain']}' sensitivity vs. utility needed for training. Justify the specific trade-off. (Range 1.0-5.0 often considered, but justify *your* choice).\n",
    "# 2.  **`target_delta`**: Recommend a specific small value (e.g., 1e-5, 1e-6) or suggest \"1/N\". Justify why (e.g., related to approx N={approx_N_train}).\n",
    "# 3.  **`max_grad_norm`**: **Connect this directly to the Gradient Stability & Imbalanced Data points above.** Given the heavy imbalance, suggest a value (e.g., range 5.0 - 15.0, or a specific reasoned value) likely higher than a generic default (like 1.0) to preserve minority class signals. Justify based *explicitly* on imbalance and the need for stable yet informative gradients.\n",
    "# 4.  **`column_sensitivity_epsilon`**: Conceptual relative sensitivity hints (0.0 low, 1.0 high). Reflect potential identifiability/sensitivity based on domain/name. Exclude target.\n",
    "# 5.  **`reasoning`**: Concise but detailed justification for epsilon, delta, and max_grad_norm, *explicitly linking* choices to dataset context (domain, N, imbalance) and the relevant ML best practices mentioned (normalization, gradient stability).\n",
    "\n",
    "# **Output Format:**\n",
    "# Provide recommendations ONLY in a structured JSON format with keys: \"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\".\n",
    "\n",
    "# JSON Output ONLY:\n",
    "# \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "# Description: Define the standard prompt structure and functions to call LLMs.\n",
    "\n",
    "def create_llm_prompt(task_config, schema_string, data_shape, task_type=\"classification\"): # Add task_type\n",
    "    \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices\n",
    "       and including general ML best practices context. Adapts for task_type.\"\"\"\n",
    "\n",
    "    approx_N_train = int(data_shape[0] * (1-TEST_SIZE)) if data_shape else 'Unknown'\n",
    "\n",
    "    # Adjust parts of the prompt based on task_type\n",
    "    if task_type == \"regression\":\n",
    "        imbalance_guidance = \"\" # No class imbalance for regression\n",
    "        target_guidance_metrics = \"metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or R2 Score. The goal is often to minimize error.\"\n",
    "        max_grad_norm_focus = \"focus on overall gradient stability, considering the range and scale of the target variable values, rather than specific class signals. A moderate value (e.g., 1.0-10.0, depending on target scale and feature normalization) is common.\"\n",
    "        epsilon_utility_focus = \"utility needs for accurate predictions (e.g., low MAE/MSE)\"\n",
    "        model_type_in_prompt = \"Linear Regression\" # Or generic \"Regression Model\"\n",
    "    else: # classification (default)\n",
    "        imbalance_guidance = \"(Pay close attention to class imbalance if mentioned in 'Extra details').\"\n",
    "        target_guidance_metrics = \"metrics like F1-score, Precision, Recall for the minority class, not just accuracy. The goal is often to improve detection of the rare class.\"\n",
    "        max_grad_norm_focus = \"**Connect this directly to the Gradient Stability & Imbalanced Data points above.** Given potential class imbalance (see 'Extra details'), suggest a value (e.g., range 5.0 - 15.0, or a specific reasoned value) likely higher than a generic default (like 1.0) to preserve minority class signals. Justify based *explicitly* on imbalance and the need for stable yet informative gradients.\"\n",
    "        epsilon_utility_focus = \"utility needs for model training (which often requires sufficient signal)\"\n",
    "        model_type_in_prompt = \"Logistic Regression\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a {model_type_in_prompt} model using DP-SGD.\n",
    "The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "**Dataset Context:**\n",
    "- Name: {task_config['dataset_name']}\n",
    "- Domain: {task_config['data_domain']}\n",
    "- Task: {task_config['task_description']}\n",
    "- Schema (Original Columns): {schema_string}\n",
    "- Data Shape: {data_shape} (Approx. Training N = {approx_N_train})\n",
    "- Extra details: {task_config['details']} {imbalance_guidance}\n",
    "\n",
    "**General ML Best Practices Context (Keep these in mind):**\n",
    "- **Normalization/Scaling:** Features MUST be normalized or standardized for models like {model_type_in_prompt} and especially before applying gradient clipping in DP-SGD.\n",
    "- **Gradient Stability & Clipping (`max_grad_norm`):** Bound influence of single points. Trade-off:\n",
    "    - Too low: Clips useful info, hinders learning.\n",
    "    - Too high: Less protection, more noise needed.\n",
    "    - { \"Imbalanced Data Impact: Gradients from rare class samples might be infrequent but large; aggressive clipping can disproportionately affect learning for that class.\" if task_type==\"classification\" else \"For regression, consider the scale of the target variable when thinking about gradient magnitudes.\"}\n",
    "- **Evaluation Focus ({task_type}):** Model evaluation should focus on {target_guidance_metrics}\n",
    "- **Preprocessing Order:** Typically: Split Data -> Impute -> Encode -> Scale -> Train.\n",
    "\n",
    "**Parameter Guidance - IMPORTANT:** Base recommendations *specifically* on context. Avoid generic defaults.\n",
    "\n",
    "1.  **`target_epsilon`**: Balance '{task_config['data_domain']}' sensitivity vs. {epsilon_utility_focus}. Justify. (Range 1.0-5.0 often considered, but justify *your* choice).\n",
    "2.  **`target_delta`**: Recommend small value (e.g., 1e-5) or \"1/N\". Justify (e.g., N={approx_N_train}).\n",
    "3.  **`max_grad_norm`**: For this {task_type} task, {max_grad_norm_focus} Justify.\n",
    "4.  **`column_sensitivity_epsilon`**: Conceptual relative sensitivity hints (0.0 low, 1.0 high). Exclude target.\n",
    "5.  **`reasoning`**: Detailed justification for epsilon, delta, `max_grad_norm`, linking to dataset context (domain, N, imbalance/target scale) and ML practices.\n",
    "\n",
    "**Output Format:**\n",
    "JSON ONLY with keys: \"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\".\n",
    "\n",
    "JSON Output ONLY:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "def get_gemini_config(prompt, client):\n",
    "    \"\"\"Gets DP config from Gemini API.\"\"\"\n",
    "    if not client:\n",
    "        print(\"Gemini client not available.\")\n",
    "        return None\n",
    "    print(\"\\nSending request to Gemini...\")\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=GEMINI_MODEL_NAME, contents=prompt\n",
    "            )\n",
    "        response_text = response.text\n",
    "        print(\"Gemini Response Received.\")\n",
    "        # Extract JSON part\n",
    "        start_index = response_text.find('{')\n",
    "        end_index = response_text.rfind('}')\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_string_only = response_text[start_index : end_index + 1]\n",
    "            config = json.loads(json_string_only)\n",
    "            print(\"Successfully parsed Gemini config.\")\n",
    "            print(config)\n",
    "            # Basic validation\n",
    "            required_keys = [\"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\"]\n",
    "            if not all(key in config for key in required_keys):\n",
    "                print(\"Warning: Gemini response missing some required keys.\")\n",
    "            return config\n",
    "        else:\n",
    "            print(\"Error: Could not find JSON object in Gemini response.\")\n",
    "            print(\"Raw Response:\", response_text)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call or parsing: {e}\")\n",
    "        try:\n",
    "            print(\"Gemini Response Content (if available):\", response.candidates) # Might show safety blocks\n",
    "        except: pass\n",
    "        return None\n",
    "\n",
    "def get_groq_config(prompt, client, model_name):\n",
    "    \"\"\"Gets DP config from Groq API.\"\"\"\n",
    "    if not client:\n",
    "        print(\"Groq client not available.\")\n",
    "        return None\n",
    "    print(\"\\nSending request to Groq...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model_name,\n",
    "            temperature=0.2,\n",
    "            max_tokens=3024, \n",
    "            top_p=0.8,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "        print(\"Groq Response Received.\")\n",
    "        config = json.loads(response_content)\n",
    "        print(\"Successfully parsed Groq config.\")\n",
    "        print(config)\n",
    "\n",
    "        # Basic validation\n",
    "        required_keys = [\"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\"]\n",
    "        if not all(key in config for key in required_keys):\n",
    "            print(\"Warning: Groq response missing some required keys.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Groq API call or parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"LLM Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff6f82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Training/Evaluation function (with task_type) defined/updated.\n"
     ]
    }
   ],
   "source": [
    "# Description: Function to train and evaluate a DP model using Opacus, for classification or regression.\n",
    "\n",
    "def train_evaluate_dp_model(\n",
    "    config, run_name, train_loader, test_loader,\n",
    "    n_features, device, epochs, learning_rate,\n",
    "    task_type=\"classification\", # New parameter\n",
    "    pos_weight_tensor=None, # Only for classification\n",
    "    target_metric_prefix=\"Class 1\" # For naming metrics in results\n",
    "):\n",
    "    \"\"\"Trains and evaluates DP model, returns results dictionary.\"\"\"\n",
    "    print(f\"\\n--- Running: {run_name} (Task: {task_type}) ---\")\n",
    "    if config is None: # ... (unchanged null check) ...\n",
    "        print(\"Skipping run due to missing configuration.\")\n",
    "        return None\n",
    "\n",
    "    # ... (config key extraction, delta calculation - unchanged) ...\n",
    "    target_eps = config.get(\"target_epsilon\", DEFAULT_TARGET_EPSILON)\n",
    "    target_del_config = config.get(\"target_delta\", \"1/N\")\n",
    "    max_norm = config.get(\"max_grad_norm\", DEFAULT_MAX_GRAD_NORM)\n",
    "    llm_reasoning = config.get(\"reasoning\", \"N/A\")\n",
    "    llm_eps_suggestion = config.get(\"target_epsilon\", \"N/A (Default used)\")\n",
    "\n",
    "    if isinstance(target_del_config, str) and \"1/N\" in target_del_config:\n",
    "        actual_delta = 1 / len(train_loader.dataset)\n",
    "    elif isinstance(target_del_config, (int, float)):\n",
    "        actual_delta = target_del_config\n",
    "    else:\n",
    "        actual_delta = 1 / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    dp_model = LogisticRegression(n_features).to(device) # Model class is same\n",
    "    dp_optimizer = optim.SGD(dp_model.parameters(), lr=learning_rate)\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    try:\n",
    "        dp_model, dp_optimizer, dp_data_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=dp_model, optimizer=dp_optimizer, data_loader=train_loader,\n",
    "            max_grad_norm=max_norm, target_epsilon=target_eps, target_delta=actual_delta, epochs=epochs\n",
    "        )\n",
    "        print(f\"Opacus Attached. Target ε={target_eps:.2f}, Target δ={actual_delta:.2e}, Max Grad Norm={max_norm}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching Opacus PrivacyEngine: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Set Loss Function based on task_type ---\n",
    "    if task_type == \"classification\":\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    elif task_type == \"regression\":\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_type. Must be 'classification' or 'regression'.\")\n",
    "    # ------------------------------------------\n",
    "\n",
    "    print(f\"Training DP Model ({task_type})...\")\n",
    "    dp_model.train()\n",
    "    for epoch in range(epochs): # ... (training loop - unchanged core logic) ...\n",
    "        epoch_loss_dp = 0.0\n",
    "        for batch_X, batch_y in dp_data_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            dp_optimizer.zero_grad()\n",
    "            outputs = dp_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            dp_optimizer.step()\n",
    "            epoch_loss_dp += loss.item()\n",
    "        # print(f\"Epoch {epoch+1} Loss: {epoch_loss_dp / len(dp_data_loader):.4f}\")\n",
    "\n",
    "\n",
    "    final_epsilon = privacy_engine.get_epsilon(delta=actual_delta) # ... (unchanged) ...\n",
    "    print(f\"DP Training Complete. Final ε = {final_epsilon:.4f}\")\n",
    "\n",
    "    # --- Evaluation based on task_type ---\n",
    "    print(f\"Evaluating DP Model ({task_type})...\")\n",
    "    dp_model.eval()\n",
    "    all_preds_dp = []\n",
    "    all_targets_dp = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = dp_model(batch_X)\n",
    "            if task_type == \"classification\":\n",
    "                preds = torch.round(torch.sigmoid(outputs))\n",
    "            else: # regression\n",
    "                preds = outputs # Direct output\n",
    "            all_preds_dp.extend(preds.cpu().numpy().flatten())\n",
    "            all_targets_dp.extend(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        \"Run Type\": run_name,\n",
    "        \"LLM Used\": config.get(\"llm_model_name\", \"N/A\"),\n",
    "        \"Target Epsilon\": target_eps, \"Final Epsilon\": final_epsilon,\n",
    "        \"Target Delta\": actual_delta, \"Max Grad Norm\": max_norm,\n",
    "        \"LLM Epsilon Suggestion\": llm_eps_suggestion, \"LLM Reasoning\": llm_reasoning\n",
    "    }\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        accuracy_dp = accuracy_score(all_targets_dp, all_preds_dp)\n",
    "        precision_dp = precision_score(all_targets_dp, all_preds_dp, pos_label=1, zero_division=0)\n",
    "        recall_dp = recall_score(all_targets_dp, all_preds_dp, pos_label=1, zero_division=0)\n",
    "        f1_dp = f1_score(all_targets_dp, all_preds_dp, pos_label=1, zero_division=0)\n",
    "        print(f\"Accuracy: {accuracy_dp:.4f}, Precision ({target_metric_prefix}): {precision_dp:.4f}, Recall ({target_metric_prefix}): {recall_dp:.4f}, F1 ({target_metric_prefix}): {f1_dp:.4f}\")\n",
    "        results.update({\n",
    "            \"Accuracy\": accuracy_dp,\n",
    "            f\"Precision ({target_metric_prefix})\": precision_dp,\n",
    "            f\"Recall ({target_metric_prefix})\": recall_dp,\n",
    "            f\"F1 ({target_metric_prefix})\": f1_dp\n",
    "        })\n",
    "    elif task_type == \"regression\":\n",
    "        mae_dp = mean_absolute_error(all_targets_dp, all_preds_dp)\n",
    "        mse_dp = mean_squared_error(all_targets_dp, all_preds_dp)\n",
    "        r2_dp = r2_score(all_targets_dp, all_preds_dp)\n",
    "        print(f\"MAE: {mae_dp:.2f}, MSE: {mse_dp:.2f}, R2 Score: {r2_dp:.4f}\")\n",
    "        results.update({\"MAE\": mae_dp, \"MSE\": mse_dp, \"R2 Score\": r2_dp})\n",
    "    # ----------------------------------\n",
    "\n",
    "    print(f\"{run_name} results recorded.\")\n",
    "    return results\n",
    "\n",
    "print(\"DP Training/Evaluation function (with task_type) defined/updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43822b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Config and Prompt for Insurance Regression ---\n",
      "Insurance Regression task config and LLM prompt prepared.\n"
     ]
    }
   ],
   "source": [
    "# Description: Define task details for Insurance dataset (Regression) and generate prompt.\n",
    "\n",
    "print(\"\\n--- Preparing Config and Prompt for Insurance Regression ---\")\n",
    "if df_insurance is not None and 'original_columns_insurance' in locals():\n",
    "    # Regression details\n",
    "    regression_details = f\"Predict continuous medical charges ('{TARGET_COLUMN_INSURANCE}'). Focus on minimizing MAE/MSE. Target variable likely has a skewed distribution.\"\n",
    "\n",
    "    task_config_insurance = {\n",
    "        \"dataset_name\": \"Kaggle Medical Cost Personal\",\n",
    "        \"data_domain\": \"Healthcare/Finance\",\n",
    "        \"task_description\": f\"Train a Linear Regression model using DP-SGD to predict medical charges ('{TARGET_COLUMN_INSURANCE}').\",\n",
    "        \"target_variable\": TARGET_COLUMN_INSURANCE,\n",
    "        \"model_type\": \"Linear Regression\", # For LLM context\n",
    "        \"dp_mechanism_family\": \"DP-SGD\",\n",
    "        \"details\": regression_details\n",
    "    }\n",
    "\n",
    "    schema_string_insurance = \", \".join(original_columns_insurance)\n",
    "    data_shape_tuple_insurance = df_insurance.shape\n",
    "\n",
    "    # Call create_llm_prompt with task_type=\"regression\"\n",
    "    llm_prompt_insurance = create_llm_prompt(\n",
    "        task_config_insurance,\n",
    "        schema_string_insurance,\n",
    "        data_shape_tuple_insurance,\n",
    "        task_type=\"regression\" # Specify task type\n",
    "    )\n",
    "    print(\"Insurance Regression task config and LLM prompt prepared.\")\n",
    "    # Optional: print prompt\n",
    "    # print(\"\\n--- Insurance Regression LLM Prompt ---\")\n",
    "    # print(llm_prompt_insurance)\n",
    "else:\n",
    "    print(\"Skipping Insurance Regression prompt creation due to data loading error.\")\n",
    "    llm_prompt_insurance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "631dd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Insurance Fixed DP Regression ---\n",
      "\n",
      "--- Running: Insurance Fixed DP Regression (Task: regression) ---\n",
      "Opacus Attached. Target ε=1.00, Target δ=9.35e-04, Max Grad Norm=1.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 0.9917\n",
      "Evaluating DP Model (regression)...\n",
      "MAE: 12967.19, MSE: 323394481.00, R2 Score: -1.0831\n",
      "Insurance Fixed DP Regression results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Execute Fixed DP run for Insurance Regression.\n",
    "\n",
    "print(\"\\n--- Running: Insurance Fixed DP Regression ---\")\n",
    "if n_features_ins is not None and train_loader_ins is not None and 'DEFAULT_TARGET_DELTA_INS' in locals():\n",
    "    fixed_dp_config_ins_reg = {\n",
    "        \"dp_algorithm\": \"DP-SGD with Gaussian Noise (Fixed)\",\n",
    "        \"target_epsilon\": DEFAULT_TARGET_EPSILON,\n",
    "        \"target_delta\": DEFAULT_TARGET_DELTA_INS,\n",
    "        \"max_grad_norm\": DEFAULT_MAX_GRAD_NORM, # May need adjustment for regression\n",
    "        \"reasoning\": \"Using standard fixed parameters for Insurance Regression.\",\n",
    "        \"llm_model_name\": \"N/A (Fixed Defaults)\",\n",
    "        \"preprocessing_suggestions\": [\"Default\"],\n",
    "        \"column_sensitivity_epsilon\": {\"Info\": \"Fixed parameters\"}\n",
    "    }\n",
    "    results_fixed_ins_reg = train_evaluate_dp_model(\n",
    "        fixed_dp_config_ins_reg,\n",
    "        \"Insurance Fixed DP Regression\",\n",
    "        train_loader_ins, test_loader_ins, n_features_ins, device,\n",
    "        EPOCHS, LEARNING_RATE,\n",
    "        task_type=\"regression\" # Specify task type\n",
    "    )\n",
    "    if results_fixed_ins_reg:\n",
    "        results_list.append(results_fixed_ins_reg)\n",
    "else:\n",
    "    print(\"Skipping Insurance Fixed DP Regression due to missing components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a2cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Insurance Gemini DP Regression ---\n",
      "\n",
      "Sending request to Gemini...\n",
      "Gemini Response Received.\n",
      "Successfully parsed Gemini config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 3.0, 'target_delta': 0.0009345794392523364, 'max_grad_norm': 1.0, 'preprocessing_suggestions': ['Split data into training and testing sets first.', 'Handle missing values if any (e.g., mean/median imputation for numerical, mode for categorical).', \"Encode categorical features: 'sex' and 'smoker' can be binary encoded (0/1). 'region' should be One-Hot Encoded.\", \"Critically, transform the target variable 'charges' due to its likely skewed distribution and large range. Apply a log transformation (e.g., `numpy.log1p`) first, then standardize the transformed target (e.g., using `sklearn.preprocessing.StandardScaler`). This is crucial for stabilizing gradients and improving model performance.\", \"Scale numerical features ('age', 'bmi', 'children') using `sklearn.preprocessing.StandardScaler` after transforming the target.\", 'Ensure the same transformations (fitted on training data) are applied to the test data.'], 'column_sensitivity_epsilon': {'age': 0.6, 'sex': 0.3, 'bmi': 0.5, 'children': 0.4, 'smoker': 0.8, 'region': 0.2}, 'reasoning': \"The recommended DP settings are tailored for training a Linear Regression model on the Kaggle Medical Cost Personal dataset (N_train approx 1070) to predict 'charges' using DP-SGD, aiming to balance privacy with predictive utility (MAE/MSE).\\n\\n1.  **`target_epsilon` (3.0)**: The dataset pertains to Healthcare/Finance, indicating a need for meaningful privacy. However, to achieve reasonable utility (accurate 'charges' prediction crucial for finance aspects) with a relatively small dataset (N=1070), a very low epsilon (e.g., <1) might severely degrade model performance. Epsilon=3.0 is chosen as a moderate value, offering a quantifiable privacy guarantee while still allowing for potentially useful model training. This value should be tuned based on empirical utility and the specific privacy requirements of the application.\\n\\n2.  **`target_delta` (0.0009345... approx 1/1070)**: Setting delta to `1/N` (where N is the training dataset size, approx 1070) is a common practice. It ensures that the probability of an arbitrary privacy violation is small and scales with the dataset size. A delta of `1/1070` is significantly smaller than commonly cited insecure values (e.g., >0.01) and provides a reasonable interpretation of the (epsilon, delta)-DP guarantee.\\n\\n3.  **`max_grad_norm` (1.0)**: This value is highly dependent on the scale of features and, critically for regression, the target variable. The recommendation of 1.0 is **contingent on robust preprocessing**: \\n    *   All input features ('age', 'sex', 'bmi', 'children', 'smoker', 'region' post-encoding) MUST be scaled (e.g., StandardScaler).\\n    *   The target variable 'charges' MUST be transformed and scaled. Given its nature (medical costs, likely skewed with a wide range), a log transformation (e.g., `np.log1p`) followed by standardization (e.g., StandardScaler) is strongly recommended. This makes the target distribution more Gaussian-like and scales its values to a range (e.g., mean 0, std 1) where gradients `X^T * (Xw - y_scaled)` become much smaller and more stable. \\n    A `max_grad_norm` of 1.0 is a common starting point when both features and the (transformed) target are normalized/standardized. It effectively bounds the influence of any single data point. If the target is not appropriately scaled, this value would need to be significantly larger, leading to much higher noise addition and poor utility. The choice of 1.0 aims to clip outlier gradients while preserving useful information from typical data points after appropriate scaling.\\n\\n4.  **Preprocessing Suggestions**: The suggestions emphasize encoding categorical variables and scaling numerical features. Most importantly, they highlight the transformation (log + scale) of the target variable 'charges'. This is vital for Linear Regression's assumptions, for mitigating the impact of skewness on metrics like MSE, and for ensuring that `max_grad_norm` can be set to a reasonably small value for effective DP-SGD.\\n\\n5.  **`column_sensitivity_epsilon`**: These values are conceptual relative sensitivity hints for each feature (excluding the target). 'smoker' is rated highest (0.8) as it's a strong behavioral indicator directly linked to health outcomes and costs. 'age' and 'bmi' are moderate (0.6, 0.5) as they are demographic/health indicators. 'sex', 'children', and 'region' are rated lower. This is not a direct parameter for standard DP-SGD but provides context on feature sensitivity if more granular privacy controls were considered.\\n\\nOverall, these settings aim to provide a robust starting point for differentially private linear regression on this dataset. Empirical tuning, especially of `target_epsilon` and `max_grad_norm` (if preprocessing varies), will be necessary to optimize the privacy-utility trade-off for specific MAE/MSE goals.\"}\n",
      "\n",
      "--- Running: Insurance Gemini DP Regression (Task: regression) ---\n",
      "Opacus Attached. Target ε=3.00, Target δ=9.35e-04, Max Grad Norm=1.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 2.9914\n",
      "Evaluating DP Model (regression)...\n",
      "MAE: 12966.90, MSE: 323384357.60, R2 Score: -1.0830\n",
      "Insurance Gemini DP Regression results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Get config from Gemini for Insurance Regression and run.\n",
    "\n",
    "print(\"\\n--- Running: Insurance Gemini DP Regression ---\")\n",
    "if gemini_client and llm_prompt_insurance and n_features_ins is not None and train_loader_ins is not None:\n",
    "    gemini_config_ins_reg = get_gemini_config(llm_prompt_insurance, gemini_client)\n",
    "    if gemini_config_ins_reg:\n",
    "        gemini_config_ins_reg[\"llm_model_name\"] = GEMINI_MODEL_NAME\n",
    "        results_gemini_ins_reg = train_evaluate_dp_model(\n",
    "            gemini_config_ins_reg,\n",
    "            \"Insurance Gemini DP Regression\",\n",
    "            train_loader_ins, test_loader_ins, n_features_ins, device,\n",
    "            EPOCHS, LEARNING_RATE,\n",
    "            task_type=\"regression\"\n",
    "        )\n",
    "        if results_gemini_ins_reg:\n",
    "            results_list.append(results_gemini_ins_reg)\n",
    "    else:\n",
    "        print(\"Failed to get config from Gemini for Insurance Regression.\")\n",
    "elif not gemini_client:\n",
    "    print(\"Skipping Insurance Gemini run - client not initialized.\")\n",
    "else:\n",
    "    print(\"Skipping Insurance Gemini run - missing components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac5b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Insurance Groq DP Regression ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 2.0, 'target_delta': 1e-05, 'max_grad_norm': 1.0, 'preprocessing_suggestions': ['Normalize features using StandardScaler to ensure features are on a similar scale.', 'Encode categorical variables (sex, smoker, region, children) using OneHotEncoder.'], 'column_sensitivity_epsilon': {'age': 0.8, 'sex': 0.2, 'bmi': 0.8, 'children': 0.2, 'smoker': 0.2, 'region': 0.2}, 'reasoning': 'The target_epsilon of 2.0 balances privacy and utility, suitable for healthcare/finance data. Target_delta is set to 1e-5 to ensure strong privacy guarantees. max_grad_norm of 1.0 is chosen to stabilize gradients without excessive clipping. Column sensitivity values reflect the relative importance and privacy concerns of each feature.'}\n",
      "\n",
      "--- Running: Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression (Task: regression) ---\n",
      "Opacus Attached. Target ε=2.00, Target δ=1.00e-05, Max Grad Norm=1.0\n",
      "Training DP Model (regression)...\n",
      "DP Training Complete. Final ε = 1.9966\n",
      "Evaluating DP Model (regression)...\n",
      "MAE: 12967.05, MSE: 323394121.42, R2 Score: -1.0831\n",
      "Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Get config from Groq for Insurance Regression and run.\n",
    "\n",
    "print(\"\\n--- Running: Insurance Groq DP Regression ---\")\n",
    "if groq_client and llm_prompt_insurance and n_features_ins is not None and train_loader_ins is not None:\n",
    "    llama_config_ins_reg = get_groq_config(llm_prompt_insurance, groq_client, GROQ_MODEL_NAME)\n",
    "    if llama_config_ins_reg:\n",
    "        llama_config_ins_reg[\"llm_model_name\"] = GROQ_MODEL_NAME\n",
    "        results_llama_ins_reg = train_evaluate_dp_model(\n",
    "            llama_config_ins_reg,\n",
    "            f\"Insurance Groq ({GROQ_MODEL_NAME}) DP Regression\",\n",
    "            train_loader_ins, test_loader_ins, n_features_ins, device,\n",
    "            EPOCHS, LEARNING_RATE,\n",
    "            task_type=\"regression\"\n",
    "        )\n",
    "        if results_llama_ins_reg:\n",
    "            results_list.append(results_llama_ins_reg)\n",
    "    else:\n",
    "        print(\"Failed to get config from Groq for Insurance Regression.\")\n",
    "elif not groq_client:\n",
    "    print(\"Skipping Insurance Groq run - client not initialized.\")\n",
    "else:\n",
    "    print(\"Skipping Insurance Groq run - missing components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d99065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined Experiment Results (Including Insurance Regression) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>Precision (Readmit)</th>\n",
       "      <th>Recall (Readmit)</th>\n",
       "      <th>F1 (Readmit)</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insurance Non-DP Regression</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>6062.432718</td>\n",
       "      <td>6.603719e+07</td>\n",
       "      <td>0.574636</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insurance Fixed DP Regression</td>\n",
       "      <td>N/A (Fixed Defaults)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991737</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12967.187698</td>\n",
       "      <td>3.233945e+08</td>\n",
       "      <td>-1.083073</td>\n",
       "      <td>Using standard fixed parameters for Insurance Regression.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insurance Gemini DP Regression</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.991402</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12966.904507</td>\n",
       "      <td>3.233844e+08</td>\n",
       "      <td>-1.083008</td>\n",
       "      <td>The recommended DP settings are tailored for training a Linear Regression model on the Kaggle Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.996596</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12967.054016</td>\n",
       "      <td>3.233941e+08</td>\n",
       "      <td>-1.083071</td>\n",
       "      <td>The target_epsilon of 2.0 balances privacy and utility, suitable for healthcare/finance data. Ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm LLM Epsilon Suggestion Accuracy Precision (Stroke) Recall (Stroke) F1 (Stroke) Precision (Readmit) Recall (Readmit) F1 (Readmit)           MAE           MSE  R2 Score                                                                                        LLM Reasoning\n",
       "0                                   Insurance Non-DP Regression                            N/A            N/A           N/A          N/A           N/A                    N/A      N/A                N/A             N/A         N/A                 N/A              N/A          N/A   6062.432718  6.603719e+07  0.574636                                                                                                  N/A\n",
       "1                                 Insurance Fixed DP Regression           N/A (Fixed Defaults)            1.0      0.991737     0.000935           1.0                    1.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12967.187698  3.233945e+08 -1.083073                                            Using standard fixed parameters for Insurance Regression.\n",
       "2                                Insurance Gemini DP Regression       gemini-2.5-pro-exp-03-25            3.0      2.991402     0.000935           1.0                    3.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12966.904507  3.233844e+08 -1.083008  The recommended DP settings are tailored for training a Linear Regression model on the Kaggle Me...\n",
       "3  Insurance Groq (deepseek-r1-distill-llama-70b) DP Regression  deepseek-r1-distill-llama-70b            2.0      1.996596      0.00001           1.0                    2.0      N/A                N/A             N/A         N/A                 N/A              N/A          N/A  12967.054016  3.233941e+08 -1.083071  The target_epsilon of 2.0 balances privacy and utility, suitable for healthcare/finance data. Ta..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Description: Show the results from ALL runs in a table.\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    # Define desired column order, including ALL possible metrics\n",
    "    # Order them logically\n",
    "    cols_order = [\n",
    "        \"Run Type\", \"LLM Used\",\n",
    "        \"Target Epsilon\", \"Final Epsilon\", \"Target Delta\", \"Max Grad Norm\",\n",
    "        \"LLM Epsilon Suggestion\", # Moved up for easier comparison with Target Epsilon\n",
    "        # Classification Metrics\n",
    "        \"Accuracy\", \"Precision (Stroke)\", \"Recall (Stroke)\", \"F1 (Stroke)\",\n",
    "        \"Precision (Readmit)\", \"Recall (Readmit)\", \"F1 (Readmit)\",\n",
    "        # Regression Metrics\n",
    "        \"MAE\", \"MSE\", \"R2 Score\",\n",
    "        # LLM Reasoning last as it can be long\n",
    "        \"LLM Reasoning\"\n",
    "    ]\n",
    "    # Ensure all expected columns exist, add if missing (fill with N/A or np.nan)\n",
    "    for col in cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = np.nan # Use np.nan for numerical/potentially numerical\n",
    "\n",
    "    results_df = results_df[cols_order]\n",
    "    # Fill NaNs that might have occurred if some runs failed or metrics weren't applicable\n",
    "    results_df.fillna(\"N/A\", inplace=True) # Replace np.nan with \"N/A\" string for display\n",
    "\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', 1200) # Wider for more columns\n",
    "    print(\"\\n--- Combined Experiment Results (Including Insurance Regression) ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results recorded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c9d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('regression_v1.csv', index_label='SrNo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0f487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diabetes Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.606169</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diabetes Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.004022</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.595428</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Recommendations are tailored for DP-SGD training of a Logistic Regression model on the UCI Diabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Groq (deepseek-r1-distill-llama-70b) DP SGD</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.997348</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.595153</td>\n",
       "      <td>0.616633</td>\n",
       "      <td>0.502479</td>\n",
       "      <td>0.553734</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diabetes Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.004022</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.608923</td>\n",
       "      <td>0.628164</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.576877</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The recommended DP settings balance the sensitivity of healthcare data with the need for model u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.604792</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>0.511295</td>\n",
       "      <td>0.563962</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm  Accuracy Precision (Stroke) Recall (Stroke) F1 (Stroke) LLM Epsilon Suggestion                                                                                        LLM Reasoning\n",
       "0                          Diabetes Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.606169                N/A             N/A         N/A                    N/A                                                                                                  N/A\n",
       "1                       Diabetes Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      3.004022      0.00001           1.5  0.595428                N/A             N/A         N/A                    3.0  Recommendations are tailored for DP-SGD training of a Logistic Regression model on the UCI Diabe...\n",
       "2  Groq (deepseek-r1-distill-llama-70b) DP SGD  deepseek-r1-distill-llama-70b            2.0      1.997348      0.00001          10.0  0.595153           0.616633        0.502479    0.553734                    2.0  The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...\n",
       "3                       Diabetes Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      3.004022      0.00001           7.5  0.608923           0.628164        0.533333    0.576877                    3.0  The recommended DP settings balance the sensitivity of healthcare data with the need for model u...\n",
       "4                          Diabetes Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.604792           0.628726        0.511295    0.563962                    N/A                                                                                                  N/A"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Description: Show the results from all runs in a table.\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    # Reorder columns for clarity\n",
    "    cols_order = [\n",
    "        \"Run Type\", \"LLM Used\", \"Target Epsilon\", \"Final Epsilon\", \"Target Delta\",\n",
    "        \"Max Grad Norm\", \"Accuracy\", \"Precision (Stroke)\", \"Recall (Stroke)\", \"F1 (Stroke)\",\n",
    "        \"LLM Epsilon Suggestion\", \"LLM Reasoning\"\n",
    "    ]\n",
    "    # Ensure all expected columns exist, add if missing\n",
    "    for col in cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = \"N/A\"\n",
    "\n",
    "    results_df = results_df[cols_order]\n",
    "    pd.set_option('display.max_colwidth', 100) # Show more of the reasoning column\n",
    "    pd.set_option('display.width', 1000) # Adjust display width\n",
    "    print(\"\\n--- Combined Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results recorded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "100a18c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Groq (deepseek-r1-distill-llama-70b) DP SGD</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.997348</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.595153</td>\n",
       "      <td>0.616633</td>\n",
       "      <td>0.502479</td>\n",
       "      <td>0.553734</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diabetes Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.004022</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.608923</td>\n",
       "      <td>0.628164</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.576877</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The recommended DP settings balance the sensitivity of healthcare data with the need for model u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.604792</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>0.511295</td>\n",
       "      <td>0.563962</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm  Accuracy Precision (Stroke) Recall (Stroke) F1 (Stroke) LLM Epsilon Suggestion                                                                                        LLM Reasoning\n",
       "2  Groq (deepseek-r1-distill-llama-70b) DP SGD  deepseek-r1-distill-llama-70b            2.0      1.997348      0.00001          10.0  0.595153           0.616633        0.502479    0.553734                    2.0  The target_epsilon of 2.0 balances privacy and utility, considering the healthcare domain's sens...\n",
       "3                       Diabetes Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      3.004022      0.00001           7.5  0.608923           0.628164        0.533333    0.576877                    3.0  The recommended DP settings balance the sensitivity of healthcare data with the need for model u...\n",
       "4                          Diabetes Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.604792           0.628726        0.511295    0.563962                    N/A                                                                                                  N/A"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.drop(0, axis=0, inplace=True)\n",
    "results_df.drop(1, axis=0, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e8a5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0741e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec24248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
