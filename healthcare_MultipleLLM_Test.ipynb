{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd545ae",
   "metadata": {},
   "source": [
    "### Comparing Groq model and Gemini 2.5 pro on a Stroke dataset\n",
    "- Main issues encountered: The stroke is heavily imbalanced as there are extremely few 1 in the stroke. This leads to high accuracy but less recall. \n",
    "- Updated prompt with little bit of guidelines\n",
    "\n",
    "## Comparing Multiple LLM's\n",
    "- gemini 2.5 catches the conetxt best and also has best results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b7cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Groq client initialized successfully for model: deepseek-r1-distill-llama-70b\n",
      "Gemini client initialized successfully for model: gemini-2.5-pro-exp-03-25\n"
     ]
    }
   ],
   "source": [
    "# Description: Import libraries, set up constants, initialize results list.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Opacus\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator # Optional\n",
    "\n",
    "# LLM Clients\n",
    "from groq import Groq\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv() # Load environment variables from .env file if present\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = 'cleaned_healthcare_stroke.csv' # Ensure this file exists\n",
    "TARGET_COLUMN = 'stroke'\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Training Hyperparameters (Base)\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10 # Keep consistent for comparison unless tuning epochs\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Default DP Parameters (for Fixed DP run)\n",
    "DEFAULT_TARGET_EPSILON = 1.0\n",
    "DEFAULT_TARGET_DELTA = 1e-5 # Will likely recalculate based on N\n",
    "DEFAULT_MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Initialize Results Storage ---\n",
    "results_list = []\n",
    "\n",
    "# --- Initialize LLM Clients ---\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\", \"llama3-70b-8192\") # Or your preferred Groq model\n",
    "GEMINI_MODEL_NAME = os.getenv(\"GEMINI_MODEL_NAME\", \"gemini-1.5-flash\") # Or your preferred Gemini model\n",
    "\n",
    "groq_client = None\n",
    "gemini_client = None\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    try:\n",
    "        groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "        print(f\"Groq client initialized successfully for model: {GROQ_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Groq client: {e}\")\n",
    "else:\n",
    "    print(\"Warning: GROQ_API_KEY not found. Groq LLM will not be used.\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    try:\n",
    "        gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        print(f\"Gemini client initialized successfully for model: {GEMINI_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Gemini client: {e}\")\n",
    "else:\n",
    "    print(\"Warning: GEMINI_API_KEY not found. Gemini LLM will not be used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b81313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Dataset: cleaned_healthcare_stroke.csv for Classification ---\n",
      "Kaggle Stroke Prediction Dataset loaded successfully.\n",
      "Dataset shape: (5110, 14)\n",
      "\n",
      "Verifying data assumptions...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   gender             5110 non-null   object \n",
      " 1   age                5110 non-null   float64\n",
      " 2   hypertension       5110 non-null   int64  \n",
      " 3   heart_disease      5110 non-null   int64  \n",
      " 4   ever_married       5110 non-null   object \n",
      " 5   work_type          5110 non-null   object \n",
      " 6   Residence_type     5110 non-null   object \n",
      " 7   avg_glucose_level  5110 non-null   float64\n",
      " 8   bmi                5110 non-null   float64\n",
      " 9   smoking_status     5110 non-null   object \n",
      " 10  stroke             5110 non-null   int64  \n",
      " 11  age_group          5110 non-null   object \n",
      " 12  glucose_group      5110 non-null   object \n",
      " 13  bmi_group          5110 non-null   object \n",
      "dtypes: float64(3), int64(3), object(8)\n",
      "memory usage: 559.0+ KB\n",
      "\n",
      "Target Column 'stroke' Value Counts:\n",
      "stroke\n",
      "0    4861\n",
      "1     249\n",
      "Name: count, dtype: int64\n",
      "Balance ratio:\n",
      "stroke\n",
      "0    0.951272\n",
      "1    0.048728\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Description: Load a classification dataset (e.g., Stroke).\n",
    "# Verify assumptions (cleaning, balance for this specific dataset).\n",
    "\n",
    "# --- Configuration for CURRENT Classification Dataset (e.g., Stroke) ---\n",
    "CURRENT_DATA_FILE = 'cleaned_healthcare_stroke.csv' # Or your target dataset\n",
    "CURRENT_TARGET_COLUMN = 'stroke'\n",
    "CURRENT_DATASET_NAME_FOR_LLM = \"Kaggle Stroke Prediction\"\n",
    "CURRENT_TARGET_METRIC_PREFIX = \"Stroke\" # For naming P/R/F1 columns\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n--- Loading Dataset: {CURRENT_DATA_FILE} for Classification ---\")\n",
    "df_current = None # Initialize\n",
    "original_columns_current = []\n",
    "pos_count_current, neg_count_current = 0, 0 # For imbalance info\n",
    "\n",
    "try:\n",
    "    df_current = pd.read_csv(CURRENT_DATA_FILE)\n",
    "    print(f\"{CURRENT_DATASET_NAME_FOR_LLM} Dataset loaded successfully.\")\n",
    "    print(\"Dataset shape:\", df_current.shape)\n",
    "\n",
    "    # --- Verification (adapt as needed) ---\n",
    "    print(\"\\nVerifying data assumptions...\")\n",
    "    df_current.info()\n",
    "    if CURRENT_TARGET_COLUMN in df_current.columns:\n",
    "        print(f\"\\nTarget Column '{CURRENT_TARGET_COLUMN}' Value Counts:\")\n",
    "        print(df_current[CURRENT_TARGET_COLUMN].value_counts())\n",
    "        balance_check = df_current[CURRENT_TARGET_COLUMN].value_counts(normalize=True)\n",
    "        print(f\"Balance ratio:\\n{balance_check}\")\n",
    "        pos_count_current = df_current[CURRENT_TARGET_COLUMN].sum()\n",
    "        neg_count_current = len(df_current) - pos_count_current\n",
    "    else:\n",
    "        print(f\"ERROR: Target column '{CURRENT_TARGET_COLUMN}' not found!\")\n",
    "        df_current = None\n",
    "\n",
    "    if df_current is not None:\n",
    "        original_columns_current = df_current.columns.tolist()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {CURRENT_DATA_FILE}.\")\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8c8228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Kaggle Stroke Prediction Data ---\n",
      "\n",
      "Identified Categorical Features: ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'age_group', 'glucose_group', 'bmi_group']\n",
      "Identified Numerical Features: ['age', 'avg_glucose_level', 'bmi']\n",
      "\n",
      "Number of features after preprocessing: 26\n",
      "\n",
      "Kaggle Stroke Prediction preprocessing and splitting complete.\n",
      "Training set size: 4088\n",
      "\n",
      "Calculated pos_weight for loss: 19.54\n",
      "Default Target Delta for Kaggle Stroke Prediction (1/N): 2.45e-04\n"
     ]
    }
   ],
   "source": [
    "# Description: Preprocess current classification data, split, convert to tensors, calculate pos_weight.\n",
    "\n",
    "print(f\"\\n--- Preprocessing {CURRENT_DATASET_NAME_FOR_LLM} Data ---\")\n",
    "# Initialize to prevent NameErrors if df_current is None\n",
    "n_features_current = None\n",
    "train_loader_current = None\n",
    "test_loader_current = None\n",
    "pos_weight_tensor_current = torch.tensor([1.0], dtype=torch.float32).to(device) # Default\n",
    "DEFAULT_TARGET_DELTA_CURRENT = 1e-5 # Fallback\n",
    "\n",
    "if df_current is not None and CURRENT_TARGET_COLUMN in df_current.columns:\n",
    "    X_current = df_current.drop(CURRENT_TARGET_COLUMN, axis=1)\n",
    "    y_current = df_current[CURRENT_TARGET_COLUMN]\n",
    "\n",
    "    # !!! IMPORTANT: Define these accurately for CURRENT_DATA_FILE !!!\n",
    "    categorical_features_current = X_current.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    potential_numerical_features = ['age', 'avg_glucose_level', 'bmi'] # Example for Stroke\n",
    "    numerical_features_current = [col for col in potential_numerical_features if col in X_current.columns and pd.api.types.is_numeric_dtype(X_current[col])]\n",
    "    # Add any other numerical columns specific to your dataset\n",
    "    # numerical_features_current.extend([col for col in X_current.columns if pd.api.types.is_numeric_dtype(X_current[col]) and col not in numerical_features_current and col not in categorical_features_current])\n",
    "\n",
    "\n",
    "    print(f\"\\nIdentified Categorical Features: {categorical_features_current}\")\n",
    "    print(f\"Identified Numerical Features: {numerical_features_current}\")\n",
    "\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
    "\n",
    "    preprocessor_current = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features_current),\n",
    "            ('cat', categorical_transformer, categorical_features_current)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    X_train_curr, X_test_curr, y_train_curr, y_test_curr = train_test_split(\n",
    "        X_current, y_current, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_current\n",
    "    )\n",
    "    y_test_labels_curr = y_test_curr.values # Keep for evaluation if needed by DP function\n",
    "\n",
    "    try:\n",
    "        X_train_processed_curr = preprocessor_current.fit_transform(X_train_curr)\n",
    "        X_test_processed_curr = preprocessor_current.transform(X_test_curr)\n",
    "    except ValueError as ve: # Catch errors e.g. if a category only appears in test\n",
    "        print(f\"Preprocessing Error: {ve}. This might be due to new categories in test data or empty feature lists.\")\n",
    "        print(\"Ensure categorical_features_current and numerical_features_current are correctly defined and not empty.\")\n",
    "        X_train_processed_curr = None # Prevent further errors\n",
    "\n",
    "    if X_train_processed_curr is not None:\n",
    "        n_features_current = X_train_processed_curr.shape[1]\n",
    "        print(f\"\\nNumber of features after preprocessing: {n_features_current}\")\n",
    "\n",
    "        X_train_tensor_curr = torch.tensor(X_train_processed_curr.astype(np.float32)).to(device)\n",
    "        y_train_tensor_curr = torch.tensor(y_train_curr.values.astype(np.float32)).unsqueeze(1).to(device)\n",
    "        X_test_tensor_curr = torch.tensor(X_test_processed_curr.astype(np.float32)).to(device)\n",
    "        y_test_tensor_curr = torch.tensor(y_test_curr.values.astype(np.float32)).unsqueeze(1).to(device) # For loss\n",
    "\n",
    "        train_dataset_curr = TensorDataset(X_train_tensor_curr, y_train_tensor_curr)\n",
    "        test_dataset_curr = TensorDataset(X_test_tensor_curr, y_test_tensor_curr)\n",
    "        train_loader_current = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader_current = DataLoader(test_dataset_curr, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        print(f\"\\n{CURRENT_DATASET_NAME_FOR_LLM} preprocessing and splitting complete.\")\n",
    "        print(f\"Training set size: {len(train_dataset_curr)}\")\n",
    "\n",
    "        # Calculate Positive Class Weight\n",
    "        neg_count = (y_train_curr == 0).sum()\n",
    "        pos_count = (y_train_curr == 1).sum()\n",
    "        if pos_count > 0:\n",
    "            pos_weight_value = neg_count / pos_count\n",
    "            pos_weight_tensor_current = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "            print(f\"\\nCalculated pos_weight for loss: {pos_weight_value:.2f}\")\n",
    "        else:\n",
    "            pos_weight_tensor_current = torch.tensor([1.0], dtype=torch.float32).to(device)\n",
    "            print(\"Warning: No positive samples in training data. Using default pos_weight (1.0).\")\n",
    "\n",
    "        DEFAULT_TARGET_DELTA_CURRENT = 1 / len(train_dataset_curr)\n",
    "        print(f\"Default Target Delta for {CURRENT_DATASET_NAME_FOR_LLM} (1/N): {DEFAULT_TARGET_DELTA_CURRENT:.2e}\")\n",
    "    else:\n",
    "        print(f\"Skipping {CURRENT_DATASET_NAME_FOR_LLM} tensor conversion due to preprocessing error.\")\n",
    "else:\n",
    "    print(f\"Skipping {CURRENT_DATASET_NAME_FOR_LLM} preprocessing: df_current is None or target missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3412e3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression model class defined.\n"
     ]
    }
   ],
   "source": [
    "# Description: Define a simple Logistic Regression model using PyTorch nn.Module.\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "print(\"\\nLogistic Regression model class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91c15f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Kaggle Stroke Prediction Non-DP SGD ---\n",
      "Training Standard Logistic Regression (Kaggle Stroke Prediction)...\n",
      "Standard Training Complete (Kaggle Stroke Prediction).\n",
      "Evaluating Standard Model (Kaggle Stroke Prediction)...\n",
      "Accuracy: 0.7202, Precision (Stroke): 0.1266, Recall (Stroke): 0.8000, F1 (Stroke): 0.2186\n",
      "Kaggle Stroke Prediction Non-DP SGD results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Train and evaluate standard SGD model for the CURRENT classification dataset.\n",
    "\n",
    "run_name_non_dp = f\"{CURRENT_DATASET_NAME_FOR_LLM} Non-DP SGD\"\n",
    "print(f\"\\n--- Running: {run_name_non_dp} ---\")\n",
    "\n",
    "if n_features_current is not None and train_loader_current is not None and test_loader_current is not None:\n",
    "    model_non_dp = LogisticRegression(n_features_current).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor_current)\n",
    "    optimizer_non_dp = optim.SGD(model_non_dp.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(f\"Training Standard Logistic Regression ({CURRENT_DATASET_NAME_FOR_LLM})...\")\n",
    "    # ... (Training loop same as before, using _current variables) ...\n",
    "    model_non_dp.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_X, batch_y in train_loader_current:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer_non_dp.zero_grad(); outputs = model_non_dp(batch_X)\n",
    "            loss = criterion(outputs, batch_y); loss.backward(); optimizer_non_dp.step()\n",
    "    print(f\"Standard Training Complete ({CURRENT_DATASET_NAME_FOR_LLM}).\")\n",
    "\n",
    "    print(f\"Evaluating Standard Model ({CURRENT_DATASET_NAME_FOR_LLM})...\")\n",
    "    # ... (Evaluation loop same as before, using _current variables) ...\n",
    "    model_non_dp.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader_current:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model_non_dp(batch_X); preds = torch.round(torch.sigmoid(outputs))\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_targets.extend(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, pos_label=1, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision ({CURRENT_TARGET_METRIC_PREFIX}): {precision:.4f}, Recall ({CURRENT_TARGET_METRIC_PREFIX}): {recall:.4f}, F1 ({CURRENT_TARGET_METRIC_PREFIX}): {f1:.4f}\")\n",
    "\n",
    "    results_list.append({\n",
    "        \"Run Type\": run_name_non_dp, \"LLM Used\": \"N/A\",\n",
    "        \"Target Epsilon\": \"N/A\", \"Final Epsilon\": \"N/A\", \"Target Delta\": \"N/A\", \"Max Grad Norm\": \"N/A\",\n",
    "        \"Accuracy\": accuracy,\n",
    "        f\"Precision ({CURRENT_TARGET_METRIC_PREFIX})\": precision,\n",
    "        f\"Recall ({CURRENT_TARGET_METRIC_PREFIX})\": recall,\n",
    "        f\"F1 ({CURRENT_TARGET_METRIC_PREFIX})\": f1,\n",
    "        \"LLM Epsilon Suggestion\": \"N/A\", \"LLM Reasoning\": \"N/A\"\n",
    "    })\n",
    "    print(f\"{run_name_non_dp} results recorded.\")\n",
    "else:\n",
    "    print(f\"Skipping {run_name_non_dp} run due to missing components for {CURRENT_DATASET_NAME_FOR_LLM}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73cb8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Description: Define the standard prompt structure and functions to call LLMs.\n",
    "\n",
    "# # v2\n",
    "# def create_llm_prompt(task_config, schema_string, data_shape):\n",
    "#     \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices.\"\"\"\n",
    "#     prompt = f\"\"\"\n",
    "# Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a Logistic Regression model using DP-SGD.\n",
    "# The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "# **Dataset Context:**\n",
    "# - Name: {task_config['dataset_name']}\n",
    "# - Domain: {task_config['data_domain']} (Note: Healthcare data is generally considered sensitive).\n",
    "# - Task: {task_config['task_description']}\n",
    "# - Schema (Original Columns): {schema_string}\n",
    "# - Extra details: {task_config['details']} (Pay close attention to class imbalance).\n",
    "\n",
    "# **Parameter Guidance - IMPORTANT:** Avoid generic default values. Base your recommendations *specifically* on the context provided above.\n",
    "\n",
    "# Provide your recommendations ONLY in a structured JSON format. The JSON object must include the following keys:\n",
    "# - \"dp_algorithm\": String, the specific DP algorithm variant recommended (e.g., \"DP-SGD with Gaussian Noise\").\n",
    "# - \"target_epsilon\": Float, recommended privacy budget epsilon (e.g., 1.5). Justify this based on sensitivity, utility needs, and domain.\n",
    "# - \"target_delta\": Float or String, recommended privacy budget delta (e.g., 1e-5 or suggest calculating as \"1/N\"). Justify choice.\n",
    "# - \"max_grad_norm\": Float, recommended gradient clipping norm (e.g., 1.0). Justify based on model stability and potential gradient explosion, especially considering class imbalance if noted.\n",
    "# - \"preprocessing_suggestions\": List of strings, specific preprocessing actions recommended BEFORE applying DP (e.g., \"Remove: id\", \"Normalize: age, avg_glucose_level, bmi\").\n",
    "# - \"column_sensitivity_epsilon\": A dictionary where keys are original column names and values are *conceptual* relative sensitivity floats (0.0=low, 1.0=high/ID) or labels (Low, Medium, High). This guides understanding, not direct budget split in standard DP-SGD. Exclude the target variable.\n",
    "# - \"reasoning\": String, concise reasoning behind the overall recommendations (epsilon, delta, max_grad_norm choices, linking back to context).\n",
    "\n",
    "# JSON Output ONLY:\n",
    "# \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "# #v3\n",
    "# def create_llm_prompt(task_config, schema_string, data_shape):\n",
    "#     \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices\n",
    "#        and including general ML best practices context.\"\"\"\n",
    "\n",
    "#     # Calculate approximate training size N for context\n",
    "#     approx_N_train = int(data_shape[0] * (1-TEST_SIZE)) if data_shape else 'Unknown'\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a Logistic Regression model using DP-SGD.\n",
    "# The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "# **Dataset Context:**\n",
    "# - Name: {task_config['dataset_name']}\n",
    "# - Domain: {task_config['data_domain']} (Note: Healthcare data is generally considered sensitive).\n",
    "# - Task: {task_config['task_description']}\n",
    "# - Schema (Original Columns): {schema_string}\n",
    "# - Data Shape: {data_shape} (Approx. Training N = {approx_N_train})\n",
    "# - Extra details: {task_config['details']} (Pay close attention to class imbalance).\n",
    "\n",
    "# **General ML Best Practices Context (Keep these in mind):**\n",
    "# - **Normalization/Scaling:** Features with different scales (like 'age' vs 'avg_glucose_level') MUST be normalized or standardized (e.g., StandardScaler, MinMaxScaler) for models like Logistic Regression and especially before applying gradient clipping in DP-SGD. This ensures stable gradient computations. Apply scaling AFTER splitting data and ideally AFTER imputation if applicable.\n",
    "# - **Gradient Stability & Clipping:** DP-SGD uses gradient clipping (`max_grad_norm`) to bound the influence of any single data point. Choosing the norm value is a trade-off:\n",
    "#     - Too low: Clips potentially useful gradient information, slowing learning or preventing convergence, especially for minority classes or complex patterns.\n",
    "#     - Too high: Less protection against outliers, potentially higher noise required for the same privacy budget (ε).\n",
    "#     - Imbalanced Data Impact: Gradients from rare class samples might be infrequent but large; aggressive clipping can disproportionately affect learning for that class.\n",
    "# - **Imbalanced Data Handling:** Beyond class weighting in the loss (which is assumed here), model evaluation should focus on metrics like F1-score, Precision, Recall for the minority class, not just accuracy. The goal is often to improve detection of the rare class.\n",
    "# - **Preprocessing Order:** Typically: Split Data -> Impute Missing -> Encode Categorical -> Scale Numerical -> Train Model. DP is applied during the training step.\n",
    "\n",
    "# **Parameter Guidance - IMPORTANT:** Based on the Dataset Context AND the ML Best Practices above, provide specific, justified recommendations. Avoid generic defaults.\n",
    "\n",
    "# 1.  **`target_epsilon`**: Balance '{task_config['data_domain']}' sensitivity vs. utility needed for training. Justify the specific trade-off. (Range 1.0-5.0 often considered, but justify *your* choice).\n",
    "# 2.  **`target_delta`**: Recommend a specific small value (e.g., 1e-5, 1e-6) or suggest \"1/N\". Justify why (e.g., related to approx N={approx_N_train}).\n",
    "# 3.  **`max_grad_norm`**: **Connect this directly to the Gradient Stability & Imbalanced Data points above.** Given the heavy imbalance, suggest a value (e.g., range 5.0 - 15.0, or a specific reasoned value) likely higher than a generic default (like 1.0) to preserve minority class signals. Justify based *explicitly* on imbalance and the need for stable yet informative gradients.\n",
    "# 4.  **`column_sensitivity_epsilon`**: Conceptual relative sensitivity hints (0.0 low, 1.0 high). Reflect potential identifiability/sensitivity based on domain/name. Exclude target.\n",
    "# 5.  **`reasoning`**: Concise but detailed justification for epsilon, delta, and max_grad_norm, *explicitly linking* choices to dataset context (domain, N, imbalance) and the relevant ML best practices mentioned (normalization, gradient stability).\n",
    "\n",
    "# **Output Format:**\n",
    "# Provide recommendations ONLY in a structured JSON format with keys: \"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\".\n",
    "\n",
    "# JSON Output ONLY:\n",
    "# \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "# Description: Define the standard prompt structure and functions to call LLMs.\n",
    "\n",
    "def create_llm_prompt(task_config, schema_string, data_shape, task_type=\"classification\"): # Add task_type\n",
    "    \"\"\"Creates a more detailed prompt string for the LLM, guiding parameter choices\n",
    "       and including general ML best practices context. Adapts for task_type.\"\"\"\n",
    "\n",
    "    approx_N_train = int(data_shape[0] * (1-TEST_SIZE)) if data_shape else 'Unknown'\n",
    "\n",
    "    # Adjust parts of the prompt based on task_type\n",
    "    if task_type == \"regression\":\n",
    "        imbalance_guidance = \"\" # No class imbalance for regression\n",
    "        target_guidance_metrics = \"metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or R2 Score. The goal is often to minimize error.\"\n",
    "        max_grad_norm_focus = \"focus on overall gradient stability, considering the range and scale of the target variable values, rather than specific class signals. A moderate value (e.g., 1.0-10.0, depending on target scale and feature normalization) is common.\"\n",
    "        epsilon_utility_focus = \"utility needs for accurate predictions (e.g., low MAE/MSE)\"\n",
    "        model_type_in_prompt = \"Linear Regression\" # Or generic \"Regression Model\"\n",
    "    else: # classification (default)\n",
    "        imbalance_guidance = \"(Pay close attention to class imbalance if mentioned in 'Extra details').\"\n",
    "        target_guidance_metrics = \"metrics like F1-score, Precision, Recall for the minority class, not just accuracy. The goal is often to improve detection of the rare class.\"\n",
    "        max_grad_norm_focus = \"**Connect this directly to the Gradient Stability & Imbalanced Data points above.** Given potential class imbalance (see 'Extra details'), suggest a value (e.g., range 5.0 - 15.0, or a specific reasoned value) likely higher than a generic default (like 1.0) to preserve minority class signals. Justify based *explicitly* on imbalance and the need for stable yet informative gradients.\"\n",
    "        epsilon_utility_focus = \"utility needs for model training (which often requires sufficient signal)\"\n",
    "        model_type_in_prompt = \"Logistic Regression\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Analyze the provided dataset context and task to recommend **optimized and justified** Differential Privacy (DP) settings for training a {model_type_in_prompt} model using DP-SGD.\n",
    "The goal is to predict the target variable '{task_config['target_variable']}'.\n",
    "\n",
    "**Dataset Context:**\n",
    "- Name: {task_config['dataset_name']}\n",
    "- Domain: {task_config['data_domain']}\n",
    "- Task: {task_config['task_description']}\n",
    "- Schema (Original Columns): {schema_string}\n",
    "- Data Shape: {data_shape} (Approx. Training N = {approx_N_train})\n",
    "- Extra details: {task_config['details']} {imbalance_guidance}\n",
    "\n",
    "**General ML Best Practices Context (Keep these in mind):**\n",
    "- **Normalization/Scaling:** Features MUST be normalized or standardized for models like {model_type_in_prompt} and especially before applying gradient clipping in DP-SGD.\n",
    "- **Gradient Stability & Clipping (`max_grad_norm`):** Bound influence of single points. Trade-off:\n",
    "    - Too low: Clips useful info, hinders learning.\n",
    "    - Too high: Less protection, more noise needed.\n",
    "    - { \"Imbalanced Data Impact: Gradients from rare class samples might be infrequent but large; aggressive clipping can disproportionately affect learning for that class.\" if task_type==\"classification\" else \"For regression, consider the scale of the target variable when thinking about gradient magnitudes.\"}\n",
    "- **Evaluation Focus ({task_type}):** Model evaluation should focus on {target_guidance_metrics}\n",
    "- **Preprocessing Order:** Typically: Split Data -> Impute -> Encode -> Scale -> Train.\n",
    "\n",
    "**Parameter Guidance - IMPORTANT:** Base recommendations *specifically* on context. Avoid generic defaults.\n",
    "\n",
    "1.  **`target_epsilon`**: Balance '{task_config['data_domain']}' sensitivity vs. {epsilon_utility_focus}. Justify. (Range 1.0-5.0 often considered, but justify *your* choice).\n",
    "2.  **`target_delta`**: Recommend small value (e.g., 1e-5) or \"1/N\". Justify (e.g., N={approx_N_train}).\n",
    "3.  **`max_grad_norm`**: For this {task_type} task, {max_grad_norm_focus} Justify.\n",
    "4.  **`column_sensitivity_epsilon`**: Conceptual relative sensitivity hints (0.0 low, 1.0 high). Exclude target.\n",
    "5.  **`reasoning`**: Detailed justification for epsilon, delta, `max_grad_norm`, linking to dataset context (domain, N, imbalance/target scale) and ML practices.\n",
    "\n",
    "**Output Format:**\n",
    "JSON ONLY with keys: \"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\".\n",
    "\n",
    "JSON Output ONLY:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "def get_gemini_config(prompt, client):\n",
    "    \"\"\"Gets DP config from Gemini API.\"\"\"\n",
    "    if not client:\n",
    "        print(\"Gemini client not available.\")\n",
    "        return None\n",
    "    print(\"\\nSending request to Gemini...\")\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=GEMINI_MODEL_NAME, contents=prompt\n",
    "            )\n",
    "        response_text = response.text\n",
    "        print(\"Gemini Response Received.\")\n",
    "        # Extract JSON part\n",
    "        start_index = response_text.find('{')\n",
    "        end_index = response_text.rfind('}')\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_string_only = response_text[start_index : end_index + 1]\n",
    "            config = json.loads(json_string_only)\n",
    "            print(\"Successfully parsed Gemini config.\")\n",
    "            print(config)\n",
    "            # Basic validation\n",
    "            required_keys = [\"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\"]\n",
    "            if not all(key in config for key in required_keys):\n",
    "                print(\"Warning: Gemini response missing some required keys.\")\n",
    "            return config\n",
    "        else:\n",
    "            print(\"Error: Could not find JSON object in Gemini response.\")\n",
    "            print(\"Raw Response:\", response_text)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call or parsing: {e}\")\n",
    "        try:\n",
    "            print(\"Gemini Response Content (if available):\", response.candidates) # Might show safety blocks\n",
    "        except: pass\n",
    "        return None\n",
    "\n",
    "def get_groq_config(prompt, client, model_name):\n",
    "    \"\"\"Gets DP config from Groq API.\"\"\"\n",
    "    if not client:\n",
    "        print(\"Groq client not available.\")\n",
    "        return None\n",
    "    print(\"\\nSending request to Groq...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model_name,\n",
    "            temperature=0.2,\n",
    "            max_tokens=3024, \n",
    "            top_p=0.8,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "        print(\"Groq Response Received.\")\n",
    "        config = json.loads(response_content)\n",
    "        print(\"Successfully parsed Groq config.\")\n",
    "        print(config)\n",
    "\n",
    "        # Basic validation\n",
    "        required_keys = [\"dp_algorithm\", \"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"preprocessing_suggestions\", \"column_sensitivity_epsilon\", \"reasoning\"]\n",
    "        if not all(key in config for key in required_keys):\n",
    "            print(\"Warning: Groq response missing some required keys.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Groq API call or parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"LLM Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6f82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP CLASSIFICATION Training/Evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "# Description: Function to train and evaluate a DP CLASSIFICATION model using Opacus.\n",
    "\n",
    "def train_evaluate_dp_classification_model(\n",
    "    config, run_name, train_loader, test_loader, # test_loader provides X and y_labels for metrics\n",
    "    n_features, device, epochs, learning_rate,\n",
    "    pos_weight_tensor,\n",
    "    target_metric_prefix=\"Class 1\", # e.g., \"Stroke\", \"Readmit\"\n",
    "    y_test_labels_for_eval=None # Pass the actual y_test labels (numpy array)\n",
    "):\n",
    "    \"\"\"Trains and evaluates DP classification model, returns results dictionary.\"\"\"\n",
    "    print(f\"\\n--- Running: {run_name} (Task: Classification) ---\")\n",
    "    if config is None: # ... (null check) ...\n",
    "        print(\"Skipping run due to missing config.\"); return None\n",
    "\n",
    "    target_eps = config.get(\"target_epsilon\", DEFAULT_TARGET_EPSILON)\n",
    "    target_del_config = config.get(\"target_delta\", \"1/N\")\n",
    "    max_norm = config.get(\"max_grad_norm\", DEFAULT_MAX_GRAD_NORM)\n",
    "    # ... (actual_delta calculation - same as before) ...\n",
    "    if isinstance(target_del_config, str) and \"1/N\" in target_del_config and train_loader:\n",
    "        actual_delta = 1 / len(train_loader.dataset)\n",
    "    elif isinstance(target_del_config, (int, float)):\n",
    "        actual_delta = target_del_config\n",
    "    else: # Fallback\n",
    "        actual_delta = 1 / len(train_loader.dataset) if train_loader else DEFAULT_TARGET_DELTA\n",
    "\n",
    "\n",
    "    dp_model = LogisticRegression(n_features).to(device)\n",
    "    dp_optimizer = optim.SGD(dp_model.parameters(), lr=learning_rate)\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    try:\n",
    "        dp_model, dp_optimizer, dp_data_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=dp_model, optimizer=dp_optimizer, data_loader=train_loader,\n",
    "            max_grad_norm=max_norm, target_epsilon=target_eps, target_delta=actual_delta, epochs=epochs\n",
    "        )\n",
    "        print(f\"Opacus Attached. Target ε={target_eps:.2f}, Target δ={actual_delta:.2e}, Max Grad Norm={max_norm}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching Opacus: {e}\"); return None\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    print(\"Training DP Classification Model...\")\n",
    "    # ... (Training loop - same as before, using dp_data_loader) ...\n",
    "    dp_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_y in dp_data_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            dp_optimizer.zero_grad(); outputs = dp_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y); loss.backward(); dp_optimizer.step()\n",
    "\n",
    "    try:\n",
    "      final_epsilon = privacy_engine.get_epsilon(delta=actual_delta)\n",
    "      print(f\"DP Training Complete. Final ε = {final_epsilon:.4f}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error getting epsilon: {e}\"); final_epsilon = float('nan')\n",
    "\n",
    "\n",
    "    print(\"Evaluating DP Classification Model...\")\n",
    "    dp_model.eval()\n",
    "    all_preds_dp, all_targets_dp_eval = [], [] # all_targets_dp_eval will be from y_test_labels_for_eval\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader: # We only need X for prediction\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = dp_model(batch_X)\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            all_preds_dp.extend(preds.cpu().numpy().flatten())\n",
    "    \n",
    "    if y_test_labels_for_eval is None:\n",
    "        print(\"Error: y_test_labels_for_eval not provided for metrics calculation.\")\n",
    "        return None\n",
    "    all_targets_dp_eval = y_test_labels_for_eval # Use the passed numpy array of true labels\n",
    "\n",
    "    accuracy_dp = accuracy_score(all_targets_dp_eval, all_preds_dp)\n",
    "    precision_dp = precision_score(all_targets_dp_eval, all_preds_dp, pos_label=1, zero_division=0)\n",
    "    recall_dp = recall_score(all_targets_dp_eval, all_preds_dp, pos_label=1, zero_division=0)\n",
    "    f1_dp = f1_score(all_targets_dp_eval, all_preds_dp, pos_label=1, zero_division=0)\n",
    "    print(f\"Accuracy: {accuracy_dp:.4f}, Precision ({target_metric_prefix}): {precision_dp:.4f}, Recall ({target_metric_prefix}): {recall_dp:.4f}, F1 ({target_metric_prefix}): {f1_dp:.4f}\")\n",
    "\n",
    "    results = {\n",
    "        \"Run Type\": run_name, \"LLM Used\": config.get(\"llm_model_name\", \"N/A\"),\n",
    "        \"Target Epsilon\": target_eps, \"Final Epsilon\": final_epsilon,\n",
    "        \"Target Delta\": actual_delta, \"Max Grad Norm\": max_norm,\n",
    "        \"Accuracy\": accuracy_dp,\n",
    "        f\"Precision ({target_metric_prefix})\": precision_dp,\n",
    "        f\"Recall ({target_metric_prefix})\": recall_dp,\n",
    "        f\"F1 ({target_metric_prefix})\": f1_dp,\n",
    "        \"LLM Epsilon Suggestion\": config.get(\"target_epsilon\", \"N/A (Default)\"),\n",
    "        \"LLM Reasoning\": config.get(\"reasoning\", \"N/A\")\n",
    "    }\n",
    "    print(f\"{run_name} results recorded.\")\n",
    "    return results\n",
    "\n",
    "print(\"DP CLASSIFICATION Training/Evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08a3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Config and Prompt for Kaggle Stroke Prediction ---\n",
      "Kaggle Stroke Prediction task config and LLM prompt prepared.\n"
     ]
    }
   ],
   "source": [
    "# Description: Define task details for CURRENT classification dataset and generate prompt.\n",
    "\n",
    "print(f\"\\n--- Preparing Config and Prompt for {CURRENT_DATASET_NAME_FOR_LLM} ---\")\n",
    "if df_current is not None and original_columns_current and 'pos_count_current' in locals():\n",
    "    imbalance_details_current = f\"The target '{CURRENT_TARGET_COLUMN}' has {pos_count_current} positive vs {neg_count_current} negative samples in train set.\"\n",
    "    if abs(pos_count_current - neg_count_current) / (pos_count_current + neg_count_current + 1e-6) < 0.1: # Approx balanced\n",
    "        imbalance_details_current += \" This is considered relatively balanced.\"\n",
    "    else:\n",
    "        imbalance_details_current += \" This is considered imbalanced.\"\n",
    "\n",
    "\n",
    "    task_config_current = {\n",
    "        \"dataset_name\": CURRENT_DATASET_NAME_FOR_LLM,\n",
    "        \"data_domain\": \"Healthcare\", # Adjust if domain changes\n",
    "        \"task_description\": f\"Train a binary classification model (Logistic Regression using DP-SGD) to predict '{CURRENT_TARGET_COLUMN}'.\",\n",
    "        \"target_variable\": CURRENT_TARGET_COLUMN,\n",
    "        \"model_type\": \"Logistic Regression\",\n",
    "        \"dp_mechanism_family\": \"DP-SGD\",\n",
    "        \"details\": imbalance_details_current\n",
    "    }\n",
    "\n",
    "    schema_string_current = \", \".join(original_columns_current)\n",
    "    data_shape_tuple_current = df_current.shape\n",
    "\n",
    "    llm_prompt_current = create_llm_prompt(\n",
    "        task_config_current,\n",
    "        schema_string_current,\n",
    "        data_shape_tuple_current,\n",
    "        task_type=\"classification\" # Explicitly classification\n",
    "    )\n",
    "    print(f\"{CURRENT_DATASET_NAME_FOR_LLM} task config and LLM prompt prepared.\")\n",
    "else:\n",
    "    print(f\"Skipping {CURRENT_DATASET_NAME_FOR_LLM} prompt creation: data not loaded or details missing.\")\n",
    "    llm_prompt_current = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "631dd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Kaggle Stroke Prediction Fixed DP SGD ---\n",
      "\n",
      "--- Running: Kaggle Stroke Prediction Fixed DP SGD (Task: Classification) ---\n",
      "Opacus Attached. Target ε=1.00, Target δ=2.45e-04, Max Grad Norm=1.0\n",
      "Training DP Classification Model...\n",
      "DP Training Complete. Final ε = 0.9920\n",
      "Evaluating DP Classification Model...\n",
      "Accuracy: 0.9511, Precision (Stroke): 0.0000, Recall (Stroke): 0.0000, F1 (Stroke): 0.0000\n",
      "Kaggle Stroke Prediction Fixed DP SGD results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Execute Fixed DP run for the CURRENT classification dataset.\n",
    "\n",
    "run_name_fixed_dp = f\"{CURRENT_DATASET_NAME_FOR_LLM} Fixed DP SGD\"\n",
    "print(f\"\\n--- Running: {run_name_fixed_dp} ---\")\n",
    "\n",
    "if n_features_current is not None and train_loader_current is not None and 'DEFAULT_TARGET_DELTA_CURRENT' in locals() and 'y_test_labels_curr' in locals():\n",
    "    fixed_dp_config_curr = {\n",
    "        \"dp_algorithm\": \"DP-SGD with Gaussian Noise (Fixed)\",\n",
    "        \"target_epsilon\": DEFAULT_TARGET_EPSILON,\n",
    "        \"target_delta\": DEFAULT_TARGET_DELTA_CURRENT,\n",
    "        \"max_grad_norm\": DEFAULT_MAX_GRAD_NORM,\n",
    "        \"reasoning\": f\"Using fixed defaults for {CURRENT_DATASET_NAME_FOR_LLM}.\",\n",
    "        \"llm_model_name\": \"N/A (Fixed Defaults)\" # ... add other expected keys if needed\n",
    "    }\n",
    "    results_fixed_curr = train_evaluate_dp_classification_model(\n",
    "        fixed_dp_config_curr, run_name_fixed_dp,\n",
    "        train_loader_current, test_loader_current, n_features_current, device,\n",
    "        EPOCHS, LEARNING_RATE, pos_weight_tensor_current,\n",
    "        target_metric_prefix=CURRENT_TARGET_METRIC_PREFIX,\n",
    "        y_test_labels_for_eval=y_test_labels_curr\n",
    "    )\n",
    "    if results_fixed_curr: results_list.append(results_fixed_curr)\n",
    "else:\n",
    "    print(f\"Skipping {run_name_fixed_dp} run: missing components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ead0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Kaggle Stroke Prediction Gemini DP SGD ---\n",
      "\n",
      "Sending request to Gemini...\n",
      "Gemini Response Received.\n",
      "Successfully parsed Gemini config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 3.0, 'target_delta': 0.000245, 'max_grad_norm': 10.0, 'preprocessing_suggestions': ['Split Data: Perform train-test split first to prevent data leakage.', \"Imputation: Impute missing values (e.g., 'bmi' often has NaNs). Median imputation for numerical features and mode for categorical features are common strategies.\", \"Encoding: Convert categorical features ('gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status') into numerical representations using one-hot encoding. Consider `drop='first'` to avoid multicollinearity if appropriate for the model library.\", 'Normalization/Scaling: Standardize or normalize all numerical features (including one-hot encoded ones, or apply after encoding numericals and before concatenating with OHE features). StandardScaler (to mean 0, std 1) or MinMaxScaler (to range [0,1]) are recommended. This is crucial for Logistic Regression performance and for making `max_grad_norm` effective and consistent.', \"Address Imbalance (Evaluation): Due to severe class imbalance for 'stroke', model evaluation must prioritize metrics like F1-score, Precision, and Recall for the minority class ('stroke' = 1), rather than just accuracy. While DP-SGD itself doesn't directly use techniques like SMOTE, focusing on these metrics will guide model utility assessment. Class weighting in the loss function could be explored if compatible with the DP-SGD library, but requires careful consideration of how weights affect gradient magnitudes and thus the clipping.\"], 'column_sensitivity_epsilon': {'gender': 0.6, 'age': 0.9, 'hypertension': 0.9, 'heart_disease': 0.9, 'ever_married': 0.5, 'work_type': 0.4, 'Residence_type': 0.3, 'avg_glucose_level': 0.9, 'bmi': 0.8, 'smoking_status': 0.7}, 'reasoning': \"The recommended DP settings aim to balance privacy requirements in a 'Healthcare' domain with the need for model utility, especially given the imbalanced 'stroke' target variable.\\n\\n1.  **`target_epsilon` (3.0)**: Healthcare data is sensitive, warranting strong privacy protection (lower epsilon). However, predicting a rare event like 'stroke' (highly imbalanced target) requires sufficient learning signal. An epsilon of 3.0 is a moderate value, offering a reasonable privacy guarantee while aiming to preserve enough data utility for the model to learn meaningful patterns, especially for the minority class. Extremely low epsilon values would likely render the model ineffective for this task due to excessive noise.\\n\\n2.  **`target_delta` (0.000245)**: This value is derived from the common heuristic `1/N`, where N is the training set size (approx. 4088). So, delta ≈ 1/4088 ≈ 0.0002446. A small delta ensures that the probability of an arbitrary privacy violation is very low, aligning with standard DP practices. This value is preferable to a generic small constant as it's tied to the dataset size.\\n\\n3.  **`max_grad_norm` (10.0)**: This parameter is critical for DP-SGD. For Logistic Regression, especially with normalized/scaled features, gradients are generally well-behaved. However, the dataset exhibits significant class imbalance for the 'stroke' target (249 positive vs 4861 negative in the training set). Gradients from the minority class (positive 'stroke' samples) are infrequent but crucial for learning. A very low `max_grad_norm` (e.g., 1.0) might aggressively clip these important gradients, disproportionately hindering the model's ability to learn to identify the 'stroke' class. A higher value like 10.0 (compared to a generic default) is chosen to allow more signal from these rare instances to pass through, potentially improving recall and F1-score for the minority class. This assumes features are properly scaled beforehand, which helps stabilize overall gradient magnitudes. This value is a starting point and may require empirical tuning.\\n\\n4.  **Preprocessing**: Strict adherence to preprocessing steps like scaling is vital before applying DP-SGD, as gradient clipping sensitivity (`max_grad_norm`) is directly affected by feature magnitudes. Given the imbalance, evaluation metrics focused on the minority class are paramount.\\n\\n5.  **`column_sensitivity_epsilon`**: These are conceptual relative sensitivity scores for the input features. Columns directly related to health conditions (`hypertension`, `heart_disease`, `avg_glucose_level`) or strong quasi-identifiers (`age`) are marked with higher sensitivity. These scores reflect the inherent privacy risk of the information contained within each column before any transformation or DP mechanism is applied.\"}\n",
      "\n",
      "--- Running: Kaggle Stroke Prediction Gemini DP SGD (Task: Classification) ---\n",
      "Opacus Attached. Target ε=3.00, Target δ=2.45e-04, Max Grad Norm=10.0\n",
      "Training DP Classification Model...\n",
      "DP Training Complete. Final ε = 2.9980\n",
      "Evaluating DP Classification Model...\n",
      "Accuracy: 0.9393, Precision (Stroke): 0.3333, Recall (Stroke): 0.2400, F1 (Stroke): 0.2791\n",
      "Kaggle Stroke Prediction Gemini DP SGD results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Get config from Gemini for CURRENT dataset and run.\n",
    "\n",
    "run_name_gemini_dp = f\"{CURRENT_DATASET_NAME_FOR_LLM} Gemini DP SGD\"\n",
    "print(f\"\\n--- Running: {run_name_gemini_dp} ---\")\n",
    "\n",
    "if gemini_client and llm_prompt_current and n_features_current is not None and train_loader_current is not None and 'y_test_labels_curr' in locals():\n",
    "    gemini_config_curr = get_gemini_config(llm_prompt_current, gemini_client)\n",
    "    if gemini_config_curr:\n",
    "        gemini_config_curr[\"llm_model_name\"] = GEMINI_MODEL_NAME\n",
    "        results_gemini_curr = train_evaluate_dp_classification_model(\n",
    "            gemini_config_curr, run_name_gemini_dp,\n",
    "            train_loader_current, test_loader_current, n_features_current, device,\n",
    "            EPOCHS, LEARNING_RATE, pos_weight_tensor_current,\n",
    "            target_metric_prefix=CURRENT_TARGET_METRIC_PREFIX,\n",
    "            y_test_labels_for_eval=y_test_labels_curr\n",
    "        )\n",
    "        if results_gemini_curr: results_list.append(results_gemini_curr)\n",
    "    else: print(f\"Failed to get Gemini config for {CURRENT_DATASET_NAME_FOR_LLM}.\")\n",
    "elif not gemini_client: print(\"Skipping Gemini run: client not initialized.\")\n",
    "else: print(f\"Skipping Gemini run for {CURRENT_DATASET_NAME_FOR_LLM}: missing components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e269a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Multiple Groq LLM Models for Kaggle Stroke Prediction ---\n",
      "\n",
      "--- Testing Groq Model: llama-3.3-70b-versatile for Kaggle Stroke Prediction ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 2.5, 'target_delta': 1e-05, 'max_grad_norm': 10.0, 'preprocessing_suggestions': ['Normalize features using Standard Scaler', 'Split data into training and validation sets', 'Encode categorical variables using One-Hot Encoding', 'Impute missing values with mean or median'], 'column_sensitivity_epsilon': {'gender': 0.5, 'age': 0.2, 'hypertension': 0.8, 'heart_disease': 0.8, 'ever_married': 0.3, 'work_type': 0.4, 'Residence_type': 0.4, 'avg_glucose_level': 0.6, 'bmi': 0.6, 'smoking_status': 0.7, 'age_group': 0.2, 'glucose_group': 0.6, 'bmi_group': 0.6}, 'reasoning': 'The target epsilon of 2.5 balances the need for model utility in the healthcare domain with the sensitivity of patient data. A target delta of 1e-5 is chosen to provide a high level of privacy protection. The max grad norm of 10.0 is selected to preserve minority class signals in the imbalanced dataset, while preventing single points from dominating the gradient. Normalization and encoding of features are suggested to improve model performance. Column sensitivity epsilons are assigned based on the relative sensitivity of each feature, with higher values indicating greater sensitivity.'}\n",
      "\n",
      "--- Running: Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD (Task: Classification) ---\n",
      "Opacus Attached. Target ε=2.50, Target δ=1.00e-05, Max Grad Norm=10.0\n",
      "Training DP Classification Model...\n",
      "DP Training Complete. Final ε = 2.4986\n",
      "Evaluating DP Classification Model...\n",
      "Accuracy: 0.9432, Precision (Stroke): 0.3333, Recall (Stroke): 0.1600, F1 (Stroke): 0.2162\n",
      "Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD results recorded.\n",
      "\n",
      "--- Testing Groq Model: qwen-qwq-32b for Kaggle Stroke Prediction ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 3.0, 'target_delta': 1e-05, 'max_grad_norm': 10.0, 'preprocessing_suggestions': 'Normalize/standardize all features, handle missing values (if present), encode categorical variables (e.g., one-hot encoding), and ensure scaling occurs before DP-SGD training.', 'column_sensitivity_epsilon': {'gender': 0.3, 'age': 0.8, 'hypertension': 0.9, 'heart_disease': 0.9, 'ever_married': 0.2, 'work_type': 0.3, 'Residence_type': 0.2, 'avg_glucose_level': 0.9, 'bmi': 0.7, 'smoking_status': 0.6, 'age_group': 0.8, 'glucose_group': 0.9, 'bmi_group': 0.7}, 'reasoning': \"1. **target_epsilon=3.0**: Balances healthcare data sensitivity (requires stronger privacy than epsilon=1) while maintaining utility for a critical medical task. 2. **target_delta=1e-5**: Prioritizes privacy by using a fixed small delta (lower than 1/N=2.45e-4) to minimize tail risk. 3. **max_grad_norm=10.0**: Mitigates gradient clipping's impact on the minority class (stroke positives are 6% of training data). A higher clip norm (vs default 1.0) preserves rare-class signal gradients, preventing underfitting. 4. **Column Sensitivity**: Medical features (e.g., hypertension, avg_glucose_level) receive highest sensitivity (0.9) due to privacy risks, while demographic features (e.g., work_type) have lower scores. Grouped features (age_group) inherit sensitivity from their raw counterparts. 5. **Normalization/Scaling**: Mandatory for DP-SGD stability and to ensure gradient clipping operates on comparable scales across features. Imbalanced classes require careful gradient handling to avoid minority class gradients being drowned out by majority-class noise.\"}\n",
      "\n",
      "--- Running: Kaggle Stroke Prediction Groq (qwen-qwq-32b) DP SGD (Task: Classification) ---\n",
      "Opacus Attached. Target ε=3.00, Target δ=1.00e-05, Max Grad Norm=10.0\n",
      "Training DP Classification Model...\n",
      "DP Training Complete. Final ε = 2.9902\n",
      "Evaluating DP Classification Model...\n",
      "Accuracy: 0.9403, Precision (Stroke): 0.2800, Recall (Stroke): 0.1400, F1 (Stroke): 0.1867\n",
      "Kaggle Stroke Prediction Groq (qwen-qwq-32b) DP SGD results recorded.\n",
      "\n",
      "--- Testing Groq Model: gemma2-9b-it for Kaggle Stroke Prediction ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 3.0, 'target_delta': 1e-05, 'max_grad_norm': 10.0, 'preprocessing_suggestions': ['Normalize or standardize all numerical features', 'Encode categorical features using one-hot encoding or similar'], 'column_sensitivity_epsilon': {'gender': 0.5, 'age': 1.0, 'hypertension': 0.5, 'heart_disease': 0.5, 'ever_married': 0.5, 'work_type': 0.8, 'Residence_type': 0.5, 'avg_glucose_level': 1.0, 'bmi': 1.0, 'smoking_status': 0.5}, 'reasoning': 'The chosen DP settings are based on a balance between privacy and utility.  A `target_epsilon` of 3.0 provides a reasonable level of privacy protection while still allowing for effective model training. The `target_delta` of 1e-5 ensures a very low probability of privacy breaches.  The `max_grad_norm` of 10.0 is set higher than typical defaults to mitigate the impact of gradient clipping on the minority class (stroke) in the imbalanced dataset. This helps preserve the signal from rare positive samples.  The `column_sensitivity_epsilon` values are conceptual estimates based on the potential sensitivity of each feature to privacy.  For example, age and avg_glucose_level are likely to have a higher sensitivity than categorical features like gender or smoking_status.  These values can be further refined based on empirical analysis and experimentation.'}\n",
      "\n",
      "--- Running: Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD (Task: Classification) ---\n",
      "Opacus Attached. Target ε=3.00, Target δ=1.00e-05, Max Grad Norm=10.0\n",
      "Training DP Classification Model...\n",
      "DP Training Complete. Final ε = 2.9902\n",
      "Evaluating DP Classification Model...\n",
      "Accuracy: 0.9217, Precision (Stroke): 0.2414, Recall (Stroke): 0.2800, F1 (Stroke): 0.2593\n",
      "Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD results recorded.\n",
      "\n",
      "--- Testing Groq Model: deepseek-r1-distill-llama-70b for Kaggle Stroke Prediction ---\n",
      "\n",
      "Sending request to Groq...\n",
      "Groq Response Received.\n",
      "Successfully parsed Groq config.\n",
      "{'dp_algorithm': 'DP-SGD', 'target_epsilon': 3.0, 'target_delta': '1/4088', 'max_grad_norm': 10.0, 'preprocessing_suggestions': ['Normalize/standardize features before training.', 'Impute missing values appropriately.', 'Encode categorical variables using one-hot encoding or label encoding.', 'Scale features to ensure they are on a similar scale.'], 'column_sensitivity_epsilon': {'gender': 0.1, 'age': 0.5, 'hypertension': 0.5, 'heart_disease': 0.5, 'ever_married': 0.1, 'work_type': 0.1, 'Residence_type': 0.1, 'avg_glucose_level': 1.0, 'bmi': 1.0, 'smoking_status': 0.1, 'age_group': 0.5, 'glucose_group': 0.5, 'bmi_group': 0.5}, 'reasoning': 'The target epsilon of 3.0 balances privacy and utility needs for healthcare data, providing sufficient privacy while maintaining model accuracy. The target delta is set to 1/4088, which is appropriate given the dataset size, ensuring a strong privacy guarantee. The max_grad_norm of 10.0 is chosen to accommodate the class imbalance, preserving gradients from the minority class. Column sensitivity values are assigned based on feature types, with higher sensitivity for numerical features like age, avg_glucose_level, and bmi, and lower for categorical features. Preprocessing steps ensure data is properly prepared for training.'}\n",
      "\n",
      "--- Running: Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD (Task: Classification) ---\n",
      "Opacus Attached. Target ε=3.00, Target δ=2.45e-04, Max Grad Norm=10.0\n",
      "Training DP Classification Model...\n",
      "DP Training Complete. Final ε = 2.9985\n",
      "Evaluating DP Classification Model...\n",
      "Accuracy: 0.9354, Precision (Stroke): 0.2647, Recall (Stroke): 0.1800, F1 (Stroke): 0.2143\n",
      "Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD results recorded.\n"
     ]
    }
   ],
   "source": [
    "# Description: Loop through Groq models, get config, and run DP training for CURRENT dataset.\n",
    "\n",
    "print(f\"\\n--- Comparing Multiple Groq LLM Models for {CURRENT_DATASET_NAME_FOR_LLM} ---\")\n",
    "groq_models_to_test = [\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"qwen-qwq-32b\",\n",
    "    \"gemma2-9b-it\",\n",
    "    \"deepseek-r1-distill-llama-70b\" \n",
    "]\n",
    "\n",
    "if not groq_client:\n",
    "    print(\"Skipping Groq model comparison: Groq client not initialized.\")\n",
    "elif not llm_prompt_current:\n",
    "    print(f\"Skipping Groq model comparison for {CURRENT_DATASET_NAME_FOR_LLM}: LLM prompt not generated.\")\n",
    "elif n_features_current is None or train_loader_current is None or 'y_test_labels_curr' not in locals():\n",
    "    print(f\"Skipping Groq model comparison for {CURRENT_DATASET_NAME_FOR_LLM}: data components missing.\")\n",
    "else:\n",
    "    for groq_model_id in groq_models_to_test:\n",
    "        run_name_groq_dp = f\"{CURRENT_DATASET_NAME_FOR_LLM} Groq ({groq_model_id}) DP SGD\"\n",
    "        print(f\"\\n--- Testing Groq Model: {groq_model_id} for {CURRENT_DATASET_NAME_FOR_LLM} ---\")\n",
    "\n",
    "        current_groq_config = get_groq_config(llm_prompt_current, groq_client, groq_model_id)\n",
    "        if current_groq_config:\n",
    "            current_groq_config[\"llm_model_name\"] = groq_model_id\n",
    "            results_current_groq = train_evaluate_dp_classification_model(\n",
    "                current_groq_config, run_name_groq_dp,\n",
    "                train_loader_current, test_loader_current, n_features_current, device,\n",
    "                EPOCHS, LEARNING_RATE, pos_weight_tensor_current,\n",
    "                target_metric_prefix=CURRENT_TARGET_METRIC_PREFIX,\n",
    "                y_test_labels_for_eval=y_test_labels_curr\n",
    "            )\n",
    "            if results_current_groq:\n",
    "                results_list.append(results_current_groq)\n",
    "        else:\n",
    "            print(f\"Failed to get config from Groq model {groq_model_id} for {CURRENT_DATASET_NAME_FOR_LLM}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined Experiment Results ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaggle Stroke Prediction Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.720157</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.80</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kaggle Stroke Prediction Fixed DP SGD</td>\n",
       "      <td>N/A (Fixed Defaults)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991973</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Using fixed defaults for Kaggle Stroke Prediction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kaggle Stroke Prediction Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.997967</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.939335</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The recommended DP settings aim to balance privacy requirements in a 'Healthcare' domain with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD</td>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.498573</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.925636</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.5</td>\n",
       "      <td>The target epsilon of 2.5 balances the need for model utility in the healthcare domain with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD</td>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.990195</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.933464</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The chosen DP settings are based on a balance between privacy and utility.  A `target_epsilon` o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.990195</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.935421</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The target epsilon of 3.0 balances privacy and utility, suitable for healthcare applications. De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD</td>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.498573</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.943249</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>The target epsilon of 2.5 balances the need for model utility in the healthcare domain with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (qwen-qwq-32b) DP SGD</td>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.990195</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.940313</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1. **target_epsilon=3.0**: Balances healthcare data sensitivity (requires stronger privacy than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD</td>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.990195</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.921722</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The chosen DP settings are based on a balance between privacy and utility.  A `target_epsilon` o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.998465</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.935421</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The target epsilon of 3.0 balances privacy and utility needs for healthcare data, providing suff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm  Accuracy  F1 (Stroke)  Precision (Stroke)  Recall (Stroke) LLM Epsilon Suggestion                                                                                        LLM Reasoning\n",
       "0                                   Kaggle Stroke Prediction Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.720157     0.218579            0.126582             0.80                    N/A                                                                                                  N/A\n",
       "1                                 Kaggle Stroke Prediction Fixed DP SGD           N/A (Fixed Defaults)            1.0      0.991973     0.000245           1.0  0.951076     0.000000            0.000000             0.00                    1.0                                                   Using fixed defaults for Kaggle Stroke Prediction.\n",
       "2                                Kaggle Stroke Prediction Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      2.997967     0.000245          10.0  0.939335     0.279070            0.333333             0.24                    3.0  The recommended DP settings aim to balance privacy requirements in a 'Healthcare' domain with th...\n",
       "3        Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD        llama-3.3-70b-versatile            2.5      2.498573      0.00001          10.0  0.925636     0.155556            0.175000             0.14                    2.5  The target epsilon of 2.5 balances the need for model utility in the healthcare domain with the ...\n",
       "4                   Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD                   gemma2-9b-it            3.0      2.990195      0.00001          10.0  0.933464     0.190476            0.235294             0.16                    3.0  The chosen DP settings are based on a balance between privacy and utility.  A `target_epsilon` o...\n",
       "5  Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD  deepseek-r1-distill-llama-70b            3.0      2.990195      0.00001          10.0  0.935421     0.232558            0.277778             0.20                    3.0  The target epsilon of 3.0 balances privacy and utility, suitable for healthcare applications. De...\n",
       "6        Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD        llama-3.3-70b-versatile            2.5      2.498573      0.00001          10.0  0.943249     0.216216            0.333333             0.16                    2.5  The target epsilon of 2.5 balances the need for model utility in the healthcare domain with the ...\n",
       "7                   Kaggle Stroke Prediction Groq (qwen-qwq-32b) DP SGD                   qwen-qwq-32b            3.0      2.990195      0.00001          10.0  0.940313     0.186667            0.280000             0.14                    3.0  1. **target_epsilon=3.0**: Balances healthcare data sensitivity (requires stronger privacy than ...\n",
       "8                   Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD                   gemma2-9b-it            3.0      2.990195      0.00001          10.0  0.921722     0.259259            0.241379             0.28                    3.0  The chosen DP settings are based on a balance between privacy and utility.  A `target_epsilon` o...\n",
       "9  Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD  deepseek-r1-distill-llama-70b            3.0      2.998465     0.000245          10.0  0.935421     0.214286            0.264706             0.18                    3.0  The target epsilon of 3.0 balances privacy and utility needs for healthcare data, providing suff..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Description: Show the results from all runs in a table, handling dynamic metric names.\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Define base column order\n",
    "    base_cols = [\n",
    "        \"Run Type\", \"LLM Used\", \"Target Epsilon\", \"Final Epsilon\", \"Target Delta\",\n",
    "        \"Max Grad Norm\", \"Accuracy\"\n",
    "    ]\n",
    "    # Dynamically find all P/R/F1 columns\n",
    "    metric_cols = sorted([col for col in results_df.columns if \"Precision (\" in col or \"Recall (\" in col or \"F1 (\" in col])\n",
    "    \n",
    "    end_cols = [\"LLM Epsilon Suggestion\", \"LLM Reasoning\"]\n",
    "\n",
    "    # Combine and ensure all are present\n",
    "    final_cols_order = base_cols + metric_cols + end_cols\n",
    "    \n",
    "    # Add missing columns with N/A if they don't exist in the DataFrame yet\n",
    "    for col in final_cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = np.nan # Use np.nan for potential later numerical ops\n",
    "\n",
    "    results_df = results_df[final_cols_order] # Reorder\n",
    "    results_df.fillna(\"N/A\", inplace=True) # Fill any remaining NaNs for display\n",
    "\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', 1500) # Wider for more metric columns\n",
    "    print(\"\\n--- Combined Experiment Results ---\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results recorded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae42f305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Type</th>\n",
       "      <th>LLM Used</th>\n",
       "      <th>Target Epsilon</th>\n",
       "      <th>Final Epsilon</th>\n",
       "      <th>Target Delta</th>\n",
       "      <th>Max Grad Norm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 (Stroke)</th>\n",
       "      <th>Precision (Stroke)</th>\n",
       "      <th>Recall (Stroke)</th>\n",
       "      <th>LLM Epsilon Suggestion</th>\n",
       "      <th>LLM Reasoning</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaggle Stroke Prediction Non-DP SGD</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.720157</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.80</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>cleaned_healthcare_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kaggle Stroke Prediction Fixed DP SGD</td>\n",
       "      <td>N/A (Fixed Defaults)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991973</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Using fixed defaults for Kaggle Stroke Prediction.</td>\n",
       "      <td>cleaned_healthcare_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kaggle Stroke Prediction Gemini DP SGD</td>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.997967</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.939335</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The recommended DP settings aim to balance privacy requirements in a 'Healthcare' domain with th...</td>\n",
       "      <td>cleaned_healthcare_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD</td>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.498573</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.943249</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>The target epsilon of 2.5 balances the need for model utility in the healthcare domain with the ...</td>\n",
       "      <td>cleaned_healthcare_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (qwen-qwq-32b) DP SGD</td>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.990195</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.940313</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1. **target_epsilon=3.0**: Balances healthcare data sensitivity (requires stronger privacy than ...</td>\n",
       "      <td>cleaned_healthcare_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD</td>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.990195</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.921722</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The chosen DP settings are based on a balance between privacy and utility.  A `target_epsilon` o...</td>\n",
       "      <td>cleaned_healthcare_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD</td>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.998465</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.935421</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The target epsilon of 3.0 balances privacy and utility needs for healthcare data, providing suff...</td>\n",
       "      <td>cleaned_healthcare_stroke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               Run Type                       LLM Used Target Epsilon Final Epsilon Target Delta Max Grad Norm  Accuracy  F1 (Stroke)  Precision (Stroke)  Recall (Stroke) LLM Epsilon Suggestion                                                                                        LLM Reasoning                    dataset\n",
       "0                                   Kaggle Stroke Prediction Non-DP SGD                            N/A            N/A           N/A          N/A           N/A  0.720157     0.218579            0.126582             0.80                    N/A                                                                                                  N/A  cleaned_healthcare_stroke\n",
       "1                                 Kaggle Stroke Prediction Fixed DP SGD           N/A (Fixed Defaults)            1.0      0.991973     0.000245           1.0  0.951076     0.000000            0.000000             0.00                    1.0                                                   Using fixed defaults for Kaggle Stroke Prediction.  cleaned_healthcare_stroke\n",
       "2                                Kaggle Stroke Prediction Gemini DP SGD       gemini-2.5-pro-exp-03-25            3.0      2.997967     0.000245          10.0  0.939335     0.279070            0.333333             0.24                    3.0  The recommended DP settings aim to balance privacy requirements in a 'Healthcare' domain with th...  cleaned_healthcare_stroke\n",
       "6        Kaggle Stroke Prediction Groq (llama-3.3-70b-versatile) DP SGD        llama-3.3-70b-versatile            2.5      2.498573      0.00001          10.0  0.943249     0.216216            0.333333             0.16                    2.5  The target epsilon of 2.5 balances the need for model utility in the healthcare domain with the ...  cleaned_healthcare_stroke\n",
       "7                   Kaggle Stroke Prediction Groq (qwen-qwq-32b) DP SGD                   qwen-qwq-32b            3.0      2.990195      0.00001          10.0  0.940313     0.186667            0.280000             0.14                    3.0  1. **target_epsilon=3.0**: Balances healthcare data sensitivity (requires stronger privacy than ...  cleaned_healthcare_stroke\n",
       "8                   Kaggle Stroke Prediction Groq (gemma2-9b-it) DP SGD                   gemma2-9b-it            3.0      2.990195      0.00001          10.0  0.921722     0.259259            0.241379             0.28                    3.0  The chosen DP settings are based on a balance between privacy and utility.  A `target_epsilon` o...  cleaned_healthcare_stroke\n",
       "9  Kaggle Stroke Prediction Groq (deepseek-r1-distill-llama-70b) DP SGD  deepseek-r1-distill-llama-70b            3.0      2.998465     0.000245          10.0  0.935421     0.214286            0.264706             0.18                    3.0  The target epsilon of 3.0 balances privacy and utility needs for healthcare data, providing suff...  cleaned_healthcare_stroke"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df.drop([3,4,5])\n",
    "results_df['dataset'] = 'cleaned_healthcare_stroke'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e8a5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results_multiple_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0741e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: Outline and provide example code for tuning DP parameters.\n",
    "\n",
    "print(\"\\n--- Phase 3: Tuning (Example - Epsilon vs. Utility) ---\")\n",
    "print(\"This section shows how you *could* tune epsilon. Uncomment and adapt to run.\")\n",
    "\n",
    "# tuning_results_list = []\n",
    "# epsilon_values_to_test = [0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
    "\n",
    "# # Choose a base config (e.g., Gemini's recommendations for other params)\n",
    "# tuning_base_config = gemini_config.copy() if 'gemini_config' in locals() and gemini_config else fixed_dp_config.copy()\n",
    "# print(f\"Using base config from: {tuning_base_config.get('llm_model_name', 'Fixed Defaults')}\")\n",
    "\n",
    "# if tuning_base_config and 'n_features' in locals() and 'train_loader' in locals():\n",
    "#     for eps_val in epsilon_values_to_test:\n",
    "#         print(f\"\\nTuning Run: Epsilon = {eps_val}\")\n",
    "#         current_tuning_config = tuning_base_config.copy()\n",
    "#         current_tuning_config[\"target_epsilon\"] = eps_val\n",
    "#         run_name = f\"Tuned DP (Base: {tuning_base_config.get('llm_model_name', 'Fixed')}, Eps={eps_val})\"\n",
    "\n",
    "#         results_tuned = train_evaluate_dp_model(\n",
    "#             current_tuning_config,\n",
    "#             run_name,\n",
    "#             train_loader,\n",
    "#             test_loader,\n",
    "#             n_features,\n",
    "#             device,\n",
    "#             EPOCHS,\n",
    "#             LEARNING_RATE,\n",
    "#             pos_weight_tensor\n",
    "#         )\n",
    "#         if results_tuned:\n",
    "#             tuning_results_list.append(results_tuned)\n",
    "# else:\n",
    "#     print(\"Cannot run tuning - missing base config or other components.\")\n",
    "\n",
    "# if tuning_results_list:\n",
    "#     tuning_df = pd.DataFrame(tuning_results_list)\n",
    "#     print(\"\\n--- Tuning Results (Epsilon vs. F1) ---\")\n",
    "#     display(tuning_df[['Run Type', 'Target Epsilon', 'Final Epsilon', 'F1 (Stroke)']])\n",
    "#     # Add plotting code here if desired (e.g., using matplotlib or seaborn)\n",
    "#     # import matplotlib.pyplot as plt\n",
    "#     # plt.plot(tuning_df['Final Epsilon'], tuning_df['F1 (Stroke)'], marker='o')\n",
    "#     # plt.xlabel('Final Epsilon (ε)')\n",
    "#     # plt.ylabel('F1 Score (Stroke Class)')\n",
    "#     # plt.title('DP Utility-Privacy Trade-off (F1 vs. Epsilon)')\n",
    "#     # plt.grid(True)\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec24248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase 4: Documentation and Conclusion\n",
    "\n",
    "# **Summary of Findings:**\n",
    "# * **Non-DP Baseline:** Establish performance without privacy (often high recall, low precision on minority).\n",
    "# * **Fixed DP:** Show the utility cost of applying generic DP settings (e.g., ε=1.0). Usually significant drop, especially in recall/F1 for minority.\n",
    "# * **LLM-Guided DP (Gemini vs. Llama 3):** Compare the configurations suggested by different LLMs.\n",
    "#     * Did they suggest similar parameters (epsilon, delta, max_grad_norm)?\n",
    "#     * Did they identify similar sensitivities?\n",
    "#     * How did the resulting model performance compare? Did one LLM's config lead to better utility (e.g., F1 score) for a comparable privacy level (final epsilon)?\n",
    "#     * Evaluate the quality and relevance of the 'reasoning' provided by each LLM.\n",
    "# * **Tuning (If Performed):** Discuss the trade-off observed (e.g., how F1 score changes as epsilon increases). What seems like a reasonable balance for this specific task?\n",
    "# * **Overall:** Conclude on the feasibility and potential benefits/drawbacks of using LLMs as DP advisors in this context. Highlight the importance of human oversight and the need to validate LLM suggestions. Mention limitations (e.g., only tested two LLMs, one dataset, simple model).\n",
    "\n",
    "# **Next Steps/Future Work:**\n",
    "# * Test more LLMs.\n",
    "# * Experiment with different datasets and tasks (regression, more complex models).\n",
    "# * Explore prompt engineering variations to improve LLM guidance (e.g., explicitly asking to optimize for F1).\n",
    "# * Investigate using the 'column_sensitivity_epsilon' hints more directly (requires advanced DP techniques beyond standard DP-SGD)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
